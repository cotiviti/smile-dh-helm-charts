# Module Configuration
Configuring modules is fairly straight forward, but somewhat different
than the existing methods using the `cdr-config-Master.properties` file.
This file is still used behind the scenes, but it is generated by the Helm Chart
and included in the application deployment.

> **NOTE**: When using Helm Charts, they become the 'single source of truth' for
your configuration. This means that repeatable, consistent deployments become a
breeze. It also means you should not edit your configuration options in the Smile CDR web
admin console.

You can define your modules in your main values file, or you can define them
in separate files and include them using the `-f` command. This is possible because Helm
[accepts multiple values files](https://helm.sh/docs/chart_template_guide/values_files/)

We recommend defining them in one or more separate files, as this allows you
to manage common settings as well as per-environment overlays. We will discuss this further
down in the Advanced Configuration section below.

## Mapping traditional Smile CDR configuration to Helm

Mapping existing configurations to values files is relatively straight forwards:
### Identify the module configuration parameter.
e.g. [Concurrent Bundle Validation](https://smilecdr.com/docs/configuration_categories/fhir_performance.html#property-concurrent-bundle-validation)
Config.properties format:
`module.persistence.config.dao_config.concurrent_bundle_validation = false`
### Specify them in the values yaml file format:
```yaml
modules:
  persistence:
    config:
      dao_config.concurrent_bundle_validation: "false"
```
The same effective mapping can be used for any module configurations supported by Smile CDR.
## Module definition considerations
Here are some additional fields/considerations that need to be included in your module definitions files:

* Though not strictly required by the `yaml` spec, all values should be quoted.
  You may run into trouble with some values if you do not quote them.
  Specifically, values starting with `*` or `#` will fail if not quoted.
* The `module id` is taken from the yaml key name.
* Modules can be defined, but disabled. They need to be enabled with the `enabled: true` entry.
* Modules other than the cluster manager need to define `type`. A list of module types is available [here](https://smilecdr.com/docs/product_reference/enumerated_types.html#module-types)
* Modules which expose an endpoint need to de defined with a `service` entry, which includes `enabled` and `svcName` entries.
* DB credentials/details can be referenced from your module configurations via `DB_XXX` environment variables.

Any configurations you specify will merge with the defaults, priority going to the values file.

### Disabling included default module definitios
If you wish to disable any of the default modules, we recommend you disable all default modules and define
your own from scratch. This way it will be easier to determine the exact modules you have defined just by
looking at your values files.
You can disable all default modules using:
```yaml
modules:
  useDefaultModules: false
```
You use the `default-modules.yaml` file as a reference by untarring the Helm Chart.

Here is an example of what your module definition may look like when configuring
Smile CDR with the `clustermgr`, `persistence`, `local_security`,
`fhir_endpoint` and `admin_web` modules.
#### `my-module-values.yaml`
<details>
  <summary>Click to expand</summary>

```yaml
modules:
  useDefaultModules: false
  clustermgr:
    name: Cluster Manager Configuration
    enabled: true
    config:
      db.driver: POSTGRES_9_4
      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require
      db.password: "#{env['DB_PASS']}"
      db.username: "#{env['DB_USER']}"
  persistence:
    name: Database Configuration
    enabled: true
    type: PERSISTENCE_R4
    config:
      db.driver: POSTGRES_9_4
      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require
      db.password: "#{env['DB_PASS']}"
      db.username: "#{env['DB_USER']}"
  local_security:
    name: Local Storage Inbound Security
    enabled: true
    type: SECURITY_IN_LOCAL
    config:
      seed.users.file: classpath:/config_seeding/users.json
      password_encoding_type: BCRYPT_12_ROUND
  admin_web:
    name: Web Admin
    enabled: true
    type: ADMIN_WEB
    service:
      enabled: true
      svcName: admin-web
      hostName: default
    requires:
      SECURITY_IN_UP: local_security
    config:
      context_path: ""
      port: 9100
      tls.enabled: false
      https_forwarding_assumed: true
      respect_forward_headers: true
  fhir_endpoint:
    name: FHIR Service
    enabled: true
    type: ENDPOINT_FHIR_REST_R4
    service:
      enabled: true
      svcName: fhir
      hostName: default
    requires:
      PERSISTENCE_R4: persistence
      SECURITY_IN_UP: local_security
    config:
      context_path: fhir_request
      port: 8000
      base_url.fixed: default
```
</details>

### Define Readiness Probe
As Kubernetes only supports a single readiness probe per container, you need to define which endpoint module Kubernetes should use to consider the 'readiness' of your installation.

The default modules included with this chart are configured so that the `fhir_endpoint` module is used for the readiness probe. This is done by setting the `enableReadinessProbe` key to `true` in the module definition.

If you wish to use a different module for the readiness probe, you must disable it for the `fhir_endpoint` module and enable it for the module of your choice. e.g.

#### `my-module-values.yaml`

```yaml
modules:
  fhir_endpoint:
    enableReadinessProbe: false
  my_fhir_endpoint:
    enableReadinessProbe: true
    enabled: true
    ...
```
Alternatively, you may disable the included default modules as described above, and then enable the probe on one of your custom defined modules.

>**Note:** You must enable the readiness probe for exactly one endpoint module. If you specify none, or more than one, the Helm Chart will return an error.
## Install Smile CDR with extra modules definition files

When splitting your configuration into multiple `values` files, pass them in to your `helm upgrade` commandline like so:
```shell
$ helm upgrade -i my-smile-env --devel -f my-values.yaml -f my-module-values.yaml smiledh/smilecdr
```

## Experimental/Unsupported Features

There are scenarios where you may wish to update Smile CDR module configurations directly in the Web console.

* You need to do some realtime troubleshooting that requires live updates of module configuration
* You are working in a *development* environment and you do not have a suitable code pipeline in place to enable fast iteration of changes

In these cases, there are two settings that you can use to update configuration live in the Smile CDR Web Admin console.
Using these settings will alter the values of `config.Locked` and `node.propertysource` in the resulting Smile CDR configuration.
Refer to the [Smile CDR Docs](https://smilecdr.com/docs/installation/installing_smile_cdr.html#module-property-source) for more info on property sources.

### Troubleshooting Mode

You may enable troubleshooting mode for a given Smile CDR Node as follows:
>**Note:** If you have defined a different configuration, please alter the code below accordingly.

```yaml
cdrNodes:
  masterdev:
    config:
      troubleshooting: true
```
Enabling this option lets you update module configurations in the console for troubleshooting/testing/experimenting.

* Your changes will be lost if the pod restarts or if another pod joins the cluster.
* It sets `config.Locked` to `false` and sets `node.propertysource` to `PROPERTIES_UNLOCKED`

### Database Mode

The troubleshooting mode may not meet your requirements in some scenarios:

* You need these changes to persist for a longer time period and your underlying compute resources could be interrupted. For example:
    * You are using ephemeral compute resources such as AWS EC2 Spot instances which can go away with short notice.
    * Your infrastructure needs to be scaled down when not actively working on it.
* You wish to restart the Smile CDR pods while maintaining your manual configuration changes.
* New Kubernetes Pods may come online (If you are testing HPA or HA, for example.)

In these scenarios, it would be a more robust solution to regularly mirror your configuration changes to your Helm `values` file and reconcile.

In the event that this is not possible, and your manually entered configurations must persist in the above scenarios, you may use the `database` mode as follows:
>**Note:** If you have defined a different configuration, please alter the code below accordingly.

```yaml
cdrNodes:
  masterdev:
    config:
      database: true
```

When enabling this mode, consider the following:

* This is an ***experimental*** feature and is unsupported. Use at your own risk.
* It sets `config.Locked` to `false` and sets `node.propertysource` to `DATABASE`
* The Helm Chart will still create surrounding Kubernetes resources (Ingress, Service, Extra files, Mapped secrets etc) based on the contents of the Helm `values` file.
* If you add a new module from the Smile CDR Web console and that module has an endpoint configuration, you will **not** be able to access it. No Ingress or Service objects will be created.
* Any changes made in the Smile CDR Web console that do not match the Helm `values` settings will lead to configuration drift that may cause unpredictable behaviour.
* In the event of such drift occurring, reverting this mode to `disabled` may then lead to unpredictable behaviour that could result in modules being incorrectly configured, resulting to critical system or data integrity faults.
>***!!!DO NOT USE THIS EXPERIMENTAL UNSUPPORTED FEATURE IN NON-DEVELOPMENT ENVIRONMENTS!!!***
