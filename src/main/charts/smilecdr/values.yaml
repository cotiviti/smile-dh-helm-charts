## @section Image parameters
## @param image.repository OCI repository with Smile CDR images
## @param image.tag Smile CDR version to install. Default is the chart appVersion.
## @skip image.pullPolicy
image:
  # -- Smile CDR version to install. Default is the chart appVersion.
  tag: ""
  # -- OCI repository with Smile CDR images
  repository: docker.smilecdr.com/smilecdr
  # -- Image Pull Policy
  pullPolicy: IfNotPresent

  ## @section Image Credentials
  ## @param image.credentials.type Type of credentials. `sscsi` or `k8sSecret`

  ## @section Image Credentials (values)
  ## @param image.credentials.values[0].registry container registry these values are for
  ## @param image.credentials.values[0].username container registry username
  ## @param image.credentials.values[0].password container registry password

  # -- For private registries, you must provide image credentials. These can be regular references to K8s `Secret` resources, or they can use Secrets Store CSI Driver (sscsi).
  # -- If using sscsi, specify `type`, `provider` and `secretArn`. K8s `Secret` resources will be auto generated.
  # -- If not using sscsi, you can pass in standard 'k8s' imagePullSecrets definitions
  # -- You may leave undefined if using ECR and your worker nodes have instance profiles with an appropriate IAM role to access the registry.
  imagePullSecrets: []
    # A list of sscsi or k8sSecret definitions
    # - type: sscsi
    #   provider: aws
    #   secretArn: "arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname"
    #   nameOverride: name (Optional)

    # - name: name
    #   type: k8sSecret

## @section Public URL
## @param specs.hostname Hostname for Smile CDR instance
## @param specs.rootPath Root pathname
specs:
  # -- Hostname for Smile CDR instance
  hostname: smilecdr-example.local
  # Define a common root context path, if required.
  rootPath: /

## @section Resources
## @param.requests.cpu CPU Requests
## @param.limits.memory Memory allocation
## @param.jvm.memoryFactor JVM HeapSize factor. `limits.memory` is multiplied this to calculate `-Xmx`
## @param.jvm.xms Set JVM heap `-Xms` == `-Xmx`
## @param.jvm.args Set extra JVM args
resources:
  # As Smile CDR is a high performance Java based application, special consideration needs to be given
  # to the resource settings and JVM tuning parameters.
  # Typical cloud best practices are to start small and increase as workload increases. This can be a
  # challenging approach with an appliction such as Smile CDR if we are to avoid OOM Killed errors.
  # You should start with a somewhat larger memory allocation and perform monitoring of your workload
  # to determine real life memory usage. Then tune-down to a size that still gives a reasonable buffer
  # for any workload spikes.
  # These settings go hand-in-hand with the jvm.memoryFactor and jvm.xmx settings as any Java based
  # application will use more memory than specified in -Xmx alone. See the jvm section for more detail
  # on this.
  requests:
    # -- CPU Requests
    cpu: "1"
    # As per best practice, limits.memory == requests.memory
    # This is easily achieved by only setting limits.memory
    # If requests.memory is undefined, Kubernetes sets it to match limits.memory
  limits:
    # As per best practice, do not set limits.cpu
    # This approach may be changed if there are per-core licensing concerns
    # Note that this will cause a warning in some linters
    # -- Memory allocation
    memory: 4Gi

jvm:
  # As Smile CDR is a Java application, we need to pay special attention to the
  # JVM memory settings and tuning.
  # A Java application will use more memory than is specified in the `-Xmx` option,
  # so setting it to the same as the l`imits.memory` setting almost guarantees that
  # the pod will be evicted at some point with an OOM Killed error.
  # The exact factor can be hard to determine as it will depend on factors such as
  # the module configurations, required workload throughput and horizontal scaling
  # factors.
  # The `jvm.memoryFactor` setting will set `-Xmx` based on `resources.memory` if
  # explicitly set, or fall back to `limits.memory` (Which ends up being the same
  # as `requests.memory` if it was unset)
  # For example:
  #  * A setting of 0.5 means the `-Xmx` will be set to `0.5 * resources.memory` or
  #    `0.5 * limits.memory`
  #  * If `limits.memory` is set to `8Gi`, `-Xmx` will be set to `4096m`
  #  * If `limits.memory` is set to `8Gi` and `resources.memory` is set to `4Gi`,
  #    `-Xmx` will be set to `2048m`
  # Conversion of Kubernetes style suffix to Java style suffixes is performed
  # automatically
  # -- JVM HeapSize factor. `limits.memory` is multiplied this to calculate `-Xmx`
  memoryFactor: 0.5

  # If you have your memory settings set such that an OOM Kill event is likely as the
  # JVM Heap grows, it may not become apparent until you reach some high workload
  # spike, or after the heap grows over time. In these cases it can be troublesome
  # to determine the cause. This is much more relevant in a K8s Pod scenario than when
  # running directly on a VM, as there is no swap space enabled to "absorb" this memory groth.
  # It is recommended to set `-Xms`` to the same value as `-Xmx``. This way, any insufficient
  # memory settings may be surfaced sooner. It results in a more stable and predictable
  # environment.
  # -- Set JVM heap `-Xms` == `-Xmx`
  xms: true

  # -- Set extra JVM args
  args:
  - "-Dsun.net.inetaddr.ttl=60"
  - "-Djava.security.egd=file:/dev/./urandom"


## @section Scaling
## @param replicaCount Number of replicas to deploy. Note that this setting is ignored if autoscaling is enabled. Should always start a new installation with 1
## @param autoscaling.enabled Enable or disable autoscaling
## @param autoscaling.minReplicas Recommend 1 for dev environments, 2 for prod or 3 for HA prod
## @param autoscaling.maxReplicas Depends on peak workload requirements and available licensing
## @param autoscaling.targetCPUUtilizationPercentage Tune this value to alter autoscaling behaviour

# -- Number of replicas to deploy. Note that this setting is ignored if autoscaling is enabled. Should always start a new installation with 1
replicaCount: 1

autoscaling:
  # -- Enable or disable autoscaling
  enabled: false
  # -- Recommend 1 for dev environments, 2 for prod or 3 for HA prod
  minReplicas: 1
  # -- Depends on peak workload requirements and available licensing
  maxReplicas: 4
  targetCPUUtilizationPercentage: 80

## @section External Database Configuration
## @param database.external.enabled Enable database external to K8s cluster
## @param database.external.credentials Source of DB secret for external database. A map with `type` set to `sscsi` or `k8sSecret` and `provider` set to `aws`
## @param database.external.databases List of databases used by this Smile CDR installation
## @param database.external.databases.secretName Name of K8s secret with DB connection credentials
## @param database.external.databases.module The module that this database is used for
## @skip database.external.databases.url
## @skip database.external.databases.dbType
## @skip database.external.databases.dbname
## @skip database.external.databases.user
## @skip database.external.databases.passKey

database:
  # Either crunchypgo or external should be set to true. Not both.
  # If using external DB, then Kubernetes secret needs to be set up with
  # the correct credentials. See docs.
  external:
    # -- Enable database external to K8s cluster
    enabled: false
    defaults:
      connectionConfigSource:
        # Default secret used for connection configuration. Type sscsi or k8sSecret
        source: k8sSecret
        # If using sscsi, provider must be specified
        # provider: aws
      connectionConfig:
        authentication:
          # Authentication type: `password`, `iam` or `secretsmanager`
          type: password
          # `provider` required with types `iam` or `secretsmanager`. Currently only supports `aws`
          provider: aws
    # Per-database configurations
    # databases:
    # - name: exampleDatabaseConfiguration
    #   enabled: false
    #   # List of modules that use this database
    #   modules:
    #     - clustermgr
    #     - persistence
    #     - audit
    #     - transaction
    #   # Use this to override the default connectionConfigSource
    #   connectionConfigSource:
    #     # `source` - The source of the secret that contains connection details.
    #     # Options: `k8sSecret`, `sscsi` or `none` if using IAM and required details are provided in `connectionOverrides`
    #     # source: k8sSecret
    #
    #     # `secretName` is required with `k8sSecret` source. Must be a pre-existing Kubernetes Secret object in the same namespace.
    #     secretName: smilecdr
    #
    #     # `provider` is required with `sscsi` source.
    #     provider: aws
    #     # `secretArn` required with `sscsi` source and `aws` provider
    #     secretArn: "arn:aws:secretsmanager:us-east-1:1234567890:secret:dbsecretname"

    #     # Optional: If your vault secret has different key names, you can override
    #     # them with secretKeyMappings
    #     secretKeyMappings:
    #       urlKey: url
    #       userKey: user
    #   # Use this to override the default connectionConfig. Useful if different DB connections
    #   # use different authentication mechanisms.
    #   connectionConfig:
    #     authentication:
    #       type: iam
    #     # The below are optional and override any values provided by the secret configured in `connectionConfigSource`
    #     # url: theDatabaseUrl
    #     # port: 6543
    #     # dbname: theDatabaseName
    #     # user: theDBUserName
    #     # You cannot override password. You MUST use a secret, IAM or Secrets Manager authentication.

  ## @section CrunchyData PGO Database Configuration
  ## @param database.crunchypgo.enabled Enable database provisioned in-cluster via CrunchyData PGO
  ## @param database.crunchypgo.users List of PostgreSQL users/databases to create.
  ## @param database.crunchypgo.users[1].name PostgreSQL username
  ## @param database.crunchypgo.users[1].module Smile CDR module that will use this user/database
  ## @param database.crunchypgo.internal Create the Postgres database as part of this Helm Chart
  ## @param database.crunchypgo.config.postgresVersion PostgreSQL version to use
  ## @param database.crunchypgo.config.instanceReplicas Number of Postgres instances to run (For HA)
  ## @param database.crunchypgo.config.instanceCPU PostgrSQL cpu allocation
  ## @param database.crunchypgo.config.instanceMemory PostgrSQL memory allocation
  ## @param database.crunchypgo.config.instanceSize PostgrSQL storage allocation
  ## @param database.crunchypgo.config.backupsSize PostgrSQL backups storage allocation
  ##

  crunchypgo:
    # -- Enable database provisioned in-cluster via CrunchyData PGO
    enabled: false
    # -- Create the Postgres database as part of this Helm Chart
    internal: false

    config:
      # -- PostgreSQL version to use
      postgresVersion: 14
      # -- Number of Postgres instances to run (For HA)
      instanceReplicas: 2
      # -- PostgrSQL cpu allocation
      instanceCPU: 1
      # -- PostgrSQL memory allocation
      instanceMemory: 2Gi
      # -- PostgrSQL storage allocation
      instanceSize: 10Gi
      # -- PostgrSQL backups storage allocation
      backupsSize: 10Gi
      # If you need faster disk for the PostgreSQL cluster, you can specify a higher performance
      # `storageClass` in your cluster and reference it here.
      #storageClass: gp3-fast
    users:
      # - PostgreSQL username
    - name: smilecdr
      # -- Smile CDR module that will use this user/database
      module: clustermgr
    - name: audit
      module: audit
    - name: transaction
      module: transaction
    - name: persistence
      module: persistence


## @skip cdrNodes
# This is where we define multiple CDR node configurations for Smile CDR
# This is a developing feature and there may be breaking changes later on.
# Any of the top level keys may be used within a CDR node definition.
# If something is not defined, it will fall back to the default top level definition.
# @ignored
cdrNodes:
  masterdev:
    name: Masterdev
    enabled: true
    config:
      locked: true
      #
      # troubleshooting:
      # This option lets you update configs in the console for troubleshooting/testing/experimenting.
      # Your changes will be lost if the pod restarts or if another pod joins the cluster.
      # It sets `config.Locked` to `false` and sets `node.propertysource` to `PROPERTIES_UNLOCKED`
      # See the [Smile CDR Docs](https://smilecdr.com/docs/installation/installing_smile_cdr.html#module-property-source) for more info
      troubleshooting: false
      #
      # database:
      # This mode is unsupported and not recommended for use when deploying using Helm
      # It will set the properties mode to `DATABASE`.
      # If modules are added or altered in the console, the environment will be in a state of
      # drift compared to the Helm Chart values.
      # In this mode, it will not be possible to update certain module configurations that affect
      # the supporting infrastructure (i.e. context roots, ports, databases)
      # In the event of drift occurring, reverting this mode to `disabled` may then lead to unpredictable behaviour
      # that could result in modules being incorrectly configured, resulting to critical system
      # faults.
      #database: false
    security:
      strict: false

volumeConfig:
  cdr:
    log:
      size: 10Gi

## @section Extra Environment Variables
## @param extraEnvVars map of environment variables to be passed directly to Smile CDR container spec.
# Define extra environment variables to pass in to the Smile CDR container.
extraEnvVars: []

## @section Extra Volumes
## @param extraVolumes map of volumes to be passed directly to Smile CDR container spec.
# Define extra volumes to pass in to the Smile CDR container.
extraVolumes: {}

## @section Extra Volume mounts
## @param extraEnvVars map of volume mounts to be passed directly to Smile CDR container spec.
# Define extra volume mounts to pass in to the Smile CDR container.
extraVolumeMounts: {}

## @section Smile CDR Modules Configuration
## @param modules.useDefaultModules Enable or disable included default modules configuration
## @param autoDeploy Enable or disable automatic deployment of changes to Smile CDR configuration
# Define modules to be used. Some of these will contain service definitions.
# A service and an ingress rule will be created for modules that use services.
# Canonical endpoint URLs will be generated by _smile-module-helpers.tpl and
# populated in the smilecdr.services variable. These can be consumed by other
# modules that reference them.
modules:
  # -- Enable or disable included default modules configuration
  useDefaultModules: true

# Automatically do rolling deployment when configmaps or files are changed
# -- Enable or disable automatic deployment of changes to Smile CDR configuration
autoDeploy: true

## @section Including Extra Files into Smile CDR
## @param Map of file definitions to map into the Smile CDR instance
# -- Map of file definitions to map into the Smile CDR instance
mappedFiles: {}
# This needs to be used in conjunction with the --set-file option of Helm
#   testfile.txt:
#     type: configMap
#     configMapBaseName: smilecdr
#     path: /home/smile/smilecdr/classes

## @section Copying Extra Files into Smile CDR Pods from external location
copyFiles: {}
##  Copies files using curl to the classes directory
#   classes:
#     syncDefaults: true
#     sources:
#     - type: curl
#       url: https://github.com/pathe/to/releases/download/required-package.jar
#       filename: required-package.jar
##  Copies files from S3 to the customerlib directory
#   customerlib:
#     sources:
#     - type: s3
#       bucket: s3-bucket-name
#       path: /version/customerlib
## Default configuration for images used for file copying.
# config:
#   curl:
#     tag: 8.1.2
#     # Using Quay.io (Default)
#     repository: quay.io/curl/curl
#     # Using DockerHub
#     repository: curlimages/curl
#     securityContext:
#       runAsUser: 100
#   s3:
#     tag: 2.11.25
#     # Using public.ecr.aws (Default)
#     repository: public.ecr.aws/aws-cli/aws-cli
#     # Using DockerHub
#     repository: amazon/aws-cli
#     securityContext:
#       runAsUser: 1000

## @section Ingresses
## @param ingresses Map of ingress definitions
## @param ingresses.<ingressName>.enabled Enable Ingress
## @param ingresses.<ingressName>.type Ingress type (`nginx-ingress`,`aws-lbc-alb`, `azure-agic`)
##
ingresses:
  # This existing entry configures the default ingress.
  # All services use this ingress by default if it is enabled, unless they are configured to use a different
  # named ingress resource.
  default:
    # -- Enable ingress
    enabled: true
    # Make this the default ingress
    defaultIngress: true
    # This forces the default ingress to use the '-scdr' suffix. Only valid in the context of the default ingress
    # to help with migrations from the legacy Ingress functionality.
    # This will ultimately be deprecated
    useLegacyResourceSuffix: true
    # Select the cloud vendor and GW/NLB controller being used.
    # Currently supported values are:
    # nginx-ingress - Nginx Ingress Controller (Network Load Balancer when using AWS Load Balancer Controller)
    # - This will automatically use the `nginx` ingressClass
    # aws-lbc-alb - AWS Load Balancer Controller (Application Load Balancer)
    # - This will automatically use the `nginx` ingressClass
    # azure-agic - Azure Application Gateway Ingress Controller
    # - This will automatically use the `alb` ingressClass
    # other - If it is none of the above, or unset, no default Ingress annotations will be created. It is up to
    #         you to provide appropriate `extraAnnotations` below
    # -- Ingress type (`nginx-ingress`,`aws-lbc-alb`,`azure-agic`)
    type: nginx-ingress
    tls13NginxConfigSnippet: true

    # Should this ingress be publicly accessible. Enabled by default for the default ingress.
    public: true

    # This chart assumes certain default Ingress Class names. If your environment has defined a different class
    # name then override it here
    # If using nginx-ingress and multiple load balancers, you would specify the different nginx-ingress classes here.
    # ingressClassNameOverride: my-nginx

    # Define any extra ingress annotations here.
    # @ignore
    annotations: {}

  # myCustomNonPublicIngress:
  #   enabled: true
  #   type: nginx-ingress
  #   public: false
  #   annotations: {}

tls:
  certificateIssuers:
    default:
      enabled: false
      defaultIssuer: true

  certificates:
    default:
      enabled: false
      defaultCertificate: true

  # Here we can easily enable TLS on all HTTP endpoint modules in Smile CDR
  defaultEndpointConfig:
    enabled: false
    extraCdrConfig:
      tls.protocol.protocol_whitelist: TLSv1.3
      tls.protocol.cipher_whitelist: TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256

## @section Service Account (IRSA)
## @param serviceAccount.create Specifies whether a service account should be created
## @param serviceAccount.annotations Annotations to add to the service account
## @param serviceAccount.name Autogenerated if not set
serviceAccount:
  # -- Specifies whether a service account should be created
  create: false
  # -- Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  # -- Autogenerated if not set
  name: ""

## @section messageBroker Message Broker Configuration
## @param messageBroker.external.enabled Enable external Message Broker
## @param messageBroker.external.type External message broker type
## @param messageBroker.external.bootstrapAddress External message broker bootstrap address
## @param messageBroker.external.tls External message broker TLS support
## @param messageBroker.strimzi.enabled Enable provisioning of Kafka using Strimzi Operator
## @skip messageBroker.strimzi.config

## @param.messageBroker.channelPrefix Topic Channel Prefix

messageBroker:
  external:
    enabled: false
    # -- External message broker type. `kafka` or `activemq`
    type: kafka
    config:
      connection:
        type: tls # Default to `tls`. Optional. Can set to `plaintext`
        # -- Mandatory: External message broker bootstrap address
        # bootstrapAddress: kafka-example1.local, kafka-example2.local
        caCert: {} # Optional. If not specified, then assume public CA. (i.e. for MSK)
          # type: sscsi # Or k8sSecret or public
          # provider: aws
          # secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkacacert
          # Mandatory if `type: k8sSecret`, otherwise ignored.
          # secretName: pre-existing-secret

      authentication:
        # Default is `tls` for mTLS. Use `iam` for IAM or `none` for no auth
        # If using `tls` or `iam`, `connection.type` MUST be set to `tls`
        # Using `iam` implies using MSK, although you can still use `tls` with MSK if you have an AWS Private CA
        type: tls

        # Configure autoconfiguration for IAM authentication
        # iamConfig:
        #   autoJarCopy: true
        #   # Can use curl or s3
        #   copyType: curl
        #   fileName: "aws-msk-iam-auth-1.1.6-all.jar"
        #   url: "https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.6/aws-msk-iam-auth-1.1.6-all.jar"
        #   # S3...
          # bucket: s3-bucket-name
          # path: /path/tojarfile/aws-msk-iam-auth-1.1.6-all.jar

          # If using S3, you must use `copyFiles.customerlib` to explicitly configure copying a bucket that contains
          # this jar file to the customerlib directory.

        userCert: {} # Mandatory if `authentication.type: tls`. Ignored if using `iam`.
          # type: sscsi # Or k8sSecret
          # provider: aws
          # secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkaclientcert
          # Mandatory if `type: k8sSecret`, otherwise ignored.
          # secretName: pre-existing-secret
  strimzi:
    # -- Enable provisioning of Kafka using Strimzi Operator
    enabled: false # Can only enable `strimzi` or `external`, not both.

    kafka:
      # To make this 'just work' and in a secure manner, we are currently opinionated
      # and will only support automatic configuration of mTLS.
      connection:
        type: tls # Default to `tls`.
      authentication:
        type: tls # Default to `tls`.
        # Creation of user is implicit when using mTLS, so no user auth is required, unless
        # we wish to start creating extra users for management/monitoring. As this is a single
        # tenancy solution only, user will have full access to all topics.
      version: "3.3.1"
      protocolVersion: "3.3"
      replicas: 3
      volumeSize: 10Gi
      resources:
        requests:
          cpu: "0.5"
          memory: 1Gi
        limits:
          memory: 1Gi
    zookeeper:
      replicas: 3
      volumeSize: 10Gi
      resources:
        requests:
          cpu: 0.5
          memory: 512Mi
        limits:
          memory: 512Mi
  manageTopics: true
  topics:
    # Using map instead of list as it becomes easier to merge/override settings for individual topics later on.
    batch2:
      name: "batch2.work.notification.Masterdev.persistence"
      partitions: 10
    subscription:
      name: "subscription.matching.Masterdev.persistence"
      partitions: 10
  adminPod:
    enabled: false
  clientConfiguration:
    # The Helm Chart will need to configure values in the properties files, which will override the defaults.
    # Passing in your own properties files will interfere with this mechanism. To avoid this, your Kafka
    # consumer/producer properties should be defined here instead.
    consumerProperties:
      # Taken from the default `cdr-kafka-consumer-config.properties`
      # heartbeat.interval.ms: 120000 # 2 minutes
      # session.timeout.ms: 420000 # 7 minutes
      # max.poll.interval.ms: 3600000 # 60 minutes
      # max.poll.records: 200 # Only grab 200 records at a time
      # New recommended values (Kafka default values prior to Kafka 3.0)
      heartbeat.interval.ms: 3000 # 3 seconds
      session.timeout.ms: 10000 # 10 seconds
      max.poll.interval.ms: 300000 # 5 minutes
      max.poll.records: 20 # Only grab 20 records at a time
    producerProperties: {}

## @section observability
# observability
observability:
  # Enable/disable entire observability suite
  enabled: false
  instrumentation:
    openTelemetry:
      enabled: true
      otelAgent:
        # Install OpenTelemetry Java Agent into Smile CDR pods to expose JVM metrics
        enabled: true
        # mode: "helm" or "operator"
        mode: helm
        spec:
          resourceAttributes:
          # This configuration mirrors what the OpenTelemetry operator creates by default.
          - name: k8s.container.name
            valueFrom: container.name
          - name: k8s.deployment.name
            valueFrom: deployment.name
          - name: k8s.namespace.name
            valueFrom: namespace.name
          - name: k8s.node.name
            valueFrom: spec.nodeName
          - name: k8s.pod.name
            valueFrom: metadata.name
          - name: k8s.replicaset.name
            valueFrom: replicaset.name
          # Provide a custom fixed value attribute
          # - name: custom_attribute
          #   value: Custom Value
          # Provide a custom attribute from an existing ENV var
          # - name: custom_attribute_2
          #   value: $(ENV_VAR_NAME)
          exporter:
            endpoint: http://localhost:4317
          env: []
          # - name: OTEL_LOGS_EXPORTER
          #   value: otlp
          # - name: OTEL_METRICS_EXPORTER
          #   value: "prometheus"
          # - name: OTEL_EXPORTER_PROMETHEUS_PORT
          #   value: 9464
          java:
            env: []
          # propagators:
          # - baggage
          # - b3
          # sampler:
          #   argument: 1
          #   type: parentbased_traceidratio
      otelCollector:
        # Install OpenTelemetry Sidecar container into Smile CDR pods to collect and ship JVM metrics
        enabled: true
        # Select how to install the collector.
        # Currently only supports using the otel operator
        # Choose sidecar (Default), deployment or helm(Not yet implemented)
        mode: sidecar

    prometheus:
      enabled: false
      # Sets up pod monitor CR so the prometheus operator can discover Smile CDR
      operatorConfig:
        enabled: false
      # Install Java Agent into Smile CDR pods to expose JVM metrics
      promAgent:
        enabled: true
        config:
          port: 17171
          rules:
          - pattern: ".*"

## @section Extra labels
## @param labels Extra labels to apply to all resources
# Extra labels
# -- Extra labels to apply to all resources
labels: {}

## @skip deploymentAnnotations
# @ignored
deploymentAnnotations: {}

## @skip podAnnotations
# @ignored
podAnnotations: {}

## @skip podSecurityContext
# @ignored
podSecurityContext:
  fsGroup: 1000

## @skip securityContext
# @ignored
securityContext:
  capabilities:
    drop:
    - ALL
  privileged: false
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true

## @skip nodeSelector
# @ignored
nodeSelector: {}

## @skip tolerations
# @ignored
tolerations: []

## @skip affinity
# @ignored
affinity: {}

## Temporarily force old resource naming until multi-node feature is ready for use.
oldResourceNaming: true
