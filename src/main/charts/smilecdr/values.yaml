
database:
  # Either crunchypgo or external should be set to true. Not both.
  # If using external DB, then Kubernetes secret needs to be set up with
  # the correct credentials. See docs.
  crunchypgo:
    enabled: true
    internal: false
    config:
      postgresVersion: 14
      instanceReplicas: 1
      instanceCPU: 1
      instanceMemory: 2Gi
      instanceSize: 10Gi
      backupsSize: 10Gi
      #storageClass: gp3-fast
      users:
        # Defining users here will cause the username and the database to be
        # created using the Crunchy PGO.
        - name: "smilecdr"
          databases:
            - smilecdr

    # Only change this if you change the SmileCDR username in postgresscluster.users
    # userName: smilecdr
  external:
    enabled: false
    # credentialsSource can be k8s or AWS Secrets Manager. Others to be implemented later.
    credentialsSource: k8s
    # TODO: Explain how to configure secret.
    secretName: changeme
    dbType: postgres
    port: 5432

messageBroker:
  strimzi:
    enabled: false
    config:
      version: "3.3.1"
      protocolVersion: "3.3"
      tls: true
      kafka:
        replicas: 3
        volumeSize: 10Gi
        resources:
          requests:
            cpu: "0.5"
            memory: 1Gi
          limits:
            memory: 1Gi
      zookeeper:
        replicas: 3
        volumeSize: 10Gi
        resources:
          requests:
            cpu: 0.5
            memory: 512Mi
          limits:
            memory: 512Mi

  external:
    enabled: false
    type: kafka
    bootstrapAddress: kafka-example.local
    tls: true
  channelPrefix: SCDR-ENV-


specs:
  hostname: smilecdr-example.local
  # Define a common root context path, if required.
  #rootPath: /

# Extra labels
labels: {}

# Define modules to be used. Some of these will contain service definitions.
# A service and an ingress rule will be created for modules that use services.
# Canonical endpoint URLs will be generated by _smile-module-helpers.tpl and
# populated in the smilecdr.services variable. These can be consumed by other
# modules that reference them.
modules:
  usedefaultmodules: true

mappedFiles: {}
#   testfile.txt:
#     type: configMap
#     configMapBaseName: smilecdr
#     path: /home/smile/smilecdr/classes

# This is where we define multiple node configurations for SmileCDR
# This is a developing feature and there may be breaking changes later on
# For now, it only defines the name, which is used in the properties ConfigMap
# It may later be used to define deployment-specific attributes.
# @ignored
cdrNodes:
  Masterdev:
    enabled: true # Not used yet.

# Number of replicas to deploy. Note that this setting is ignored if autoscaling
# is enabled.
replicaCount: 1

# Automatically do rolling deployment when configmaps or files are changed
autoDeploy: true

image:
  repository: docker.smilecdr.com/smilecdr
  # Override only if you have created a K8s docker config secret externally to this Helm Chart
  # If you specify credentials here, a K8s docker config secret will be auto generated.
  # Do not use in production environments.
  credentials:
    # Choose one of sscsi, extsecret, values
    # type: sscsi
    # provider: aws
    # secretarn: "arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname"
    #
    # type: extsecret
    # pullSecrets:
    #   - name: scdr-docker-secrets
    type: values
    values:
      - registry: docker.com
        username: user
        password: pass
  pullPolicy: IfNotPresent
  # Override the image tag. Default is the chart appVersion.
  tag: ""

jvm:
  # As SmileCDR is a Java application, we need to pay special attention to the
  # JVM memory settings and tuning.
  # A Java application will use more memory than is specified in the `-Xmx` option,
  # so setting it to the same as the l`imits.memory` setting almost guarantees that
  # the pod will be evicted at some point with an OOM Killed error.
  # The exact factor can be hard to determine as it will depend on factors such as
  # the module configurations, required workload throughput and horizontal scaling
  # factors.
  # The `jvm.memoryFactor` setting will set `-Xmx` based on `resources.memory` if
  # explicitly set, or fall back to `limits.memory` (Which ends up being the same
  # as `requests.memory` if it was unset)
  # For example:
  #  * A setting of 0.5 means the `-Xmx` will be set to `0.5 * resources.memory` or
  #    `0.5 * limits.memory`
  #  * If `limits.memory` is set to `8Gi`, `-Xmx` will be set to `4096m`
  #  * If `limits.memory` is set to `8Gi` and `resources.memory` is set to `4Gi`,
  #    `-Xmx` will be set to `2048m`
  # Conversion of Kubernetes style suffix to Java style suffixes is performed
  # automatically

  memoryFactor: 0.5

  # If you have your memory settings set such that an OOM Kill event is likely as the
  # JVM Heap grows, it may not become apparent until you reach some high workload
  # spike, or after the heap grows over time. In these cases it can be troublesome
  # to determine the cause.
  # It is recommended to set -Xms to the same value as -Xmx. This way, any insufficient
  # memory settings may be surfaced sooner. It results in a more stable and predictable
  # environment.
  xms: true
  args:
    - "-Dsun.net.inetaddr.ttl=60"
    - "-Djava.security.egd=file:/dev/./urandom"

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  # We won't define annotations here as they are defined in the helper template
  # annotations: {}
  # Define any extra annotations. Please keep cloud vendor specific annotations in the
  # helper template
  extraAnnotations: {}

ingress:
  enabled: true
  # Select the cloud vendor and GW/NLB controller being used.
  # Currently supported values are:
  # nginx-ingress - Nginx Ingress Controller (Network Load Balancer when using AWS Load Balancer Controller)
  # aws-lbc-alb - AWS Load Balancer Controller (Application Load Balancer)
  # azure-appgw - Azure Application Gateway
  # other - If it is none of the above, or unset, no default Ingress annotations will be created. It is up to
  #         you to provide appropriate `extraAnnotations` below
  type: nginx-ingress

  # This chart assumes certain default Ingress Class names. If your environment has defined a different class
  # name then override it here
  #ingressClassNameOverride:

  # We won't define annotations here as they are defined in the helper template
  # annotations: {}
  # Define any extra annotations. Please keep cloud vendor specific annotations in the
  # helper template
  extraAnnotations: {}

resources:
  # As SmileCDR is a high performance JVM based application, special consideration needs to be given
  # to the resource settings and JVM tuning parameters.
  # Typical cloud best practices are to start small and increase as workload increases. This can be a
  # challenging approach with an appliction such as SmileCDR if we are to avoid OOM Killed errors.
  # You should start with a somewhat larger memory allocation and perform monitoring of your workload
  # to determine real life memory usage. Then tune-down to a size that still gives a reasonable buffer
  # for any workload spikes.
  # These settings go hand-in-hand with the jvm.memoryFactor and jvm.xmx settings as any Java based
  # application will use more memory than specified in -Xmx alone. See the jvm section for more detail
  # on this.
  requests:
    cpu: "1"
    # As per best practice, limits.memory == requests.memory
    # This is easily achieved by only setting limits.memory
    # If requests.memory is undefined, Kubernetes sets it to match limits.memory
  limits:
    # As per best practice, do not set limits.cpu
    # This approach may be changed if there are per-core licensing concerns
    # Note that this will cause a warning in some linters
    memory: 4Gi


autoscaling:
  enabled: false
  # minReplicas: Recommend 1 for dev environments, 2 for prod or 3 for HA prod
  # Should always start a new installation with 1, so it's the default
  minReplicas: 1
  # Depends on peak workload requirements and available licensing
  maxReplicas: 4
  targetCPUUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
