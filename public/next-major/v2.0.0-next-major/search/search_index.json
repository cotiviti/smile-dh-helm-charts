{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is the documentation for the Smile CDR Helm Chart provided by Smile Digital Health.</p> <p>This Helm Chart is provided to simplify installation and configuration of Smile CDR on Kubernetes using a number of best practices.</p>"},{"location":"#required-helm-version","title":"Required Helm Version","text":"<p>To avoid unexpected issues, we recommend using the latest version of Helm v3.</p> <p>These Helm Charts have been developed and tested using Helm <code>&gt;=3.10.1, &lt;4.0.0</code>.</p> <p>Warning: Using older versions may cause backwards compatibility issues.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To quickly see these charts in action, follow the guide in the Quickstart section.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>Full details on configuring Smile CDR using this Helm Chart is provided in the User Guide section.</p>"},{"location":"charts/smilecdr/","title":"Smile CDR Helm Chart","text":"<p>A Helm Chart to install Smile CDR on Kubernetes</p>"},{"location":"charts/smilecdr/#releases","title":"Releases","text":"<p>Latest release: 2.0.0-next-major.1</p>"},{"location":"charts/smilecdr/CHANGELOG-PRE/","title":"CHANGELOG PRE","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre127-2024-06-28","title":"1.0.0-pre.127 (2024-06-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix database enablement comparison (82c8975)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre126-2024-06-28","title":"1.0.0-pre.126 (2024-06-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features","title":"Features","text":"<ul> <li>smilecdr: update Smile CDR version 2024.05.R03 (75dc81d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Default version of Smile CDR is updated to 2024.05.R03</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre125-2024-06-27","title":"1.0.0-pre.125 (2024-06-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_1","title":"Features","text":"<ul> <li>smilecdr: add support for extra secrets (0df7ce8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre124-2024-06-27","title":"1.0.0-pre.124 (2024-06-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_2","title":"Features","text":"<ul> <li>smilecdr: add IAM auth for RDS support (881ee35)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre123-2024-06-27","title":"1.0.0-pre.123 (2024-06-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_3","title":"Features","text":"<ul> <li>smilecdr: update Secrets mechanisms (158f2c3)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre122-2024-06-14","title":"1.0.0-pre.122 (2024-06-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>kafka: update kafka admin pod scripts (45a6136)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre121-2024-06-14","title":"1.0.0-pre.121 (2024-06-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_4","title":"Features","text":"<ul> <li>smilecdr: add support for tolerations (b02059b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre120-2024-05-28","title":"1.0.0-pre.120 (2024-05-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_5","title":"Features","text":"<ul> <li>smilecdr: add ingress TLS support (75b3955)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre119-2024-05-28","title":"1.0.0-pre.119 (2024-05-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_6","title":"Features","text":"<ul> <li>smilecdr: allow using existing certificate Issuers (106daf2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre118-2024-05-28","title":"1.0.0-pre.118 (2024-05-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_7","title":"Features","text":"<ul> <li>smilecdr: enable creation of ACME issuers (f44e770)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre117-2024-05-24","title":"1.0.0-pre.117 (2024-05-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>smilecdr: update keystore secret creation and naming (beb397f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre116-2024-05-24","title":"1.0.0-pre.116 (2024-05-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_8","title":"Features","text":"<ul> <li>smilecdr: allow global endpoint configurations (1fe5b0e)</li> <li>smilecdr: disable SNI verification with ALB (b196587)</li> <li>smilecdr: enforce TLSv1.3 encryption (49cd696)</li> <li>smilecdr: update default ELB security policy (0e49cc0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre115-2024-05-23","title":"1.0.0-pre.115 (2024-05-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_9","title":"Features","text":"<ul> <li>smilecdr: add service annotations for ALB (1d9b82b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre114-2024-05-21","title":"1.0.0-pre.114 (2024-05-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_10","title":"Features","text":"<ul> <li>smilecdr: allow configurable pod topology (349bf32)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre113-2024-05-17","title":"1.0.0-pre.113 (2024-05-17)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>kafka: fix Kafka Admin pod IAM auth (6a84d77)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre112-2024-05-16","title":"1.0.0-pre.112 (2024-05-16)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_11","title":"Features","text":"<ul> <li>smilecdr: add cert-manager support (17e3be5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre111-2024-04-11","title":"1.0.0-pre.111 (2024-04-11)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>smilecdr: auto-set s3 cp recursive option (045463d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_1","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: - If you currently specify a source path without a trailing slash, it will no longer try to recursively copy the files.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre110-2024-03-13","title":"1.0.0-pre.110 (2024-03-13)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_12","title":"Features","text":"<ul> <li>pgo: add more configurability to crunchydata PG CRD (91591e0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre109-2024-03-05","title":"1.0.0-pre.109 (2024-03-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix validate topic logic (c08e65c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre108-2024-03-04","title":"1.0.0-pre.108 (2024-03-04)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix SSCI resource naming (0e926f5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre107-2024-03-03","title":"1.0.0-pre.107 (2024-03-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>smilecdr: update Strimzi schema (3b65726)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre106-2024-02-15","title":"1.0.0-pre.106 (2024-02-15)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix quoting of ingress annotations (7252dff)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre105-2024-02-14","title":"1.0.0-pre.105 (2024-02-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_13","title":"Features","text":"<ul> <li>smilecdr: add configurable logging (20624ec)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre104-2024-02-13","title":"1.0.0-pre.104 (2024-02-13)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_14","title":"Features","text":"<ul> <li>smilecdr: add support for multiple ingresses (91485d6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre103-2023-12-08","title":"1.0.0-pre.103 (2023-12-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_15","title":"Features","text":"<ul> <li>smilecdr: allow custom startup probe timings (5bfabc5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre102-2023-11-29","title":"1.0.0-pre.102 (2023-11-29)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>smilecdr: improve handling of issuer.url (9f0976c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre101-2023-11-28","title":"1.0.0-pre.101 (2023-11-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_16","title":"Features","text":"<ul> <li>smilecdr: add AMQ support (f724142)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre100-2023-11-28","title":"1.0.0-pre.100 (2023-11-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_17","title":"Features","text":"<ul> <li>smilecdr: add Kafka password auth (bc805fa)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre99-2023-11-20","title":"1.0.0-pre.99 (2023-11-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix route rendering for AppSphere (903b00e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre98-2023-11-07","title":"1.0.0-pre.98 (2023-11-07)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>smilecdr: Remove duplicate env vars (8284743)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre97-2023-11-03","title":"1.0.0-pre.97 (2023-11-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_18","title":"Features","text":"<ul> <li>smilecdr: Add instrumentation support (2583c81)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre96-2023-09-20","title":"1.0.0-pre.96 (2023-09-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>smilecdr: Fix IAM Jar version in Kafka admin (f5048f1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre95-2023-09-19","title":"1.0.0-pre.95 (2023-09-19)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_19","title":"Features","text":"<ul> <li>smilecdr: allow custom image repos (033825a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre94-2023-09-07","title":"1.0.0-pre.94 (2023-09-07)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>smilecdr: multiple init-pull containers (80ced73)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre93-2023-08-30","title":"1.0.0-pre.93 (2023-08-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_20","title":"Features","text":"<ul> <li>smilecdr: add multi-node configuration (29848e7)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre92-2023-08-28","title":"1.0.0-pre.92 (2023-08-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_21","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.08.R01 (d3b33f1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre91-2023-08-25","title":"1.0.0-pre.91 (2023-08-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_22","title":"Features","text":"<ul> <li>smilecdr: add db suffix configuration (ddf3b63)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre90-2023-08-25","title":"1.0.0-pre.90 (2023-08-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>smilecdr: add error checking for mapped files (95b82d0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre89-2023-08-19","title":"1.0.0-pre.89 (2023-08-19)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>smilecdr: add config sanity checks (eea5ae6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre88-2023-08-18","title":"1.0.0-pre.88 (2023-08-18)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow hierarchical config (f664d5e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre87-2023-07-27","title":"1.0.0-pre.87 (2023-07-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow specifying db name (4d4cd84)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre86-2023-06-27","title":"1.0.0-pre.86 (2023-06-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix base_url for hybrid provider (e80367d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre85-2023-06-26","title":"1.0.0-pre.85 (2023-06-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>smilecdr: update default modules (f83ff90)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre84-2023-06-26","title":"1.0.0-pre.84 (2023-06-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_23","title":"Features","text":"<ul> <li>smilecdr: make readiness probe configurable (e83a98e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre83-2023-06-20","title":"1.0.0-pre.83 (2023-06-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_24","title":"Features","text":"<ul> <li>smilecdr: allow disabling of module ingress (ca3011b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre82-2023-06-09","title":"1.0.0-pre.82 (2023-06-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>smilecdr: update transaction module name (f4472ca)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre81-2023-06-08","title":"1.0.0-pre.81 (2023-06-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_25","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R02 (e7362b8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre80-2023-06-06","title":"1.0.0-pre.80 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_26","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R01 (99aa74d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre79-2023-06-06","title":"1.0.0-pre.79 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_27","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R01 (bf89b79)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre78-2023-06-06","title":"1.0.0-pre.78 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix k8s resource labels (6378fe0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre77-2023-06-06","title":"1.0.0-pre.77 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix disableAutoJarCopy option (03d5163)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre76-2023-06-06","title":"1.0.0-pre.76 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_23","title":"Bug Fixes","text":"<ul> <li>smilecdr: update uid for curl images (a7cb4de)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre75-2023-05-08","title":"1.0.0-pre.75 (2023-05-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_24","title":"Bug Fixes","text":"<ul> <li>common: remove <code>chart.shortname</code> template (f7a0a8b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre74-2023-05-04","title":"1.0.0-pre.74 (2023-05-04)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_25","title":"Bug Fixes","text":"<ul> <li>pmp: add per-component imagePullSecrets logic (880c74e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre73-2023-05-04","title":"1.0.0-pre.73 (2023-05-04)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_28","title":"Features","text":"<ul> <li>pmp: add pmp chart (d52cc0c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre72-2023-05-02","title":"1.0.0-pre.72 (2023-05-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_29","title":"Features","text":"<ul> <li>pmp: add pmp-keycloak chart (10541a4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre71-2023-05-01","title":"1.0.0-pre.71 (2023-05-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_30","title":"Features","text":"<ul> <li>pmp: add pmp-directus chart (f8c0b1e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre70-2023-05-01","title":"1.0.0-pre.70 (2023-05-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_31","title":"Features","text":"<ul> <li>common: add Smile DH common library chart (46a3e67)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre69-2023-04-05","title":"1.0.0-pre.69 (2023-04-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_26","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix s3 copy for customerlib (66ea90a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre68-2023-04-05","title":"1.0.0-pre.68 (2023-04-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_27","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix s3 copy with readonly rootfs (f7a6a12)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre67-2023-03-28","title":"1.0.0-pre.67 (2023-03-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_28","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix kafka configs with IAM auth (170a14c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre66-2023-03-27","title":"1.0.0-pre.66 (2023-03-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_29","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix value for grace period (54686f7)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre65-2023-03-23","title":"1.0.0-pre.65 (2023-03-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_30","title":"Bug Fixes","text":"<ul> <li>smilecdr: add preStop delay (2d31c8c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre64-2023-03-22","title":"1.0.0-pre.64 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_32","title":"Features","text":"<ul> <li>smilecdr: add database properties mode (bb0b614)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre63-2023-03-22","title":"1.0.0-pre.63 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_31","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix licence module settings (ceab69d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre62-2023-03-22","title":"1.0.0-pre.62 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_32","title":"Bug Fixes","text":"<ul> <li>smilecdr: use correct labels for Kafka admin (fce2935)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre61-2023-03-15","title":"1.0.0-pre.61 (2023-03-15)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_33","title":"Features","text":"<ul> <li>smilecdr: add HL7v2 support (05d5579)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre60-2023-03-08","title":"1.0.0-pre.60 (2023-03-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_34","title":"Features","text":"<ul> <li>smilecdr: update to latest Smile CDR version (db1caae)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre59-2023-03-07","title":"1.0.0-pre.59 (2023-03-07)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_35","title":"Features","text":"<ul> <li>smilecdr: add Kafka admin pod (c9f0493)</li> <li>smilecdr: rework Kafka configuration (d27a00b)</li> <li>smilecdr: update consumer properties (c1978e5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_2","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: This affects the default consumer properties configured in Smile CDR.</li> <li>smilecdr: Existing Kafka/Strimzi configurations have changed. As they were previously untested, the required changes may be unpredictable. Please refer to the docs to configure Kafka.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre58-2023-02-28","title":"1.0.0-pre.58 (2023-02-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_33","title":"Bug Fixes","text":"<ul> <li>smilecdr: update initContainer configurations (c974af5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre57-2023-02-27","title":"1.0.0-pre.57 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_36","title":"Features","text":"<ul> <li>smilecdr: improve secrets error handling (8df9476)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre56-2023-02-27","title":"1.0.0-pre.56 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_37","title":"Features","text":"<ul> <li>smilecdr: improve warnings for chart errors (b50e54e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre55-2023-02-27","title":"1.0.0-pre.55 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_38","title":"Features","text":"<ul> <li>smilecdr: refactor image pull secrets (c7d376c), closes #78</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_3","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Deprecation warning - Values files must be updated to use <code>image.imagePullSecrets</code> instead of <code>image.credentials</code>.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre54-2023-02-27","title":"1.0.0-pre.54 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_34","title":"Bug Fixes","text":"<ul> <li>smilecdr: follow redirects for curl (698d045)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre53-2023-02-26","title":"1.0.0-pre.53 (2023-02-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_39","title":"Features","text":"<ul> <li>smilecdr: add init-sync for customerlib (099cb57)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre52-2023-02-22","title":"1.0.0-pre.52 (2023-02-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_40","title":"Features","text":"<ul> <li>smilecdr: update to latest Smile CDR version (d9c0240)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre51-2023-02-21","title":"1.0.0-pre.51 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_41","title":"Features","text":"<ul> <li>smilecdr: improve readiness probe definition (25273a9)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre50-2023-02-21","title":"1.0.0-pre.50 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_42","title":"Features","text":"<ul> <li>smilecdr: allow extra env vars and volumes (9d53ec8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre49-2023-02-21","title":"1.0.0-pre.49 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_35","title":"Bug Fixes","text":"<ul> <li>smilecdr: use provided tag for initcontainer (2f68eb6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre48-2023-02-21","title":"1.0.0-pre.48 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_43","title":"Features","text":"<ul> <li>smilecdr: update k8s secrets mechanism (0733b5d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre47-2023-02-21","title":"1.0.0-pre.47 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_36","title":"Bug Fixes","text":"<ul> <li>smilecdr: correct spelling of license (25dd99a)</li> <li>smilecdr: use camelCase for <code>useDefaultModules</code> (a0178ac)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_4","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: This change affects the default module configuration.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre46-2023-02-21","title":"1.0.0-pre.46 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_37","title":"Bug Fixes","text":"<ul> <li>smilecdr: remove short-circuit dependency (9af24c2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre45-2023-02-09","title":"1.0.0-pre.45 (2023-02-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_44","title":"Features","text":"<ul> <li>smilecdr: add license support (3c429d8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre44-2023-02-09","title":"1.0.0-pre.44 (2023-02-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_38","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix paths for initcontainer (ce17551)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre43-2023-01-30","title":"1.0.0-pre.43 (2023-01-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_45","title":"Features","text":"<ul> <li>smilecdr: add support for 2023.02 release (7c583de)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre42-2023-01-30","title":"1.0.0-pre.42 (2023-01-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_46","title":"Features","text":"<ul> <li>smilecdr: add config locking options (351d48d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre41-2023-01-28","title":"1.0.0-pre.41 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_47","title":"Features","text":"<ul> <li>smilecdr: add support for Alpine3 base image (dc7e960)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre40-2023-01-28","title":"1.0.0-pre.40 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_39","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ingress annotation overrides (78d254e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre39-2023-01-27","title":"1.0.0-pre.39 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_40","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in env vars (81defae)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre38-2023-01-27","title":"1.0.0-pre.38 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_41","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in k8s secret (b84760a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre37-2023-01-26","title":"1.0.0-pre.37 (2023-01-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_42","title":"Bug Fixes","text":"<ul> <li>smilecdr: change objectAlias naming logic (ade3f22)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre36-2023-01-11","title":"1.0.0-pre.36 (2023-01-11)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_48","title":"Features","text":"<ul> <li>smilecdr: update ingress logic and docs (affff39)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre35-2023-01-06","title":"1.0.0-pre.35 (2023-01-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_49","title":"Features","text":"<ul> <li>smilecdr: copy files from external location (ea2710f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre34-2022-12-22","title":"1.0.0-pre.34 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_43","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ConfigMap reference in volume (5cc81e8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre33-2022-12-22","title":"1.0.0-pre.33 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_44","title":"Bug Fixes","text":"<ul> <li>smilecdr: force lower case in resource names (6d1e4aa)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre32-2022-12-20","title":"1.0.0-pre.32 (2022-12-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_50","title":"Features","text":"<ul> <li>smilecdr: change db secret config (8069b77)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_5","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Values file needs to be updated if using sscsi for DB secrets</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre31-2022-12-14","title":"1.0.0-pre.31 (2022-12-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_51","title":"Features","text":"<ul> <li>smilecdr: disable crunchypgo (a2a4e32)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre30-2022-12-10","title":"1.0.0-pre.30 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_52","title":"Features","text":"<ul> <li>smilecdr: add argocd feature (75a4874)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre29-2022-12-10","title":"1.0.0-pre.29 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_45","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix secret reference keys (2f6411b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-06","title":"1.0.0-pre.28 (2022-12-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_53","title":"Features","text":"<ul> <li>smilecdr: make image secret required (e068330)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-05","title":"1.0.0-pre.28 (2022-12-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_54","title":"Features","text":"<ul> <li>smilecdr: make image secret required (2c2389b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre27-2022-12-03","title":"1.0.0-pre.27 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_46","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix DB_PORT in default modules (2239f60)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre26-2022-12-03","title":"1.0.0-pre.26 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_47","title":"Bug Fixes","text":"<ul> <li>smilecdr: correct the field for image secrets (2ddeef2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre25-2022-12-02","title":"1.0.0-pre.25 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_55","title":"Features","text":"<ul> <li>smilecdr: support multiple databases (e28bb13)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre24-2022-12-02","title":"1.0.0-pre.24 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_48","title":"Bug Fixes","text":"<ul> <li>smilecdr: improve modules include logic (f7850d2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre23-2022-12-02","title":"1.0.0-pre.23 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_49","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow quoted config entries (f9d7e1f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre22-2022-12-01","title":"1.0.0-pre.22 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_50","title":"Bug Fixes","text":"<ul> <li>smilecdr: change crunchydata resource names (654417b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_56","title":"Features","text":"<ul> <li>smilecdr: improve CrunchyData integration (4289432)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre21-2022-12-01","title":"1.0.0-pre.21 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_57","title":"Features","text":"<ul> <li>smilecdr: update JVM tuning params (cc1f859)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre20-2022-12-01","title":"1.0.0-pre.20 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_51","title":"Bug Fixes","text":"<ul> <li>smilecdr: remove image reference (2b47fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre19-2022-12-01","title":"1.0.0-pre.19 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_58","title":"Features","text":"<ul> <li>smilecdr: add support for strimzi kafka (27b8fb4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre18-2022-12-01","title":"1.0.0-pre.18 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_59","title":"Features","text":"<ul> <li>smilecdr: add autoscaling support (6ff0f2f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre17-2022-12-01","title":"1.0.0-pre.17 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_60","title":"Features","text":"<ul> <li>smilecdr: configure rolling deployments (daa9c4b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre16-2022-12-01","title":"1.0.0-pre.16 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_61","title":"Features","text":"<ul> <li>smilecdr: add pod disruption budget (b8b46e4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre15-2022-12-01","title":"1.0.0-pre.15 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_62","title":"Features","text":"<ul> <li>smilecdr: add redeploy on config changes (90e33ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre14-2022-11-30","title":"1.0.0-pre.14 (2022-11-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_63","title":"Features","text":"<ul> <li>smilecdr: add readiness probe (5a03100)</li> <li>smilecdr: add startup probe (65014ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre13-2022-11-25","title":"1.0.0-pre.13 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_64","title":"Features","text":"<ul> <li>smilecdr: update CDR version and modules (116e187)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_6","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: - This updates the Smile CDR version</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre12-2022-11-25","title":"1.0.0-pre.12 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_65","title":"Features","text":"<ul> <li>smilecdr: add support for deploying postgres (0fc81b1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre11-2022-11-24","title":"1.0.0-pre.11 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_52","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix reference to configmap data (383fdd6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre10-2022-11-24","title":"1.0.0-pre.10 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_66","title":"Features","text":"<ul> <li>smilecdr: support injecting files (a823a7d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre9-2022-11-23","title":"1.0.0-pre.9 (2022-11-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_67","title":"Features","text":"<ul> <li>smilecdr: automate setting JVMARGS (6fcda8b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre8-2022-11-22","title":"1.0.0-pre.8 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_68","title":"Features","text":"<ul> <li>smilecdr: set default replicas to 1 (9788832)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre7-2022-11-22","title":"1.0.0-pre.7 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_69","title":"Features","text":"<ul> <li>smilecdr: add name override for CrunchyPGO (110e133)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre6-2022-11-22","title":"1.0.0-pre.6 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_70","title":"Features","text":"<ul> <li>smilecdr: add Secrets Store CSI support (f8f23ba)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre5-2022-11-21","title":"1.0.0-pre.5 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_71","title":"Features","text":"<ul> <li>smilecdr: add support for IRSA (IAM roles) (509bbe3)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre4-2022-11-21","title":"1.0.0-pre.4 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_72","title":"Features","text":"<ul> <li>smilecdr: update Ingress definition logic (2271d57)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_7","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Now uses <code>nginx-ingress</code> instead of <code>aws-lbc-nlb</code> for specifying Nginx Ingress Controller</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre3-2022-11-21","title":"1.0.0-pre.3 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_73","title":"Features","text":"<ul> <li>smilecdr: add back default tag functionality (46785e5)</li> <li>smilecdr: add common labels to all resources (618ba2e)</li> <li>smilecdr: normalize resource names (6ca25bd)</li> <li>smilecdr: remove extra labels from default values (d971934)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre2-2022-11-21","title":"1.0.0-pre.2 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_74","title":"Features","text":"<ul> <li>smilecdr: remove hard coded entries from ConfigMap (b6a2fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre1-2022-11-21","title":"1.0.0-pre.1 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_75","title":"Features","text":"<ul> <li>repo: initial Commit (5f98460), closes #68834381</li> <li>smilecdr: add external module files support (da374c1)</li> <li>smilecdr: update application version (9b203f2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/","title":"Version 1.x","text":""},{"location":"charts/smilecdr/CHANGELOG-V1/#111-2024-08-15","title":"1.1.1 (2024-08-15)","text":""},{"location":"charts/smilecdr/CHANGELOG-V1/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix environment variable aggregation logic (089163d)</li> <li>smilecdr: update Smile CDR patch level to <code>2024.05.R04</code> (554eea2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/#110-2024-08-08","title":"1.1.0 (2024-08-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-V1/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix smileutil command (c7efcbe)</li> <li>smilecdr: set java.io.tmpdir to correct location (7b85499)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/#features","title":"Features","text":"<ul> <li>smilecdr: allow ephemeral volume configuration (1f74351)</li> <li>smilecdr: update volumeConfig spec (29390f7)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/#100-2024-07-03","title":"1.0.0 (2024-07-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-V1/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>common: remove <code>chart.shortname</code> template (f7a0a8b)</li> <li>kafka: fix Kafka Admin pod IAM auth (6a84d77)</li> <li>kafka: update kafka admin pod scripts (45a6136)</li> <li>pmp: add per-component imagePullSecrets logic (880c74e)</li> <li>smilecdr: add config sanity checks (eea5ae6)</li> <li>smilecdr: add error checking for mapped files (95b82d0)</li> <li>smilecdr: add preStop delay (2d31c8c)</li> <li>smilecdr: allow hierarchical config (f664d5e)</li> <li>smilecdr: allow quoted config entries (201d69b)</li> <li>smilecdr: allow specifying db name (4d4cd84)</li> <li>smilecdr: auto-set s3 cp recursive option (045463d)</li> <li>smilecdr: change crunchydata resource names (ad7ee34)</li> <li>smilecdr: change objectAlias naming logic (ade3f22)</li> <li>smilecdr: correct spelling of license (25dd99a)</li> <li>smilecdr: correct the field for image secrets (ff7c819)</li> <li>smilecdr: fix base_url for hybrid provider (e80367d)</li> <li>smilecdr: fix ConfigMap reference in volume (5cc81e8)</li> <li>smilecdr: fix database enablement comparison (82c8975)</li> <li>smilecdr: fix DB_PORT in default modules (ac5b592)</li> <li>smilecdr: fix disableAutoJarCopy option (03d5163)</li> <li>smilecdr: Fix IAM Jar version in Kafka admin (f5048f1)</li> <li>smilecdr: fix ingress annotation overrides (78d254e)</li> <li>smilecdr: fix k8s resource labels (6378fe0)</li> <li>smilecdr: fix kafka configs with IAM auth (170a14c)</li> <li>smilecdr: fix key names in env vars (81defae)</li> <li>smilecdr: fix key names in k8s secret (b84760a)</li> <li>smilecdr: fix licence module settings (ceab69d)</li> <li>smilecdr: fix paths for initcontainer (ce17551)</li> <li>smilecdr: fix quoting of ingress annotations (7252dff)</li> <li>smilecdr: fix reference to configmap data (8a61d36)</li> <li>smilecdr: fix route rendering for AppSphere (903b00e)</li> <li>smilecdr: fix s3 copy for customerlib (66ea90a)</li> <li>smilecdr: fix s3 copy with readonly rootfs (f7a6a12)</li> <li>smilecdr: fix secret reference keys (2f6411b)</li> <li>smilecdr: fix SSCI resource naming (0e926f5)</li> <li>smilecdr: fix validate topic logic (c08e65c)</li> <li>smilecdr: fix value for grace period (54686f7)</li> <li>smilecdr: follow redirects for curl (698d045)</li> <li>smilecdr: force lower case in resource names (6d1e4aa)</li> <li>smilecdr: improve handling of issuer.url (9f0976c)</li> <li>smilecdr: improve modules include logic (7bfad46)</li> <li>smilecdr: multiple init-pull containers (80ced73)</li> <li>smilecdr: Remove duplicate env vars (8284743)</li> <li>smilecdr: remove image reference (54ff0b6)</li> <li>smilecdr: remove short-circuit dependency (9af24c2)</li> <li>smilecdr: update default modules (f83ff90)</li> <li>smilecdr: update initContainer configurations (c974af5)</li> <li>smilecdr: update keystore secret creation and naming (beb397f)</li> <li>smilecdr: update Strimzi schema (3b65726)</li> <li>smilecdr: update transaction module name (f4472ca)</li> <li>smilecdr: update uid for curl images (a7cb4de)</li> <li>smilecdr: use camelCase for <code>useDefaultModules</code> (a0178ac)</li> <li>smilecdr: use correct labels for Kafka admin (fce2935)</li> <li>smilecdr: use provided tag for initcontainer (2f68eb6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/#features_1","title":"Features","text":"<ul> <li>common: add Smile DH common library chart (46a3e67)</li> <li>pgo: add more configurability to crunchydata PG CRD (91591e0)</li> <li>pmp: add pmp chart (d52cc0c)</li> <li>pmp: add pmp-directus chart (f8c0b1e)</li> <li>pmp: add pmp-keycloak chart (10541a4)</li> <li>repo: initial Commit (0abcb74), closes #68834381</li> <li>smilecdr: add AMQ support (f724142)</li> <li>smilecdr: add argocd feature (75a4874)</li> <li>smilecdr: add autoscaling support (e63838d)</li> <li>smilecdr: add back default tag functionality (270a8e4)</li> <li>smilecdr: add cert-manager support (17e3be5)</li> <li>smilecdr: add common labels to all resources (7e2a45c)</li> <li>smilecdr: add config locking options (351d48d)</li> <li>smilecdr: add configurable logging (20624ec)</li> <li>smilecdr: add database properties mode (bb0b614)</li> <li>smilecdr: add db suffix configuration (ddf3b63)</li> <li>smilecdr: add external module files support (46fe6a5)</li> <li>smilecdr: add HL7v2 support (05d5579)</li> <li>smilecdr: add IAM auth for RDS support (881ee35)</li> <li>smilecdr: add ingress TLS support (75b3955)</li> <li>smilecdr: add init-sync for customerlib (099cb57)</li> <li>smilecdr: Add instrumentation support (2583c81)</li> <li>smilecdr: add Kafka admin pod (c9f0493)</li> <li>smilecdr: add Kafka password auth (bc805fa)</li> <li>smilecdr: add license support (3c429d8)</li> <li>smilecdr: add multi-node configuration (29848e7)</li> <li>smilecdr: add name override for CrunchyPGO (0ba704c)</li> <li>smilecdr: add pod disruption budget (88f80ad)</li> <li>smilecdr: add readiness probe (ea11897)</li> <li>smilecdr: add redeploy on config changes (9add457)</li> <li>smilecdr: add Secrets Store CSI support (ca9318d)</li> <li>smilecdr: add service annotations for ALB (1d9b82b)</li> <li>smilecdr: add startup probe (9697849)</li> <li>smilecdr: add support for 2023.02 release (7c583de)</li> <li>smilecdr: add support for Alpine3 base image (dc7e960)</li> <li>smilecdr: add support for deploying postgres (db8004b)</li> <li>smilecdr: add support for extra secrets (0df7ce8)</li> <li>smilecdr: add support for IRSA (IAM roles) (120a0e5)</li> <li>smilecdr: add support for multiple ingresses (91485d6)</li> <li>smilecdr: add support for strimzi kafka (87d7ccc)</li> <li>smilecdr: add support for tolerations (b02059b)</li> <li>smilecdr: allow configurable pod topology (349bf32)</li> <li>smilecdr: allow custom image repos (033825a)</li> <li>smilecdr: allow custom startup probe timings (5bfabc5)</li> <li>smilecdr: allow disabling of module ingress (ca3011b)</li> <li>smilecdr: allow extra env vars and volumes (9d53ec8)</li> <li>smilecdr: allow global endpoint configurations (1fe5b0e)</li> <li>smilecdr: allow using existing certificate Issuers (106daf2)</li> <li>smilecdr: automate setting JVMARGS (3d82f5c)</li> <li>smilecdr: change db secret config (8069b77)</li> <li>smilecdr: configure rolling deployments (8acd271)</li> <li>smilecdr: copy files from external location (ea2710f)</li> <li>smilecdr: disable crunchypgo (a2a4e32)</li> <li>smilecdr: disable SNI verification with ALB (b196587)</li> <li>smilecdr: enable creation of ACME issuers (f44e770)</li> <li>smilecdr: enforce TLSv1.3 encryption (49cd696)</li> <li>smilecdr: improve CrunchyData integration (85d7da5)</li> <li>smilecdr: improve readiness probe definition (25273a9)</li> <li>smilecdr: improve secrets error handling (8df9476)</li> <li>smilecdr: improve warnings for chart errors (b50e54e)</li> <li>smilecdr: make image secret required (e068330)</li> <li>smilecdr: make readiness probe configurable (e83a98e)</li> <li>smilecdr: normalize resource names (98020a7)</li> <li>smilecdr: refactor image pull secrets (c7d376c), closes #78</li> <li>smilecdr: remove extra labels from default values (8110939)</li> <li>smilecdr: remove hard coded entries from ConfigMap (7d24665)</li> <li>smilecdr: rework Kafka configuration (d27a00b)</li> <li>smilecdr: set default replicas to 1 (bc74a1f)</li> <li>smilecdr: support injecting files (a02cd3f)</li> <li>smilecdr: support multiple databases (8b0ee32)</li> <li>smilecdr: update application version (8bd06b4)</li> <li>smilecdr: update CDR version and modules (862d4da)</li> <li>smilecdr: update consumer properties (c1978e5)</li> <li>smilecdr: update default ELB security policy (0e49cc0)</li> <li>smilecdr: update Ingress definition logic (afed28b)</li> <li>smilecdr: update ingress logic and docs (affff39)</li> <li>smilecdr: update JVM tuning params (b0c746b)</li> <li>smilecdr: update k8s secrets mechanism (0733b5d)</li> <li>smilecdr: update Secrets mechanisms (158f2c3)</li> <li>smilecdr: update Smile CDR version 2024.05.R03 (75dc81d)</li> <li>smilecdr: update to latest Smile CDR version (db1caae)</li> <li>smilecdr: update to latest Smile CDR version (d9c0240)</li> <li>smilecdr: Update to Smile CDR 2023.05.R01 (99aa74d)</li> <li>smilecdr: Update to Smile CDR 2023.05.R01 (bf89b79)</li> <li>smilecdr: Update to Smile CDR 2023.05.R02 (e7362b8)</li> <li>smilecdr: Update to Smile CDR 2023.08.R01 (d3b33f1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V1/#breaking-changes","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Default version of Smile CDR is updated to 2024.05.R03</li> <li>smilecdr: - If you currently specify a source path without a trailing slash, it will no longer try to recursively copy the files.</li> <li>smilecdr: This affects the default consumer properties configured in Smile CDR.</li> <li>smilecdr: Existing Kafka/Strimzi configurations have changed. As they were previously untested, the required changes may be unpredictable. Please refer to the docs to configure Kafka.</li> <li>smilecdr: Deprecation warning - Values files must be updated to use <code>image.imagePullSecrets</code> instead of <code>image.credentials</code>.</li> <li>smilecdr: This change affects the default module configuration.</li> <li>smilecdr: Values file needs to be updated if using sscsi for DB secrets</li> <li>smilecdr: - This updates the SmileCDR version</li> <li>smilecdr: Now uses <code>nginx-ingress</code> instead of <code>aws-lbc-nlb</code> for specifying Nginx Ingress Controller</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2-next-major/","title":"CHANGELOG V2 next major","text":""},{"location":"charts/smilecdr/CHANGELOG-V2-next-major/#200-next-major1-2024-08-27","title":"2.0.0-next-major.1 (2024-08-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-V2-next-major/#breaking-changes","title":"\u26a0 BREAKING CHANGES","text":"<ul> <li>smilecdr: Default admin_json context root changed from <code>json-admin</code> to <code>admin_json</code></li> <li>smilecdr: Default version of Smile CDR changed to <code>2024.08.R01</code></li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2-next-major/#features","title":"Features","text":"<ul> <li>smilecdr: update default context path for admin json module (a9a69a9)</li> <li>smilecdr: update Smile CDR to <code>2024.08.R01</code> (830b4ca)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2-next-major/#reverts","title":"Reverts","text":"<ul> <li>sdh-common: remove common sub-chart (aa98d66)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2/","title":"Version 2.x","text":""},{"location":"charts/smilecdr/CHANGELOG-V2/#200-next-major1-2024-08-27","title":"2.0.0-next-major.1 (2024-08-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-V2/#breaking-changes","title":"\u26a0 BREAKING CHANGES","text":"<ul> <li>smilecdr: Default admin_json context root changed from <code>json-admin</code> to <code>admin_json</code></li> <li>smilecdr: Default version of Smile CDR changed to <code>2024.08.R01</code></li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2/#features","title":"Features","text":"<ul> <li>smilecdr: update default context path for admin json module (a9a69a9)</li> <li>smilecdr: update Smile CDR to <code>2024.08.R01</code> (830b4ca)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-V2/#reverts","title":"Reverts","text":"<ul> <li>sdh-common: remove common sub-chart (aa98d66)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/","title":"Smile CDR","text":"<p>The Smile CDR Helm Chart provides a flexible and consistent method to deploy Smile CDR on a Kubernetes cluster.</p> <p>It is provided by Smile Digital Health to help reduce the effort and complexity of installing Smile CDR on Kubernetes. It has been well tested on Amazon EKS and has growing compatibility for Azure AKS.</p>"},{"location":"charts/smilecdr/smilecdr/#feature-matrix","title":"Feature Matrix","text":"<p>The Smile CDR Helm Chart supports a number of features to help you install Smile CDR in a secure, reliable, cost effective and scalable manner with operational efficiency in mind.</p>"},{"location":"charts/smilecdr/smilecdr/#application-features","title":"Application Features","text":"<p>This following table shows you the Smile CDR features that are currently supported by this Helm Chart \"out-of-the-box\", which platform (AWS EKS / Azure AKS) they are supported on and the required Smile CDR and Helm Chart versions:</p> Smile CDR Feature EKS AKS Notes Smile CDR Version Helm Chart Version Install Smile CDR <code>2023.08</code> Smile CDR <code>2023.05</code> is the minimum supported version.Helm Install Guide <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Minor version upgrades Upgrade by overriding image tag.Smile CDR Upgrades <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Flexible CDR Node cluster configurations Configuration for single-node or multi-node Smile CDR cluster designs. Cluster Configuration <code>2023.08.R01</code> <code>v1.0.0-pre93</code> Cluster Scaling Horizontal Pod Autoscaling may be enabled. You need sufficient licenced core allocation if using autoscaling.Smile CDR Scaling <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Configuration of CDR Modules All modules can be configured and updated with zero downtime.Module Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Postgres Database Supports multiple databases. i.e. for Clustermgr, Persistence, Audit etc.Database Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> JVM Tuning Resource Tuning using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Kafka Message Broker Message Broker Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Add files to <code>classpath</code> or <code>customerlib</code> Including Files using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> HL7 v2.x with <code>HL7_OVER_HTTP</code> Configuring HL7 v2.x Endpoint using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> License Module Configuring License using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> <p>The following Smile CDR features are not currently supported:</p> Smile CDR Feature Notes GitLab Issue Install Smile CDR <code>2023.02</code> and lower Core module configuration changes were made in <code>2023.05.R01</code>, so this Helm Chart does not officially support lower versions. See CDR Versions section for more info NA Zero Downtime Upgrades Support planned to be added. See Smile CDR Docs for info on Zero Downtime Upgrades here GitLab Issue Pre-Seeding Users Smile CDR Docs GitLab Issue Pre-Seeding OIDC Servers Smile CDR Docs GitLab Issue Pre-Seeding OIDC Clients Smile CDR Docs GitLab Issue Pre-Seeding OIDC Keystores Smile CDR Docs GitLab Issue Pre-Seeding Packages Smile CDR Docs GitLab Issue Pre-Seeding FHIR Resources Smile CDR Docs GitLab Issue IAM Auth for RDS Databases Support for IAM database auth to be added GitLab Issue MSSQL Databases Support for MS SQL databases to be added GitLab Issue Oracle Databases Support for Oracle databases to be added GitLab Issue MongoDB Databases Support for MongoDB databases to be added GitLab Issue ActiveMQ Message Broker Support not currently planned NA"},{"location":"charts/smilecdr/smilecdr/#infrastructure-features","title":"Infrastructure Features","text":""},{"location":"charts/smilecdr/smilecdr/#app-networking","title":"App Networking","text":"<ul> <li>Automatic configuration of Kubernetes Services and Ingresses</li> <li>Coming soon...<ul> <li>Network Policies</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#ingress","title":"Ingress","text":"<ul> <li>TLS termination at load Balancer</li> <li>Nginx Ingress Controller</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> <li>Coming soon...<ul> <li>Multiple Ingress (e.g. internal and external, for different modules)</li> <li>End-to-end TLS encryption</li> <li>Support for other ingress controllers may be implemented as required</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#dependency-provisioning","title":"Dependency Provisioning","text":"<p>You can use this chart to configure and automatically deploy the following components. If enabled, they will automatically be configured in a production-like configuration, although we do not recommend using them in production environments at this time.</p> <ul> <li>Postgres Database - Uses the CrunchyData Postgres Operator</li> <li>Kafka Cluster - Uses the Strimzi Kafka Operator</li> <li>Coming soon...<ul> <li>MongoDB</li> </ul> </li> </ul> <p>With these components installed in your Kubernetes cluster, you can provision an entire Smile CDR stack, complete with persistent backed-up database and a Kafka cluster in about 5-10 mins. May take longer if your K8s cluster needs to autoscale to create more worker nodes first.</p>"},{"location":"charts/smilecdr/smilecdr/#security-features","title":"Security Features","text":"<p>It's no good having an easy to use Helm Chart if you cannot use it in a secure manner. As such, we have included the following features when running on Amazon EKS (Other providers to follow):</p> <ul> <li>IAM roles for service accounts(AWS Only) -   Smile CDR pods run with their own IAM role, independent and isolated from other workloads on the cluster.</li> <li>Secrets Store CSI Driver - Store secrets in a secure vault, and not in your code.<ul> <li>AWS SSCSI Provider - (Uses AWS Secrets Manager)</li> </ul> </li> <li>NEW!!! Enhanced pod security<ul> <li>Pods run as non-root, non-privileged</li> <li>Privilege escalation disabled</li> <li>Read-only root filesystem</li> <li>All container security capabilities disabled</li> </ul> </li> <li>Coming soon...<ul> <li>Execution role support in other cloud providers</li> <li>Support for other SSCSI providers</li> <li>Pod Security Policies</li> <li>Security Groups For Pods</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#reliability-features","title":"Reliability Features","text":"<ul> <li>High availability when running 2 or more Pods</li> <li>Self healing - Failed pods get restarted</li> <li>Pod disruption budgets (Prevents accidental outages)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/#operational-efficiency-features","title":"Operational Efficiency Features","text":"<ul> <li>Zero-downtime configuration changes (Using rolling deployments)</li> <li>Horizontal Auto-Scaling (Within bounds of Smile CDR licence) - to ensure cost effective use of compute resources</li> <li>Coming soon...<ul> <li>Zero-downtime upgrades with controllable manual/automatic schema upgrades</li> <li>Management dashboard for consolidated logs and metrics gathering for all components in the deployment</li> </ul> </li> </ul>"},{"location":"contributing/branching-model/","title":"Branching Strategy and Versioning Workflow for Smile CDR Helm Charts","text":"<p>This document outlines the versioning strategy and branching workflow for the Smile CDR Helm Charts.</p> <p>The workflow is based on the semantic-release workflow but has been modified to work in conjunction with the Smile CDR release process.</p> <p>This branching workflow aligns with Semantic Release guidelines and integrates with the CI pipeline for automated versioning and release management.</p>"},{"location":"contributing/branching-model/#versioning","title":"Versioning","text":"<p>As these Helm Charts follow SemVer 2.0, the version format is <code>MAJOR.MINOR.PATCH</code> with an optional suffix for pre release (<code>-pre.n</code>) or beta (<code>-beta.n</code>) versions</p> <p>Refer to the Versioning Strategy page for more information.</p>"},{"location":"contributing/branching-model/#branch-and-flow-overview","title":"Branch and Flow Overview","text":"<p>The following branches are used to auto-publish releases to the <code>STABLE</code> release channel:</p> Branch Release Format Usage <code>main</code> <code>MAJOR.MINOR.PATCH</code> The current stable production-ready release <code>release-n.x</code> <code>MAJOR.MINOR.PATCH</code> Used for maintaining stable updates for previous <code>n.x</code> production releases. Only created after the release of the 'n+1' version <p>The following branches are used to auto-publish pre releases and beta versions to the <code>DEVEL</code> release channel:</p> Branch Release Format Usage <code>pre-release</code> <code>MAJOR.MINOR.PATCH-pre.n</code> Used for developing non-breaking changes on the current release <code>pre-release-n.x</code> <code>MAJOR.MINOR.PATCH-pre.n</code> Used for previewing maintenance updates for the <code>release-n.x</code> branches. <code>next</code> <code>MAJOR.MINOR.PATCH-beta.n</code> Used for ongoing development of the next upcoming major release <code>next-n.x</code> <code>MAJOR.MINOR.PATCH-beta.n</code> Used for developing and previewing releases beyond the next release <p>Features get developed in their own feature branches and should be merged into one of the branches above</p>"},{"location":"contributing/branching-model/#branch-flow-for-release-and-maintenance-branches","title":"Branch Flow for Release and Maintenance Branches","text":"<p>The bulk of Helm Chart features and fixes are developed for the current stable release of the Helm Chart.</p> <ul> <li>Development is performed on a feature branch which is then merged into the <code>pre-release</code> branch<ul> <li>This merge will automatically create the next minor/patch pre release in the <code>DEVEL</code> channel.</li> </ul> </li> <li>To publish an official release, the repo maintainer creates a merge request from <code>pre-release</code> into <code>main</code><ul> <li>This merge will automatically create the next official minor/patch release in the <code>STABLE</code> channel.</li> </ul> </li> <li>Fixes (and optionally, features) can be back ported to previous releases using the <code>release-n.x</code> maintenance branches</li> </ul> <p>The below Git workflow demonstrates this process, assuming that we start out with version <code>1.0.0</code> being the current stable release.</p> <pre><code>---\nconfig:\n  gitGraph:\n    parallelCommits: true\n---\ngitGraph TB:\n    checkout main\n    commit id: \"v1.0.0\"\n    branch release-1.x order: 4\n    checkout main\n    commit id: \"v2.0.0\"\n    branch pre-release order: 1\n    branch v2-feat-1 order: 2\n    commit id: \"feat: add new v2 feature\"\n    checkout pre-release\n    merge v2-feat-1\n    commit id: \"Bump v2.1.0-pre.1\"\n    checkout main\n    merge pre-release\n    commit id: \"Bump v2.1.0\"\n    checkout pre-release\n    merge main id: \"mergeback\"\n\n    checkout release-1.x\n    branch v1-fix-1 order: 5\n    commit id: \"fix: fix v1\"\n    checkout release-1.x\n    merge v1-fix-1\n    commit id: \"Bump v1.1.1\"\n\n    #checkout pre-release\n    #branch general-fix order: 3\n    #commit id: \"fix: fix for all versions\"\n    #checkout pre-release\n    #merge general-fix\n    #commit id: \"Bump v2.1.1-pre.1\"\n    #checkout main\n    #merge pre-release\n    #commit id: \"Bump v2.1.1\"\n\n    #checkout release-1.x\n    #merge general-fix id: \"Backport Fix\"\n    #commit id: \"Bump v1.1.2\"\n</code></pre>"},{"location":"contributing/branching-model/#branch-flow-for-next-and-future-versions","title":"Branch Flow for Next and Future Versions","text":"<p>Some Helm Chart features and fixes contain breaking changes need to be developed for a future major release of the Helm Chart. This can occur when:</p> <ul> <li>The chart is updated to use the next major Smile CDR release</li> <li>The feature or fix contains a breaking change that cannot be implemented in a backwards compatible fashion</li> </ul> <p>The development process is the same as the regular branch flow above, except different branches are used:</p> <ul> <li>Development is performed on a feature branch which is then merged into the <code>next</code> or <code>next-n.x</code> branch.<ul> <li>This merge will automatically create the next major beta release in the <code>DEVEL</code> channel.</li> </ul> </li> <li>To initiate an official major release, the repo maintainer creates a merge request from <code>next</code> into <code>pre-release</code><ul> <li>This merge will automatically create the next major pre release in the <code>DEVEL</code> channel.</li> </ul> </li> <li>To complete an official major release, the repo maintainer creates a merge request from <code>pre-release</code> into <code>main</code><ul> <li>This merge will automatically create the next official major release in the <code>STABLE</code> channel.</li> </ul> </li> </ul> <p>The below Git workflow demonstrates this process, assuming that we start out with version <code>2.0.0</code> being the current stable release.</p> <pre><code>---\nconfig:\n  gitGraph:\n    parallelCommits: true\n---\ngitGraph TB:\n    commit id: \"v2.0.0\"\n    branch pre-release\n    branch next\n    branch v3-feat-1\n    branch next-4.x\n\n    # v3 feature 1\n    checkout v3-feat-1\n    commit id: \"feat: add v3 feature 1\"\n    checkout next\n    merge v3-feat-1\n    commit id: \"Bump v3.0.0-beta.1\"\n\n    # v4 feature 1\n    checkout next-4.x\n    branch v4-feat-1\n    commit id: \"feat: add v4 feature 1\"\n    checkout next-4.x\n    merge v4-feat-1\n    commit id: \"Bump v4.0.0-beta.1\"\n\n    # v3 feature 2 (Too busy when including this)\n    #checkout next\n    #branch v3-feat-2\n    #commit id: \"feat: add v3 feature 2\"\n    #checkout next\n    #merge v3-feat-2\n    #commit id: \"Bump v3.0.0-beta.2\"\n\n    # Start Release Process\n    checkout pre-release\n    merge next id: \"Release V3\"\n    commit id: \"Bump v3.0.0-pre.1\"\n    checkout main\n    merge pre-release\n    commit id: \"Bump v3.0.0\"\n\n    # Set up `next` for v4\n    checkout next-4.x\n    merge next id: \"mergeback\"\n    checkout next\n    merge next-4.x id: \"v4.0.0-beta.2\"\n</code></pre>"},{"location":"contributing/branching-model/#developing-new-helm-chart-features","title":"Developing New Helm Chart Features","text":"<p>When developing new features or functionality for the Helm Chart, the chosen branching strategy depends on the following:</p> <ul> <li>Does the feature depend on a specific version of Smile CDR?</li> <li>Does the feature include breaking changes?</li> <li>Is there any urgency to officially release the feature before the next major Smile CDR release?</li> <li>Does the feature need to be back-ported to previous Helm Chart releases?</li> </ul>"},{"location":"contributing/branching-model/#features-without-smile-cdr-version-dependency","title":"Features Without Smile CDR Version Dependency","text":"<p>When developing a feature that does not depend on a specific future version of Smile CDR, the branching strategy will depend on whether the feature introduces a breaking change or not.</p>"},{"location":"contributing/branching-model/#without-breaking-changes","title":"Without Breaking Changes","text":"<p>If the feature does NOT depend on a specific version of Smile CDR, and does NOT contain breaking changes, then it can be safely developed on the <code>pre-release</code> branch with the intent of inclusion as a minor version bump of the current stable major release.</p>"},{"location":"contributing/branching-model/#with-breaking-changes","title":"With Breaking Changes","text":"<p>If the feature includes breaking changes, then it should be developed on the <code>next</code> branch with the intent of inclusion in the next major release, along with the next release of Smile CDR.</p> <p>Note on Urgent Features</p> <p>In cases where there is urgency to have the new feature released before the next Smile CDR release cycle, we SHOULD NOT release a new major version of the Helm Chart, as it will throw off the predictable version mapping between Smile CDR versions and Helm Chart major versions.</p> <p>In such situations where the new functionality is critical for an environment, it may be preferable to use the next upcoming version of the Helm Chart before it is released. There are caveats with this approach, so it needs to be addressed on a case-by-case basis.</p>"},{"location":"contributing/branching-model/#features-with-smile-cdr-version-dependency","title":"Features With Smile CDR Version Dependency","text":"<p>If the feature or functionality depends on a specific future version of Smile CDR, then it should be developed on an appropriate <code>next</code> or <code>next-n.x</code> branch.</p> <p>Note</p> <p>When creating a new <code>next-n.x</code> branch, ensure that its source branch has already incremented to version <code>n-1</code>. This is to ensure that the semantic release processes choose the correct version for the pre releases.</p> <p>For example: If you create a <code>next-3.x</code> branch, the source branch should already be on version <code>2.x.x</code> in order to ensure the pre releases will be version <code>3.x.x-pre.1</code>.</p> <p>See the Post Release Tasks section for more info on updating <code>next</code> and <code>next-n.x</code> branches.</p>"},{"location":"contributing/branching-model/#examples","title":"Examples","text":"<p>The following examples assume that the current Helm Chart stable version is <code>1.1.1</code>, using Smile CDR <code>2024.05.R04</code></p> Without Smile CDR Version DependencyWith Smile CDR Version Dependency Without Breaking ChangesWith Breaking Changes <p>If you are developing a new feature WITHOUT breaking changes.</p> <ul> <li>You should use the <code>pre-release</code> branch to develop your feature.</li> <li>This will result in pre release builds (e.g. <code>1.2.0-pre.1</code>) being released in the <code>DEVEL</code> channel to facilitate testing.</li> <li>When you merge this change to <code>main</code>, version <code>1.2.0</code> will be released in the <code>STABLE</code> channel for general consumption.</li> </ul> <p>If you are developing a new feature WITH breaking changes.</p> <ul> <li>You should use the <code>next</code> branch to develop your feature.</li> <li>This will result in pre release builds (e.g. <code>2.0.0-beta.1</code>) being released in the <code>DEVEL</code> channel to facilitate testing.</li> <li>This feature will become part of version <code>2.0.0</code> when it is released to the <code>STABLE</code> channel for general consumption.</li> </ul> <p>If the new feature depends on a specific future Smile CDR version, then this is implicitly a breaking change. Therefore there is no option to use the <code>pre-release</code> branch, which is only to be used for non-breaking changes.</p> <p>If you are developing a feature that depends on Smile CDR <code>2024.11.x</code></p> <ul> <li>You should use the <code>next-3.x</code> branch to develop your feature.</li> <li>This will result in a beta version (e.g. <code>3.0.0-beta.1</code>) being published in the <code>DEVEL</code> channel to facilitate testing.</li> </ul>"},{"location":"contributing/branching-model/#developing-helm-chart-fixes","title":"Developing Helm Chart Fixes","text":"<p>When developing hot-fixes or patch updates for the Helm Chart, the chosen branching strategy depends on the following:</p> <ul> <li>Is this an upstream patch change? (i.e. Changing the default Smile CDR version from <code>2024.05.R03</code> to <code>2024.05.R04</code>)</li> <li>Is this a Helm Chart fix/patch (i.e. a bug in the functioning of the Helm Chart)</li> <li>Does this fix affect all currently supported major versions of the Helm Chart?</li> </ul>"},{"location":"contributing/branching-model/#helm-chart-fixpatch","title":"Helm Chart Fix/Patch","text":"<p>If your update is a fix for some broken Helm Chart functionality, and if it is NOT a breaking change, merge your changes to the <code>pre-release</code> branch if you want to preview the fix/patch on the <code>DEVEL</code> channel. When ready to release the change, merge your changes to the <code>main</code> branch to release to the <code>STABLE</code> channel.</p> <p>Fixing Multiple Helm Chart Versions</p> <p>If the fix is required on multiple versions of the Helm Chart, then the process will need to be repeated for the <code>release-n.x</code> branch for each of the currently supported releases.</p>"},{"location":"contributing/branching-model/#upstream-smile-cdr-patches","title":"Upstream Smile CDR Patches","text":"<p>If you are simply updating the current Smile CDR release to the latest patch version, merge your changes to the <code>pre-release</code> branch if you want to preview the fix/patch on the <code>DEVEL</code> channel. When ready to release the change, merge your changes to the <code>main</code> branch to release to the <code>STABLE</code> channel.</p> <p>This process should only be performed on Helm Chart versions who's Smile CDR version has received a patch level update.</p> <p>For example, if Smile CDR <code>2024.05.R04</code> gets updated to <code>2024.05.R05</code>, but <code>2024.08.R01</code> does not get updated, then changes would only be made to the version <code>1.x</code> release branch and not the <code>2.x</code> branch. When <code>2024.08.R01</code> gets updated to <code>2024.08.R02</code>, that change would only apply to the <code>2.x</code> branch.</p>"},{"location":"contributing/branching-model/#merging-feature-branches","title":"Merging Feature Branches","text":"<p>All updates should be developed on a feature branch. The feature branch should be created from the branch that you intent to contribute to.</p> <p>All merges back to the appropriate release or pre release branch should be done using a Merge Request in GitLab. Release branches are protected and SHOULD NOT have commits pushed directly to them.</p> <p>When creating feature branch merge requests, they should only be for a single feature. DO NOT include arbitrary changes in your feature branch, such as unrelated typos or formatting areas in different areas of code. These should be done in a separate branch/merge request.</p> <p>You MUST squash your commits in your Merge Request and ensure that the commit follows the appropriate conventional commits message.</p>"},{"location":"contributing/branching-model/#release-workflow","title":"Release workflow","text":"<p>For many of the above mentioned scenarios, there is the option to merge changes directly to a release branch (i.e. <code>main</code>, <code>release-1.x</code>, <code>release-2.x</code> etc ) or merge to a pre release branch first.</p> <p>The approach to choose depends on the requirements for the feature.</p>"},{"location":"contributing/branching-model/#merge-to-pre-release-branches","title":"Merge to Pre Release Branches","text":"<p>Merge to branches that publish to the <code>DEVEL</code> channel when:</p> <ul> <li>The change is a breaking change for a future version of the Helm Chart.</li> <li>The change needs to be tested in test environments before being released to the <code>STABLE</code> channel.</li> <li>The change needs to be shared with others for testing purposes before being released to the <code>STABLE</code> channel.</li> <li>The change is only a partial implementation for a new feature that needs to be shared for collaborative purposes before it's completed.</li> <li>Successive iterations to a new feature may introduce breaking changes for that feature.</li> <li>By adding consecutive breaking changes for a new feature in a pre release, you are still able to merge it to the <code>STABLE</code> branch when it's ready.</li> <li>If you pushed the new feature early, and then made new changes that break that new feature, you would need to wait until the next major release to push them, which is not ideal.</li> </ul>"},{"location":"contributing/branching-model/#merge-to-release-branches","title":"Merge to Release Branches","text":"<p>Merge to release branches when:</p> <ul> <li>The change is trivial, non-breaking and unlikely to need further work.</li> <li>In the event that a new feature is released as a minor version update, any breaking changes to the feature can wait until the next major version release.</li> </ul>"},{"location":"contributing/branching-model/#promoting-pre-releases","title":"Promoting Pre Releases","text":"<p>Once you have published a change with a pre release version (on the <code>DEVEL</code> channel) you can promote it to the <code>STABLE</code> channel by creating a merge request from the pre release branch to the release branch.</p> <p>For example, the branches would map as follows, assuming the current stable release is <code>2.0.0</code></p> Pre Release Version Pre Release Branch Release Version Release Branch Notes <code>2.1.0-pre.1</code> <code>pre-release</code> <code>2.1.0</code> <code>main</code> <code>2.0.0</code> is the current stable release <code>1.2.0-pre.1</code> <code>pre-release-1.x</code> <code>1.2.0</code> <code>release-1.x</code> This is a release on the 1.x maintenance branch"},{"location":"contributing/branching-model/#promoting-beta-versions","title":"Promoting Beta Versions","text":"<p>When a beta version is ready to be released, you should promote it by creating a merge request from the <code>next</code> branch to the <code>pre-release</code> branch. From there, the above process is then used to promote it from the <code>DEVEL</code> release channel to <code>STABLE</code></p> <p>For example, the branches would map as follows, assuming the current stable major release is version <code>2.x</code> and you are about to release version <code>3.0.0</code></p> Beta Version Beta Branch Pre Release Version Pre Release Branch Notes <code>3.0.0-beta.17</code> <code>next</code> <code>3.0.0-pre.1</code> <code>pre-release</code> When going from <code>next</code> to <code>pre-release</code> versioning starts with the <code>-pre.1</code> suffix <code>4.0.0-beta.3</code> <code>next-4.x</code> <code>4.0.0-beta.3</code> <code>next</code> Version number does not change"},{"location":"contributing/branching-model/#new-major-releases","title":"New Major Releases","text":"<p>Major version releases of the Smile CDR Helm Chart should coincide with new major releases of Smile CDR, i.e. when <code>2024.08.R01</code> is released, version <code>2.0.0</code> of the Helm Chart should be released.</p> <p>Before releasing a new major version of the Helm Chart, there should already be development on the <code>next</code> branch. At the very least, there should be a breaking feature change on this branch to update to the new Smile CDR version.</p> <p>Warning</p> <p>It is critical that there is at least 1 breaking change feature in this branch as Semantic Release requires this in order to automatically bump the major version</p>"},{"location":"contributing/branching-model/#performing-the-release","title":"Performing The Release","text":"<p>Promoting the <code>next</code> branch to <code>main</code> will release the next version, but there are other steps required to ensure that the above workflows function as expected afterwards.</p> <p>All of the following steps should be performed by the repository maintainer.</p> <ul> <li>Ensure there are no outstanding changes to be merged from <code>pre-release</code> into <code>main</code></li> <li>Ensure that <code>main</code> is fully merged back into <code>next</code> and tested, to capture any bug fixes that were created on the old main version.</li> <li>Create merge request to merge <code>next</code> into <code>pre-release</code>.</li> <li>Create merge request to merge <code>pre-release</code> into <code>main</code> to initiate the release process.<ul> <li>Commits MUST NOT be squashed as the pipeline relies on accurate conventional commits to determine the appropriate semantic version to apply.</li> <li>The CI pipeline uses Semantic Release to test the build, increment the version, create the release, publish the Helm Chart and update the version in the branch (by committing back to the repo)</li> </ul> </li> </ul> <p>If we had just released version <code>3.0.0</code> using the above steps, the branches would now be in the following state:</p> Branch Current Version Notes <code>release-1.x</code> <code>1.1.1</code> Already created when version <code>2.0.0</code> was released. <code>pre-release-1.x</code> <code>1.1.1-pre.1</code> Already created when version <code>2.0.0</code> was released. <code>main</code> <code>3.0.0</code> Initial major release for version <code>3.x</code>. <code>pre-release</code> <code>3.0.0-pre.1</code> This branch is now used for version <code>3.x</code> updates instead of <code>2.x</code>. <code>next</code> <code>3.0.0-beta.1</code> This now needs to be updated for the <code>4.x</code> beta releases. <code>next-v4</code> <code>4.0.0-beta.1</code> Branch for future version <code>4.x</code> beta releases. No longer required. <code>next-v5</code> <code>5.0.0-beta.1</code> Branch for future version <code>5.x</code> beta releases."},{"location":"contributing/branching-model/#post-release-tasks","title":"Post-release Tasks","text":"<p>Note that after performing the release steps above, there are missing requirements:</p> <ul> <li>There are no maintenance or pre release branches for version <code>2.x</code></li> <li>The <code>next</code> branch no longer represents the upcoming version <code>4.x</code></li> </ul> <p>These are addressed with the following post-release tasks that should be performed by the repository maintainer:</p> <ul> <li>Create a new maintenance branch for the previous version. i.e. If version <code>3.0.0</code> was just release to the <code>main</code> branch, then a new branch, <code>release-2.x</code> should be created to maintain the <code>2.x</code> releases.</li> <li>Create a new pre release branch for the previous release. i.e. Continuing from above, create a <code>pre-release-2.x</code> branch from the <code>release-2.x</code> branch</li> </ul> <ul> <li>Merge the <code>next-v4</code> branch into <code>next</code> so that it can be used for the next upcoming major release, version <code>4.0.0</code>. At this point, the <code>next-v4</code> branch is no longer required and may be deleted.</li> <li>Create the <code>next-v6</code> branch from the <code>next-v5</code> branch if the appropriate Pre-Release of Smile CDR is available.</li> </ul>"},{"location":"examples/","title":"Example Configurations","text":"<p>This section contains some example configurations and instructions to help get you started.</p>"},{"location":"examples/aws-dependencies/","title":"Configuring AWS Dependencies","text":"<p>This example shows how you would install dependencies in an AWS environment.</p> <p>We will go over the creation of the following AWS resources in preparation for deploying Smile CDR using the Helm Chart.</p> <ul> <li>IAM Role using IRSA</li> <li>AWS Secrets Manager</li> </ul> <p>Note: These resources are configured in a way that is not obvious to an AWS administrator that has not dealt with IRSA before, which is why we are including them here. For other AWS resources (Such as RDS, S3, Certificates Manager etc) conventional configurations can be used, so we do not cover them in this example at this time.</p> <p>You will need both of these if you are using the recommended method of storing your container registry secrets - using Secrets Store CSI Driver</p>"},{"location":"examples/aws-dependencies/#creating-iam-role-with-irsa","title":"Creating IAM Role with IRSA","text":"<p>To set up an IAM Role to be used by the application pods, we use IRSA (IAM Roles for Service Accounts). Detailed information and instructions for IRSA are located here</p> <p>In this example, we will be creating the role to work with an deployment of Smile CDR in a fictional EKS cluster with the following properties:</p> <ul> <li>AWS Region <code>us-east-1</code></li> <li>Cluster Name <code>mycluster</code></li> <li>Namespace <code>smilecdr</code></li> <li>Helm Release Name <code>my-smile</code></li> </ul> <p>These are important as they will be referenced in the trust policy.</p>"},{"location":"examples/aws-dependencies/#iam-policy","title":"IAM Policy","text":"<p>Before starting, you need to determine which AWS services need to be accessed by this role. Typical examples would be:</p> <ul> <li>AWS Secrets Manager - Used for storing credentials for container repository and database</li> <li>RDS - Required if Smile CDR is configured to use IAM authentication for RDS</li> <li>S3 - Required if you are including extra files using the external method</li> </ul> <p>You will need to keep these resources in mind when creating your IAM Policy.</p> <p>Note: When creating IAM Policies, you should keep the principle of least privilege in mind and only allow the minimum required access for the resources needed. Avoid using wildcard entries for <code>actions</code> and <code>resources</code> where possible.</p> <p>In this example, we will create a policy that only has access to the container repository secret that we create below</p> <p>Following the AWS CLI instructions from here we would do the following:</p> <ol> <li> <p>Create an IAM policy file</p> <p>Create IAM Policy file with the following content: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:GetSecretValue\",\n            \"Resource\": \"arn:aws:secretsmanager:us-east-1:&lt;accountid&gt;:secret:demo/dockerpull-??????\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"arn:aws:kms:*:&lt;accountid&gt;:aws/secretsmanager\"\n        }\n    ]\n}\n</code></pre></p> <p>Note: The <code>??????</code> is a wildcard that matches the random suffix added to an AWS Secrets Manager Secret. See here for more info.</p> </li> <li> <p>Create the IAM Policy <pre><code>aws iam create-policy --policy-name smilecdr-dockersecret-policy --policy-document file://my-policy.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#iam-role","title":"IAM Role","text":"<p>When creating the IAM role, it needs to be associated with the Kubernetes service account via a trust policy. To do this, we need a few details in advance:</p> <p>AWS Account number <pre><code>account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\n</code></pre></p> <p>EKS cluster's OIDC provider <pre><code>oidc_provider=$(aws eks describe-cluster --name mycluster --region us-east-1 --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\")\n</code></pre> Namespace and ServiceAccount Resource Names</p> <p>In the case of this example, we are using the <code>smilecdr</code> namespace, with the <code>my-smile</code> release name as mentioned above. This will result in a Service Account with the name <code>my-smile-smilecdr</code> <pre><code>export namespace=smilecdr\nexport service_account=my-smile-smilecdr\n</code></pre></p> <ol> <li> <p>Create trust policy file</p> <pre><code>cat &gt;trust-relationship.json &lt;&lt;EOF\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n        \"StringEquals\": {\n        \"$oidc_provider:aud\": \"sts.amazonaws.com\",\n        \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\"\n        }\n    }\n    }\n]\n}\nEOF\n</code></pre> </li> <li> <p>Create the IAM Role <pre><code>aws iam create-role --role-name smilecdr-role --assume-role-policy-document file://trust-relationship.json --description \"Smile CDR Application Role\"\n</code></pre></p> </li> <li> <p>Attach the IAM Policy to the role <pre><code>aws iam attach-role-policy --role-name smilecdr-role --policy-arn=arn:aws:iam::$account_id:policy/smilecdr-dockersecret-policy\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#using-iam-role-in-helm-values","title":"Using IAM Role in Helm Values","text":"<p>Configure your values file to use this role for the Service Account like so: <pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::&lt;account id&gt;:role/smilecdr-role\n</code></pre></p> <p>Now, when you deploy Smile CDR, it will use the above IAM role whenever accessing AWS resources.</p>"},{"location":"examples/aws-dependencies/#creating-aws-secrets-manager-secrets","title":"Creating AWS Secrets Manager Secrets","text":"<p>Secrets can be a bit of a chicken-and-egg problem.</p> <p>If you want to avoid storing secrets in code, by using a secrets vault, how do you do this 'via code'?</p> <p>One mechanism is to get the vault software to generate a random secret, or rotate the secret after the initial creation. These are not always viable options, which is certainly the case for storing secrets to access external systems, such as a container registry.</p> <p>In this example, we will create the <code>docker pull</code> secret manually via the AWS CLI. You could just as easily deploy the secret using some other mechanism and then update it with the cli or with the AWS console.</p>"},{"location":"examples/aws-dependencies/#create-secret","title":"Create Secret","text":"<p>The value of Kubernetes <code>imagePullSecrets</code> needs to be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p> <p>This essentially means the secret value is a JSON string representing the Docker <code>config.json</code> file. As an AWS Secrets Manager secret consists of a JSON map of secrets and values, we end up with a nested JSON data structure.</p> <p>The easiest way to pass this to the AWS CLI command is to temporarily store the JSON in a file which can be passed in to the <code>create-secret</code> command as a parameter.</p> <ol> <li>Create the temporary password JSON file</li> </ol> <p>Update your user &amp; password before running the below.</p> <pre><code>cat &gt;dockerconf.json &lt;&lt;EOF\n{\n  \"auths\":{\n    \"docker.smilecdr.com\":{\n      \"auth\": \"$(echo -n \"user:password\" | base64)\"\n    }\n  }\n}\nEOF\n\ncat &gt;tempsecret.json &lt;&lt;EOF\n{\n  \"dockerconfigjson\": \"$(cat dockerconf.json)\"\n  }\n}\nEOF\n</code></pre> <ol> <li> <p>Create the Secrets Manager Secret <pre><code>aws secretsmanager create-secret \\\n    --name \"demo/dockerpull\" \\\n    --secret-string file://tempsecret.json\n</code></pre></p> <p>Note: The name <code>demo/dockerpull</code> is just an example. You may use any scheme that you like for secret names. If you have an existing standard, use that.</p> </li> <li> <p>Remove the temporary secret file <pre><code>rm tempsecret.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#update-secret","title":"Update Secret","text":"<p>Use the following to update the secret with a new value if required.</p> <pre><code>aws secretsmanager update-secret \\\n    --secret-id \"demo/dockerpull\" \\\n    --secret-string file://tempsecret.json\n</code></pre> <p>Note: You need to update rather than delete and recreate, as AWS Secrets Manager implements a grace period on secrets deletion.</p>"},{"location":"examples/aws-dependencies/#use-the-secret","title":"Use The Secret","text":"<p>If the IAM Role, IAM Policies and Helm Values were all set up correctly as per the above steps, you will now be able to launch pods using images from the authenticated container repository.</p>"},{"location":"examples/external-db-multi/","title":"Multiple External DB Configuration","text":"<p>This example demonstrates using multiple external Postgres Databases.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Separate databases for Cluster Manager, Audit logs, Transaction logs and default Persistence module</li> </ul>"},{"location":"examples/external-db-multi/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Multiple external Postgres databases provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published JSON structure</li> </ul>"},{"location":"examples/external-db-multi/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\ndatabase:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\n      module: clustermgr\n    - secretName: auditSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:auditSecret\"\n      module: audit\n    - secretName: txLogsSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:txLogsSecret\"\n      module: transaction\n    - secretName: persistenceSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:persistenceSecret\"\n      module: persistence\n</code></pre>"},{"location":"examples/external-db/","title":"External DB Configuration","text":"<p>This example demonstrates using an external Postgres Database.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> </ul>"},{"location":"examples/external-db/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>External Postgres database provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published JSON structure</li> </ul>"},{"location":"examples/external-db/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\ndatabase:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\n      module: clustermgr\n</code></pre>"},{"location":"examples/extra-files-external/","title":"Adding Files Configuration","text":"<p>This example demonstrates passing in extra files to the deployment from external sources.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR, using Amazon S3</li> <li>Elastic APM <code>.jar</code> file will be included in the <code>customerlib</code> directory in Smile CDR, using curl</li> </ul>"},{"location":"examples/extra-files-external/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>Amazon S3 bucket with a customized <code>logback.xml</code> file copied to a <code>classes</code> folder</li> <li>AWS IAM Role configured to access:<ul> <li>AWS Secrets Manager</li> <li>Amazon S3 bucket</li> </ul> </li> </ul>"},{"location":"examples/extra-files-external/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\ncopyFiles:\n  classes:\n    sources:\n    # Copies files recursively from S3 to the classes directory\n    - type: s3\n      bucket: s3-bucket-name\n      # The below S3 bucket prefix must contain the custom logback.xml\n      # file, as well as any other needed files.\n      path: /path-to/classes\n  customerlib:\n    sources:\n    # Downloads a single file using curl to the customerlib directory\n    # (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n    - type: curl\n      fileName: elastic-apm/elastic-apm-agent-1.13.0.jar\n      url: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre>"},{"location":"examples/extra-files-helm/","title":"Adding Files Using Helm","text":"<p>This example demonstrates passing in extra files to the deployment via the Helm Chart.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR</li> </ul>"},{"location":"examples/extra-files-helm/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Customized <code>logback.xml</code> file available in your configuration repo/folder</li> </ul>"},{"location":"examples/extra-files-helm/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmappedFiles:\n  logback.xml:\n    type: configMap\n    path: /home/smile/smilecdr/classes\n</code></pre>"},{"location":"examples/extra-files-helm/#extra-install-steps","title":"Extra Install Steps","text":"<p>To use this feature, you must update your <code>helm upgrade</code> command to include <code>--set-file mappedFiles.logback\\\\.xml.data=logback.xml</code></p>"},{"location":"examples/minimal/","title":"Minimal Configuration","text":"<p>This values file demonstrates the minimal required configurations to install Smile CDR in a secure manner (i.e. not passing repository secrets)</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/minimal/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/minimal/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre>"},{"location":"examples/modules-advanced/","title":"Advanced Modules Configuration","text":"<p>This example demonstrates defining all modules from scratch, not using any of the default modules.</p> <p>It is based on the minimal example.</p> <p>We will create a couple of examples.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Completely custom Smile CDR module configuration<ul> <li>Disabled default module configuration</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-advanced/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-advanced/#example-1-minimal-module-config","title":"Example 1 - Minimal module config","text":"<p>This example shows how you would configure Smile CDR to just use the following modules:</p> <ul> <li>Cluster Manager</li> <li>Persistence Module</li> <li>Local Security</li> <li>Admin Web</li> <li>Fhir Endpoint</li> </ul> <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  useDefaultModules: false\n\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: false\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre>"},{"location":"examples/modules-advanced/#example-2-minimal-module-config-with-separate-values-files","title":"Example 2 - Minimal module config with separate values files","text":"<p>As you can see from the above, the values file can start getting unwieldy.</p> <p>It is advised to split them up into manageable chunks like so.</p> <p>values-common.yaml <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  useDefaultModules: false\n</code></pre></p> <p>values-clustermgr.yaml <pre><code>modules:\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n</code></pre></p> <p>values-persistence-r4.yaml <pre><code>modules:\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: false\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n</code></pre></p> <p>values-security.yaml <pre><code>modules:\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n</code></pre></p> <p>values-admin-web.yaml <pre><code>modules:\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre></p> <p>values-fhir-endpoint.yaml <pre><code>modules:\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre></p> <p>When installing the above, you would then pass in the multiple values file like so: <pre><code>helm upgrade -i my-smile-env -f values-clustermgr.yaml -f values-persistence-r4.yaml -f values-security.yaml -f values-admin-web.yaml -f values-fhir-endpoint.yaml smiledh/smilecdr\n</code></pre></p>"},{"location":"examples/modules-simple/","title":"Basic Modules Configuration","text":"<p>This example demonstrates simple reconfiguration of Smile CDR modules.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Modified Smile CDR module configuration</li> <li>We will only modify <code>dao_config.inline_resource_storage_below_size</code> for the persistence database</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-simple/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-simple/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  persistence:\n    config:\n      dao_config.inline_resource_storage_below_size: 4000\n</code></pre>"},{"location":"examples/previousrootversion/","title":"Using Previous (root) Version","text":"<p>This example demonstrates running an older version of Smile CDR that requires the container to be running as root.</p> <p>This only applies if using versions <code>2022.11.R04</code> or earlier.</p> <p>It is based on the Advanced Modules Configuration example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Container explicitly set to run as root</li> <li>Completely custom Smile CDR module configuration<ul> <li>Disabled default module configuration, including audit, transaction and license modules</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/previousrootversion/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/previousrootversion/#example-config","title":"Example Config","text":"Click to expand <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  tag: \"2022.11.R04\"\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nsecurityContext:\n  runAsNonRoot: false\n  runAsUser: 0\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  # Define modules to be used. Some of these will contain service definitions.\n  # A service and an ingress rule will be created for modules that use services.\n  # Canonical endpoint URLs will be generated by _smile-module-helpers.tpl and\n  # populated in the smilecdr.services variable. These can be consumed by other\n  # modules that reference them.\n\n  useDefaultModules: false\n\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      # Valid options include H2_EMBEDDED, DERBY_EMBEDDED, MYSQL_5_7, MARIADB_10_1, POSTGRES_9_4, ORACLE_12C, MSSQL_2012\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n      transactionlog.enabled: false\n      retain_transaction_log_days: 7\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: true\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n\n  admin_json:\n    name: JSON Admin Services\n    enabled: true\n    type: ADMIN_JSON\n    service:\n      enabled: true\n      svcName: admin-json\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: json-admin\n      port: 9000\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n\n  subscription:\n    name: Subscription\n    enabled: true\n    type: SUBSCRIPTION_MATCHER\n    requires:\n      PERSISTENCE_ALL: persistence\n\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  fhirweb_endpoint:\n    name: FHIRWeb Console\n    enabled: true\n    type: ENDPOINT_FHIRWEB\n    service:\n      enabled: true\n      svcName: fhirweb\n    requires:\n      SECURITY_IN_UP: local_security\n      ENDPOINT_FHIR: fhir_endpoint\n    config:\n      context_path: fhirweb\n      port: 8001\n      threadpool.min: 2\n      threadpool.max: 10\n      tls.enabled: false\n      anonymous.access.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  # Fhir Endpoint\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  smart_auth:\n    name: SMART Security\n    enabled: true\n    type: SECURITY_OUT_SMART\n    service:\n      enabled: true\n      svcName: smart-auth\n    requires:\n      CLUSTERMGR: clustermgr\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: smartauth\n      port: 9200\n      openid.signing.keystore_id: default-keystore\n      issuer.url: default\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  package_registry:\n    name: Package Registry\n    enabled: true\n    type: ENDPOINT_PACKAGE_REGISTRY\n    service:\n      enabled: true\n      svcName: pkg-registry\n    requires:\n      PACKAGE_CACHE: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: package_registry\n      port: 8002\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre>"},{"location":"examples/quickstart/","title":"Quickstart Configuration","text":"<p>This is the configuration used in the Quickstart</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuration</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via values file (Don't do this!)</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/quickstart/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Credentials to an image repository with the official Smile CDR images.</li> </ul>"},{"location":"examples/quickstart/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: values\n    registry: docker.smilecdr.com\n    username: &lt;DOCKER_USERNAME&gt;\n    password: &lt;DOCKER_PASSWORD&gt;\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre>"},{"location":"guide/","title":"Smile CDR Helm Chart Guide","text":"<p>In this section, we cover various aspects of installing and configuring Smile CDR using the Helm Chart.</p>"},{"location":"guide/helm-repo/","title":"Helm Repository Setup","text":""},{"location":"guide/helm-repo/#configure-helm-repository","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>This will differ, depending on the method you will be using to deploy the charts.</p>"},{"location":"guide/helm-repo/#native-helm","title":"Native Helm","text":"<p>The simplest way to get up and running is by using the native <code>helm</code> commands.</p> <p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh-stable https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\n$ helm repo update\n</code></pre>"},{"location":"guide/helm-repo/#terraform","title":"Terraform","text":"<p>If installing the chart using Terraform, you may have a resource definition like so:</p> <pre><code>resource \"helm_release\" \"example\" {\n  name       = \"my-smilecdr-release\"\n  repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\"\n  chart      = \"smilecdr\"\n  # You can omit `version` and the current latest chart will be used.\n\n  # Ideally, specify a specific version or range of versions using the\n  # semantic versioning strict constraint to specify an exact version\n  # (i.e. v2.0.0-next-major.1)\n  version    = \"2.0.0-next-major.1\"\n  #\n  # To allow patch/fix releases, use the tilde\n  # constraint (~) to specify versions\n  # (i.e. &gt;= v2.0.0-next-major.1 &lt; v2.1.0)\n  # version    = \"~2.0.0-next-major.1\"\n\n  # To allow non-breaking minor/feature releases, use the caret\n  # constraint (^) to specify versions\n  # (i.e. &gt;= v2.0.0-next-major.1 &lt; v3.0.0)\n  # version    = \"^2.0.0-next-major.1\"\n\n  values = [\n    \"${file(\"my-values.yaml\")}\"\n  ]\n\n  set {\n    name  = \"values.override\"\n    value = \"value\"\n  }\n}\n</code></pre> <p>See the Terraform Helm Provider for more info on this.</p>"},{"location":"guide/helm-repo/#argocd","title":"ArgoCD","text":"<p>If installing in ArgoCD using an <code>Application</code> Custom Resource, you will need to create a custom 'Umbrella Chart' for your deployment so that you can pass in your values file (And any other files).</p> <p>To do this, you would create a configuration directory with your configuration files as well as a <code>Chart.yaml</code> file that may look like this:</p> <pre><code>apiVersion: v2\nname: umbrella-smilecdr\ndescription: An Umbrella Helm chart to deploy the Smile CDR Helm Chart\n\n# This Umbrella Helm Chart can be used to deploy Smile CDR in ArgoCD while\n# passing in your values files.\n\ntype: application\nversion: 1.0.0\n\n# Remember, when passing values files in to dependency charts, the entire yaml map needs to be\n# moved to a root key that matches the `name` of the dependency.\ndependencies:\n- name: smilecdr\n  version: \"^2.0.0-next-major.1\"\n  repository: \"https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\"\n</code></pre>"},{"location":"guide/helm-repo/#provide-repo-credentials","title":"Provide Repo Credentials","text":"<p>This Helm Chart supports multiple methods to securely provide access to container repositories.</p> Method Security Difficulty Notes AWS ECR with IAM role Highest Medium Recommended method if using ECR.This is only an option if you have a workflow to host the Smile CDR image on your own ECR repository.This has the highest security stance as long lived credentials are not required.No K8s <code>Secret</code> objects need to be created or referenced. Secrets Store CSI High Hardest Recommended method if not using ECR.You will need the SSCSI driver, an appropriate SSCSI provider, A secrets vault Secret and any IAM roles configured to access the secret Kubernetes Secret Medium Medium Need to manually set up Kubernetes Secret Values File Low Easiest K8s secret created by chart. Password is in your code (Bad)"},{"location":"guide/helm-repo/#aws-elastic-container-registry","title":"AWS Elastic Container Registry","text":"<p>If you are using AWS ECR as your container registry, you can avoid storing long-lived credentials in secrets by using a suitable IAM role.</p> <p>In order to use this method, the IAM role added to the Instance Profile of your Kubernetes worker nodes must include a policy that allows read access to your container registry. This mechanism does not use IRSA, so can only use the worker node's Instance Profile/IAM role.</p>"},{"location":"guide/helm-repo/#private-registry","title":"Private Registry","text":"<p>If you are pulling the Smile CDR docker image directly from <code>docker.smilecdr.com</code> or if you have custom Docker images that you build and publish on a private container registry that requires credentials, then you will need to provide them in a secure manner so that Kubernetes can pull the image.</p> <p>In Kubernetes, this is done using a list of secret names in <code>pod.spec.imagePullSecrets</code>. These secrets reference the name of pre-existing <code>Secret</code> objects in the same namespace.</p> <p>Note For more information on using private container registries with Kubernetes, see the official documentation here</p> <p>More details can be found in the Secrets Handling section of these docs.</p>"},{"location":"guide/helm-repo/#configuring-to-use-ecr-repository","title":"Configuring to use ECR Repository","text":"<p>In order for Kubernetes worker nodes to pull images from ECR, they must have an Instance Profile/IAM Role that has an IAM policy with the following actions allowed for your ECR Repository</p> <p>TODO: Add details of required IAM policy actions</p> <pre><code>iamsnippet:\n  here: yes yes\n</code></pre> <p>There are no extra steps after this, you do not need to specify any image pull secrets.</p> <p>If you do define image pull secrets for other contain registries, this will not be affected. You do not need to remove them.</p>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-secrets-store-csi-driver","title":"Configuring Repo Credentials using Secrets Store CSI Driver","text":"<p>Before using this configuration in your values file, ensure that you have followed the appropriate section in the Secrets Handling guide to set up Secrets Store CSI, the AWS Provider, your AWS Secret, your IAM Role and configured the <code>ServiceAccount</code>.</p> <p>Once you have done that, you need to add an item to the <code>image.imagePullSecrets</code> list like so like so:</p> <pre><code>image:\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n</code></pre> <p>Note: This uses the <code>secretSpec</code> schema defined here</p>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-kubernetes-secret","title":"Configuring Repo Credentials using Kubernetes Secret","text":"<p>Before using this configuration, you need to create a Kubernetes <code>Secret</code> object of type <code>kubernetes.io/dockerconfigjson</code> using some external/manual mechanism. For more info on this, refer to the Kubernetes section in Secrets Handling.</p> <p>Once this <code>Secret</code> object is created, you can add it in the same way that you would with a regular <code>imagePullSecrets</code> entry in the K8s PodSpec:</p> <p>It would look like this in your custom values file: <pre><code>image:\n  imagePullSecrets:\n  - name: myK8sSecretName\n    # Optional if you wish to make it explicitly clear which credential type you are using in your code\n    # type: k8sSecret\n</code></pre></p>"},{"location":"guide/helm-repo/#configuring-multiple-repo-credentials","title":"Configuring Multiple Repo Credentials","text":"<p>If you need to connect to multiple container repositories, you can mix and match the above types in the list of <code>imagePullSecrets</code></p> <pre><code>image:\n  imagePullSecrets:\n  # Regular pre-existing `Secret` object\n  - name: myK8sSecretName\n    type: k8sSecret\n  # Helm Chart will automatically create `Secret` objects for these:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n  - type: sscsi\n    provider: aws\n    # The secretArn must be uniqe\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname2\"\n# Service Account is still required here as we are using Secrets Store CSI\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n</code></pre>"},{"location":"guide/serviceaccount/","title":"Service Accounts","text":"<p>Certain features of the application, when installed with these Helm Charts, require authorized access to external systems so that they can function correctly.</p> <p>Examples of this include:</p> <ul> <li>Retrieving credentials from secrets management systems (e.g. AWS Secrets Manager)</li> <li>Accessing AWS managed services such as:<ul> <li>RDS authentication using IAM roles</li> <li>AWS HealthLake</li> <li>Accessing Amazon MSK (Managed Kafka)</li> <li>Accessing S3 buckets</li> </ul> </li> <li>Waiting for Kubernetes jobs to complete (i.e. during product upgrades and migration tasks)</li> </ul> <p>As explained in the Secrets Handling section above, passing in secrets (such as AWS Access Keys &amp; Tokens etc) directly to your configuration is a dangerous practice. Instead we can use the mechanisms provided by various infrastructure providers to use secure methods to gain access to these external systems.</p>"},{"location":"guide/serviceaccount/#iam-roles-for-service-accounts-irsa","title":"IAM Roles For Service Accounts (IRSA)","text":"<p>To give the application access to AWS resources, we use IAM Roles For Service Accounts, also known as IRSA. This attaches AWS IAM roles to a Kubernetes <code>ServiceAccount</code> which then gets attached to the application Pods.</p> <p>As a result of this, the application can access AWS services without needing to directly pass in AWS IAM User credentials.</p> <p>Note Currently, the Smile CDR Helm Chart only supports this integration in AWS, but support for other cloud providers will be added.</p>"},{"location":"guide/serviceaccount/#service-account-configuration","title":"Service Account Configuration","text":"<p>To use this feature, you will need to enable the Service Account and reference the IAM role that it should be connected to. Note that the IAM role being used needs to have the appropriate Trust Policy set up so that it can be used by your Cluster. More info and instructions are available here</p> <ul> <li>Create an IAM role for your deployment.<ul> <li>This role will be used for any Role-based access that the application pods need,   so name it accordingly to avoid confusion, e.g. <code>smilecdr-role</code></li> <li>If being used for Secrets Store CSI, ensure that it has read access to the secrets it will need to provide, and any KMS key used to encrypt them.</li> </ul> </li> <li>Create a trust policy for the IAM role so that it can be used with IRSA. Instructions   here<ul> <li>These instructions use the <code>eksctl</code> command which abstracts away some details into CloudFormation templates. Using something like Terraform would require different steps.</li> </ul> </li> </ul> <p>Once the IAM role is set up correctly, enable the <code>ServiceAccount</code> and reference the IAM role in the annotations in your values file like so: <pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/example-role-name\n</code></pre></p>"},{"location":"guide/serviceaccount/#examples","title":"Examples","text":"<p>Examples are available that show how you would correctly set up the IAM Policy, IAM Role, Trust Policy and Secret Manager dependencies. See here</p>"},{"location":"guide/values-files-management/","title":"Managing Values Files","text":"<p>You should configure your instance of Smile CDR using separate Helm <code>values</code> files for different configurations.</p>"},{"location":"guide/values-files-management/#create-new-values-files","title":"Create New Values Files","text":"<p>It is generally recommended to create a new, empty values file rather than copying the default values.yaml file from the Helm chart. The default values file can be lengthy and may contain values that are not relevant or suitable for your specific deployment.</p> <p>By starting with a fresh values file, you can customize your configuration to only include the values that you need to override. This can help to make your values file more concise and easier to manage. Additionally, starting from a blank file allows you to ensure that your deployment is not impacted by future updates to the default values file, which could potentially cause issues if you are using an older version.</p> <p>Creating your own values file from scratch gives you greater control and flexibility over your Helm chart deployment. It can help to ensure that your deployment is secure and stable, as you have the ability to carefully consider and set the values that are most relevant to your specific needs.</p>"},{"location":"guide/values-files-management/#organizing-values-files","title":"Organizing Values Files","text":"<p>It is a common practice to put all Helm Chart configurations into a single values file and provide that to the <code>helm upgrade</code> command.</p> <p>Using multiple values files can be a more efficient way to manage configurations, particularly in complex environments. This approach can help to avoid having a single, large values file that may be difficult to read and maintain. By dividing the configuration into smaller, more focused files, it can be easier to manage and update the settings as needed.</p> <p>To use multiple values files, you would simply provide multiple <code>-f valuesfle.yaml</code> options on the <code>helm upgrade</code> command.</p>"},{"location":"guide/values-files-management/#multiple-environments","title":"Multiple Environments","text":"<p>When deploying any application, it is often necessary to consider multiple environments, such as dev, uat, and prod. While it is possible to create a completely separate configuration for each environment, this approach can lead to a number of issues such as:</p> <ul> <li>Repetition and duplication of settings.</li> <li>Challenges synchronizing common settings in multiple environments.</li> <li>Configuration drift between common settings in each environment.</li> </ul> <p>A more effective approach may be to use a base configuration for the common settings, using overlays for per-environment settings.</p> <p>This allows you to define a set of common configuration settings that apply to all environments, while also allowing you to specify any environment-specific settings as needed. This can help to minimize repetition and ensure that the configuration is consistent across all environments.</p> <p>This can be easily achieved for multiple environments using a directory structure similar to this:</p> <pre><code>\u251c\u2500\u2500 common\n\u2502   \u251c\u2500\u2500 common-values.yaml\n\u2502   \u2514\u2500\u2500 files\n\u2502       \u2514\u2500\u2500 common-extra-script1.js\n\u251c\u2500\u2500 prod\n\u2502   \u251c\u2500\u2500 files\n\u2502   \u2502   \u2514\u2500\u2500 prod-extra-script1.js\n\u2502   \u2514\u2500\u2500 prod-values.yaml\n\u2514\u2500\u2500 uat\n    \u251c\u2500\u2500 files\n    \u2502   \u2514\u2500\u2500 uat-extra-script1.js\n    \u2514\u2500\u2500 uat-values.yaml\n</code></pre> <p>Note: This is just an example directory structure to demonstrate the concept. You should design an environment configuration that suits your organisation's configuration management workflow.</p>"},{"location":"guide/values-files-management/#modular-configurations","title":"Modular Configurations","text":"<p>Using multiple values files can also be a useful way to create modular units of configuration that can be easily included or excluded in your environment. This can help to make your configuration more flexible and adaptable to changing needs.</p> <p>For example, you might create a set of values files that represent different configurations or modules that have been fully tested and approved for use in your organization. These might include a base configuration file that defines the minimum requirements for running Smile CDR, as well as additional files for specific features or components, such as an R4 persistence module, enabling settings for bulk data import, enabling Kafka, or an AWS Healthlake module.</p> <p>Expanding on the above example for multiple environments, you could do something similar to this:</p> <pre><code>\u251c\u2500\u2500 common\n\u2502   \u251c\u2500\u2500 bulk-delete.yaml\n\u2502   \u251c\u2500\u2500 bulk-import.yaml\n\u2502   \u251c\u2500\u2500 common-values.yaml\n\u2502   \u251c\u2500\u2500 dqm.yaml\n\u2502   \u251c\u2500\u2500 dtr.yaml\n\u2502   \u251c\u2500\u2500 files\n\u2502   \u2502   \u2514\u2500\u2500 common-extra-script1.js\n\u2502   \u251c\u2500\u2500 kafka-strimzi.yaml\n\u2502   \u251c\u2500\u2500 large-uploads.yaml\n\u2502   \u251c\u2500\u2500 observability-dashboard.yaml\n\u2502   \u251c\u2500\u2500 postgres-crunchy-ha.yaml\n\u2502   \u251c\u2500\u2500 smart-auth.yaml\n\u2502   \u2514\u2500\u2500 troubleshooting.yaml\n\u251c\u2500\u2500 prod\n...\n</code></pre> <p>This approach allows you to build your configuration in a modular way, which can be more manageable and easier to maintain. It also gives you the flexibility to selectively include or exclude certain modules as needed, depending on the specific requirements of your environment.</p> <p>For more examples of this directory layout, you can study the examples that are located here</p>"},{"location":"guide/values-files-management/#flexible-solution","title":"Flexible Solution","text":"<p>When it comes to managing values files, there isn't a single \"right\" way to do it - the approach that works best will depend on specific needs and organizational standards.</p> <p>Although the above techniques can be helpful for keeping things organized and efficient, they may not be right for you or your organization. You should use a technique that works for your team and organization. If this means using a single large values file per environment or some other technique, then that is fine.</p> <p>The aim here is to find a solution that helps you maintain a stable, well-organized and easy to manage configuration repository.</p>"},{"location":"guide/secrets/","title":"Secrets Handling","text":"<p>Secrets management can be a hard subject to get right. Unfortunately, the easy way quite often lacks basic security considerations and can lead to unexpected data compromises.</p> <p>At Smile Digital Health, we take security very seriously, so we have designed these Helm Charts in a way that follows secure practices, to reduce the likelihood of such compromises.</p>"},{"location":"guide/secrets/#secrets-best-practices","title":"Secrets Best Practices","text":""},{"location":"guide/secrets/#use-temporary-credentials","title":"Use Temporary Credentials","text":"<p>As a general security best-practice, avoid using long-lived credentials where possible. This practice can be achieved when using cloud based environments, using their underlying cloud provider technologies.</p> <p>For example, on AWS the best practice is to use temporary credentials using IAM Instance Profiles and IAM Roles For Service Accounts (IRSA).</p>"},{"location":"guide/secrets/#dont-store-secrets-in-your-configuration-code","title":"Don't Store Secrets in your Configuration Code","text":"<p>There are some scenarios where long-lived secret values still need to be used in your deployment. These secret values may be required at the Kubernetes level, such as when pulling container images from private repositories, or at the application level, such as when connecting to databases or other external systems that require authentication.</p> <p>While it may simplify provisioning and automation by including secrets (such as passwords, certificates or API keys) in code, it is widely accepted as a bad practice as it can compromise the security of your system by increasing the attack surface area.</p> <p>If the code containing the secrets is somehow leaked, protected resources may be compromised. Additionally, if the code is shared among multiple team members, it can be difficult to control who has access to the secrets and when they were last rotated.</p>"},{"location":"guide/secrets/#use-secrets-management-tools","title":"Use Secrets Management Tools","text":"<p>It is recommended to use a secrets management tool to store and manage secrets separately from code. This way, secrets can easily be rotated and access can be tightly controlled.</p> <p>Various secrets management tools are available that allow you to store secrets in a secure, centralized location and control access to them through granular RBAC configurations. This helps ensure that only authorized systems have access to sensitive information, helping to prevent accidental disclosure of secret material and PHI.</p>"},{"location":"guide/secrets/#supported-secret-mechanisms","title":"Supported Secret Mechanisms","text":"<p>These Helm Charts support the following three methods to reference secrets.</p> Method Security Difficulty Notes IAM Auth for AWS RDS Highest Hard Recommended method for connecting to AWS RDS databases. Secrets Store CSI High Hard Recommended method where long-lived credentials are required. You will need the SSCSI driver, an appropriate SSCSI provider, A secrets vault Secret and any IAM roles configured to access the secret Kubernetes Secret Medium Medium Need to manually set up Kubernetes Secret"},{"location":"guide/secrets/#secrets-store-csi-driver","title":"Secrets Store CSI Driver","text":"<p>Using the Secrets Store CSI Driver(SSCSI) is the preferred method to configure secrets in these Helm Charts.</p> <p>This mechanism is recommended by AWS, Azure and Google to retrieve secrets from their respective secret management services. It also has support for other Secret Vault providers such as HashiCorp Vault.</p> <p>Currently, the Smile CDR Helm Chart only supports the Secrets Store CSI Driver with the AWS Secrets Manager provider.</p> <p>In order to use this method of configuring secrets, there are some pre-requisites that need to be in place.</p> <ul> <li>Create your secret in AWS Secrets Manager.<ul> <li>The secret data should be in a suitabe structured JSON format, as described here</li> <li>Your secret should be encrypted using an AWS CMK (Customer Managed Key).</li> </ul> </li> <li>Create an IAM role and trust policy.</li> <li>Enable the <code>ServiceAccount</code> and reference the IAM role in the annotations.</li> <li>See the Service Account Configuration section for more details on setting this up.</li> </ul> <p>Note: The process of creating secrets and configuring access to them can be greatly simplified if using the supporting dependencies Terraform Module</p>"},{"location":"guide/secrets/#kubernetes-secret","title":"Kubernetes Secret","text":"<p>Alternatively, you can create the Kubernetes <code>Secret</code> object through some other method. Although it avoids the secret data being included in your code, it does not provide a centralized location to store, manage and control access to secrets.</p> <p>Be wary of including custom Kubernetes <code>Secret</code> manifests alongside your Helm values files. Although this is a convenient way to provision them, it just re-introduces the problem of secrets residing in your code, which should be avoided.</p>"},{"location":"guide/secrets/#values-file-retired","title":"Values File (Retired)","text":"<p>Finally, we do support providing credentials in the values file itself. This is not a recommended solution and really only intended as a quickstart method to allow for quick experimentation with the charts.</p> <p>NOTE: Support for this feature has now been removed from the Helm Chart. This section remains as a reference and an explanation for why it is not an option. Although using this feature allowed for a quick setup of an environment, it encouraged users to use bad security practice in higher environments.</p>"},{"location":"guide/secrets/configuring-secrets/","title":"Configuring Secrets","text":""},{"location":"guide/secrets/configuring-secrets/#the-secretspec-configuration","title":"The <code>secretSpec</code> configuration","text":"<p>Secrets can be used in various areas within this Helm Chart. To simplify configuring secrets, they all make use of a <code>secretSpec</code> object that is used consistently in multiple locations.</p> <p>The <code>secretSpec</code> schema looks like this: <pre><code>name: friendlyname\n# sscsi or k8sSecret\ntype: sscsi\n# Provider required if type = sscsi\nprovider: aws\n# Name of K8s Secret resource (This will be lower-cased)\nsecretName: mySecretName\n# Required if type = sscsi and provider = aws\nsecretArn: arn:aws:secretsmanager:us-east-1:123456789010:secret:secretname\n\n# Secret projection.\n# To expose the secret into the environment, you need to configure each secret entry accordinly\n# by usint the `secretKeyMap`\n# For some configurations (e.g. external DB configurations) this is not required as the secret\n# will be automatically mounted in the appropriate fashion.\nsecretKeyMap:\n  # The object name here is unimportant\n  mySecretKey:\n    # The key name of the secret in the Secret vault, if using sscsi\n    secretKeyName: mySecretKeyName\n    # The key name of the secret in the kubernetes secret object\n    k8sSecretKeyName: myK8sSecretKeyName\n    # If specified, will mount secret on the filesystem\n    mountSpec:\n      mountPath: /home/smile/smilecdr/classes/mountedSecretFile\n    # If specified, will mount secret as an environment variable\n    envVarName: MY_ENV_VAR_NAME\n</code></pre></p>"},{"location":"guide/secrets/configuring-secrets/#where-to-configure-secrets","title":"Where to configure secrets","text":"<p>For information on how this <code>SecretSpec</code> is used elsewhere in the Helm Chart, refer to the following sections.</p> <ul> <li>Image Repository Credentials</li> <li>Database Credentials</li> <li>Smile CDR License</li> <li>Generic Secrets</li> </ul>"},{"location":"guide/secrets/configuring-secrets/#use-secrets-in-javascript-execution-environment","title":"Use Secrets In JavaScript Execution Environment","text":"<p>If you are using a callback function in your Smile CDR configuration, there are 2 ways you could use secrets from the script.</p> <ol> <li>Use the AWS Java SDK to retrieve secrets from AWS Secrets Manager, directly within your script.</li> <li>Use the Secrets Store CSI driver to retrieve and mount the secrets inside the pod.</li> </ol> <p>There are benefits and drawbacks to each of these mechanisms, however at this time only the first option is available.</p>"},{"location":"guide/secrets/configuring-secrets/#retrieve-aws-secrets-manager-secrets-from-callback-script","title":"Retrieve AWS Secrets Manager Secrets From Callback Script","text":"<p>You can use the below function to retrieve secrets directly from AWS Secrets Manager using the included AWS Java SDK and the graal-js engine's Java compatibility layer.</p>"},{"location":"guide/secrets/configuring-secrets/#iam-configuration","title":"IAM configuration","text":"<p>Ensure that the IAM role used by your Smile CDR service account has sufficient permissions to retrieve the secret. It should have the following statement within its IAM policy. <pre><code>{\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:DescribeSecret\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:secretsmanager:&lt;region&gt;:123456789010:secret:secretName\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n</code></pre></p>"},{"location":"guide/secrets/configuring-secrets/#define-function","title":"Define Function","text":"<p>Define the following re-usable <code>getSecretsManagerSecretV1()</code> function in your JavaScript callback function: <pre><code>function getSecretsManagerSecretV1 (secretName,secretKey,awsRegion='us-east-1') {\n  let DefaultAWSCredentialsProviderChain = Java.type('com.amazonaws.auth.DefaultAWSCredentialsProviderChain');\n  let AWSSecretsManagerClientBuilder = Java.type('com.amazonaws.services.secretsmanager.AWSSecretsManagerClientBuilder');\n  let GetSecretValueRequest = Java.type('com.amazonaws.services.secretsmanager.model.GetSecretValueRequest');\n  let Regions = Java.type('com.amazonaws.regions.Regions');\n\n\n  let credentialsProvider = new DefaultAWSCredentialsProviderChain();\n  let secretsManagerClient = AWSSecretsManagerClientBuilder.standard()\n    .withCredentials(credentialsProvider)\n    .withRegion(Regions.fromName(awsRegion))\n    .build();\n  let secretValueRequest = new GetSecretValueRequest().withSecretId(secretName);\n  let secretValue = secretsManagerClient.getSecretValue(secretValueRequest);\n  if (secretKey) {\n    // Just return the value for the specified secret key\n    return JSON.parse(secretValue.getSecretString())[secretKey];\n  } else {\n    // Return the entire secret string\n    return JSON.parse(secretValue.getSecretString());\n  }\n}\n</code></pre> Note: Currently this is using the AWS Java SDK Version 1 which is being deprecated. Support for the AWS Java SDK Version 2 will be added in a future version of Smile CDR.</p>"},{"location":"guide/secrets/configuring-secrets/#environment-variables","title":"Environment Variables","text":"<p>Set some appropriate environment variables using the extraEnvVars Helm Chart feature. <pre><code>extraEnvVars:\n- name: JS_EXAMPLE_SECRET_NAME\n  value: nameOrFullARNOfSecretsManagerSecret\n- name: JS_EXAMPLE_SECRET_KEY\n  value: keyname\n# Optional: Function defaults to `us-east-1`\n# - name: JS_EXAMPLE_SECRET_REGION\n#   value: us-west-2\n</code></pre></p>"},{"location":"guide/secrets/configuring-secrets/#retrieve-the-secret","title":"Retrieve the secret","text":"<p>Retrieve the environment variables and call the function to retrieve secret. <pre><code>/**\n * We use `onAuthenticateSuccess` here for demonstration purposes, but\n * this could be any Smile CDR callback function that uses the\n * JavaScript Execution Environment.\n */\n\nfunction onAuthenticateSuccess(theOutcome, theOutcomeFactory, theContext) {\n\n    Log.info('Getting Secret from AWS Secrets Manager...');\n\n    let secretName = Environment.getEnv('JS_EXAMPLE_SECRET_NAME');\n    let secretKey = Environment.getEnv('JS_EXAMPLE_SECRET_KEY');\n\n    /**\n     * Retrieve secret using default AWS region...\n     */\n    let secretValue = getSecretsManagerSecretV1(secretName,secretKey)\n\n    /**\n     * Or optionally retrieve and use the AWS region:\n     *\n     * let secretRegion = Environment.getEnv('JS_EXAMPLE_SECRET_REGION');\n     * let secretValue = getSecretsManagerSecretV1(secretName,secretKey)\n     */\n\n    /**\n     * Warning, this will leak your secret to the Smile CDR log files.\n     * It is only used here to demonstrate the functionality\n     */\n    Log.info('Value of secret ' + secretKey + ' is: ' + secretValue) ;\n\n    return theOutcome;\n}\n</code></pre></p>"},{"location":"guide/secrets/extra-secrets/","title":"Injecting Extra Secrets","text":"<p>There are a number of situations where it may be required to securely include extra secrets into the pod. For example, there could be JavaScript callbacks or interceptors that expect secrets to be available in the running environment.</p> <p>Depending on your exact architecture, there are two main ways that this may be done securely.</p>"},{"location":"guide/secrets/extra-secrets/#use-secretspec-configurations","title":"Use <code>secretSpec</code> configurations","text":"<p>To simplify configuring extra secrets, the same <code>secretSpec</code> object that is used elsewhere in the Helm Chart can be leveraged.</p> <p>Simply add a <code>secretSpec</code> configuration under the <code>secrets</code> object in your values file:</p> <p>The following will result in the <code>mySecretKeyName</code> key in the <code>mySecret</code> AWS Secrets Manager secret, being exposed as an environment variable as well as being mounted to the filesystem.</p> <pre><code>secrets:\n  myExtraSecret:\n    name: friendlyname\n    # sscsi or k8sSecret\n    type: sscsi\n    provider: aws\n    # Name of K8s Secret resource\n    secretName: mySecretName\n    secretArn: arn:aws:secretsmanager:us-east-1:123456789101:secret:mySecret\n    secretKeyMap:\n      mySecretKey:\n        secretKeyName: mySecretKeyName\n        k8sSecretKeyName: myK8sSecretKeyName\n        mountSpec:\n          mountPath: /home/smile/smilecdr/classes/mountedSecretFile\n        envVarName: MY_ENV_VAR_NAME\n</code></pre>"},{"location":"guide/smilecdr/","title":"Smile CDR Helm Chart User Guide","text":"<p>This guide covers configuration details for the Smile CDR Helm Chart</p>"},{"location":"guide/smilecdr/#smile-cdr-configuration-options","title":"Smile CDR Configuration Options","text":"<p>Please note that for details on the Smile CDR configuration options, you should consult the official product documentation on the Smile CDR website</p>"},{"location":"guide/smilecdr/cdrversions/","title":"Supported Smile CDR Versions","text":"<p>By default, this Helm Chart supports the latest published version of the Smile CDR docker image.</p>"},{"location":"guide/smilecdr/cdrversions/#important-set-your-image-tag","title":"Important! Set Your Image Tag","text":"<p>If you need to pin to a specific version of Smile CDR, be sure to specify the appropriate value for <code>image.tag</code> in your values file. If you fail to do this, your deployment may get unexpectedly upgraded when using a newer version of the Helm Chart.</p> <p>Set <code>image.tag</code> to your required version</p>"},{"location":"guide/smilecdr/cdrversions/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>image:\n  tag: \"2024.05.R03\"\n</code></pre> <p>Warning Pre-release versions of this Helm Chart (In the <code>devel</code> channel) may default to pre-release versions of Smile CDR. Always update the image tag when using this Helm Chart to update an existing installation that is running a previous version of Smile CDR.If you do not perform this step, Smile CDR may automatically upgrade your database to the latest version, which may be an irreversible step!</p>"},{"location":"guide/smilecdr/cdrversions/#current-smile-cdr-version","title":"Current Smile CDR Version","text":""},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202405r04","title":"Smile CDR <code>2024.05.R04</code>","text":"<p>Versions <code>v1.0.0</code> and newer of the chart support the latest production release of Smile CDR - <code>2024.05.R03</code> and above.</p>"},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202308","title":"Smile CDR <code>2023.08</code>","text":"<p>Versions <code>v1.0.0-pre.92</code> and newer of the chart support the latest production release of Smile CDR - <code>2023.08.R01</code> and above.</p> <p>Please refer to the Smile CDR changelog for more information on feature changes.</p>"},{"location":"guide/smilecdr/cdrversions/#version-support-table","title":"Version Support Table","text":"<p>For each Smile CDR version, this table shows the Min and Max Helm Chart version that officially support it.</p> <p>Older Smile CDR versions will not work beyond the chart version in the Extra column - see the notes.</p> Smile CDR Min Max Extra Notes <code>2024.11</code> <code>v3.0.0</code> tbd tbd Future Release <code>2024.08</code> <code>v2.0.0</code> tbd tbd Future Release <code>2024.05</code> <code>v1.0.0</code> tbd tbd Note 1 <code>2024.02</code> <code>v1.0.0</code> tbd tbd Note 1 <code>2023.11</code> <code>v1.0.0</code> tbd tbd Note 1 <code>2023.08</code> <code>v1.0.0</code> tbd tbd Note 1 <code>2023.05</code> <code>v1.0.0</code> tbd tbd Note 1 <code>2023.02</code> <code>v1.0.0-pre.52</code> <code>v1.0.0-pre.78</code> tbd Note 2"},{"location":"guide/smilecdr/cdrversions/#notes","title":"Notes","text":"<p>Note 1 If using older version of Smile CDR with newer version of the Helm Chart, ensure that you have the correct <code>image</code> value provided.</p> <p>Note 2 Unsupported beyond the 'Max' version. If using older version of Smile CDR with newer version of the Helm Chart, please see the section below for any compatibility considerations.</p>"},{"location":"guide/smilecdr/cdrversions/#upgrading","title":"Upgrading","text":"<p>When upgrading from older versions of Smile CDR, there may be some additional required steps.</p> <p>Changes across multiple versions may be cumulative, so you should perform any upgrade steps one major version (Of Smile CDR) at a time.</p>"},{"location":"guide/smilecdr/database/","title":"Database Configuration","text":"<p>To use this chart, you must configure a database. There are two ways to do this:</p> <ul> <li>Use or provision an external database (or databases) using existing techniques/processes in your   organisation. Any external database can be referenced in this chart and Smile CDR will be configured   to use it.</li> <li>As a quick-start convenience, support has been included to provision a PostgreSQL cluster locally in   the Kubernetes cluster using the CrunchyData PostreSQL Operator. When enabling this option, the   database(s) will be automatically created and Smile CDR will be configured to connect to it/them.</li> </ul> <p>If you do not specify one or the other, the chart will fail to render any output and will return a descriptive error instead.</p> <p>WARNING - Do not use built-in H2 database: Due to the ephemeral and stateless nature of Kubernetes Pods, there is no use case where it makes sense to provision Smile CDR using the internal H2 database. You are free to configure your persistence module to do so, but every time the Pod restarts, it will start with an empty database and will perform a fresh install of Smile CDR. In addition to this, if you were to configure multiple replicas, each Pod would appear as its own distinct Smile CDR install.</p> <p>As such, you should not configure Smile CDR in this fashion and you must instead provision some external database.</p>"},{"location":"guide/smilecdr/database/#referencing-externally-provisioned-databases","title":"Referencing Externally Provisioned Databases","text":"<p>To reference a database that is external to the cluster, you will need:</p> <ul> <li>Network connectivity from the K8s cluster to your database.</li> <li>A secret containing the connection credentials in a structured Json format.<ul> <li>It is common practice to include all connection credentials in DB secrets, this way it becomes simple to manage the database without having to reconfigure Smile CDR. e.g. when 'restoring' an RDS instance, the DB cluster name will typically change. By keeping these details inside the secret then any such change will be automatically applied without reconfiguring. See here for info on the schema used by AWS for this purpose. Note that an app restart will be required to pick up the new secret value.</li> <li>The secret can be a Kubernetes <code>Secret</code> object that you provision through some external mechanism, or it can be a secret in a secure secrets vault. The latter is the preferred option for increased security and the ability to easily rotate credentials. At this time, the only supported secrets vault is AWS Secrets Manager, using the Secrets Store CSI Driver. See the Secrets Handling section for more info on this.</li> <li>It's also possible to configure connection details directly without using a secret object. However, any secret material such as passwords MUST use a secret mechanism or alternatively use IAM (See below).</li> </ul> </li> <li>Authorization can either use a password stored in a secret, or if using certain AWS RDS database types, it can use IAM Authentication.<ul> <li>When using IAM Authentication, the IAM Role assigned to the Pod's <code>ServiceAccount</code> must have the appropriate IAM policy.</li> <li>Other connection details (e.g. DB URL, Port, database name, username) can either be configured using a Secret object as per above, or they can be specifuid directly in the configuration.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/database/#database-connection-configuration-schema","title":"Database Connection Configuration Schema","text":"<p>External database connections are configured in the <code>database.external</code> section of your values file as follows: <pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        # sscsi or k8sSecret\n        source: sscsi\n        provider: aws\n        # pwd or iam\n      connectionConfig:\n        authentication:\n         type: pwd\n    databases:\n    - name: clustermgrdb-iam\n      modules:\n      - clustermgr\n      connectionConfigSource:\n        source: none\n      connectionConfig:\n        authentication:\n          type: iam\n          provider: aws\n        url: clustermgr-db-url\n        port: clustermgr-db-port\n        dbName: clustermgr-db-name\n        user: clustermgr-db-user\n</code></pre></p>"},{"location":"guide/smilecdr/database/#legacy-database-connection-configuration-schema","title":"Legacy Database Connection Configuration Schema","text":"<p>In Helm Chart version v1.0.0-pre.121 and older, the Database schema was more restrictive:</p> <ul> <li>It did not allow for different credential mechanisms for each database</li> <li>It did not follow the same <code>secretSpec</code> schema used elsewhere in the Helm Chart.</li> </ul>"},{"location":"guide/smilecdr/database/#using-aws-secret-json-structure","title":"Using AWS Secret JSON structure","text":"<p>If you are using the above mentioned JSON structure (i.e. <code>engine</code>, <code>host</code>, <code>username</code>, <code>password</code>, <code>dbname</code> and <code>port</code>) in your secret, then you should simply configure your secret as per the following yaml fragment. Those default keys will be used to extract the credentials.</p> <pre><code>database:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n      module: clustermgr\n</code></pre> <p>Note: This legacy schema has been deprecated and will be removed from a future version of the Helm Chart.</p>"},{"location":"guide/smilecdr/database/#migrating-from-legacy-to-new-database-connection-configuration-schema","title":"Migrating from legacy to new Database Connection Configuration Schema","text":"<p>To aid in upgrading to the new version of the DB Connection Configuration Schema, Helm Chart version v1.0.0-pre.122 and newer will fail with a descriptive error message.</p> <p>To convert, you should do the following:</p> <ul> <li>Move configuration from <code>database.external.credentials</code> section to <code>database.external.defaults.connectionConfigSource</code> or to <code>database.external.databases.[databaseIndex].connectionConfigSource</code></li> <li>Move <code>secretName</code> and <code>secretArn</code> from <code>database.external.databases.[databaseIndex]</code> to <code>database.external.databases.[databaseIndex].connectionConfigSource</code></li> <li>Change <code>database.external.databases.[databaseIndex].module</code> from a single value to an array of values at <code>database.external.databases.[databaseIndex].modules</code></li> </ul> <p>The above provided example would become:</p> <pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        source: sscsi\n        provider: aws\n    databases:\n    - connectionConfigSource:\n        secretName: clustermgrSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n      modules:\n      - clustermgr\n</code></pre>"},{"location":"guide/smilecdr/database/#settings-reference","title":"Settings Reference","text":"<p><code>databases</code> Section</p> <p>This section contains a list of all external database connection configurations.</p> Key Value Required Notes <code>name</code> A friendly reference name for this connection. <code>modules</code> List of modules that this connection is used by If not present, connection may be used by any modules. <code>connectionConfigSource</code> Source of connection credentials (e.g. secrets) Not required if sufficient defaults are defined <code>connectionConfig</code> Directly configured connection configurations Not required if sufficient defaults are defined <p><code>connectionConfigSource</code> Section</p> <p>This section configures where the database connection settings are pulled from. Any required configurations must come from the <code>defaults</code> section or from the per-connection section. If they are not provided anywhere, the chart will fail with a descriptive error message.</p> Key Value Required Notes <code>source</code> <code>sscsi</code>,<code>k8sSecret</code> or <code>none</code> Can only use <code>none</code> if using IAM Authentication <code>provider</code> SSCSI Provider, e.g. <code>aws</code> Required if using Secrets Store CSI Driver (sscsi). Only <code>aws</code> provider is currently supported. <code>secretName</code> Secret object name Required if using Kubernetes Secrets (<code>k8sSecret</code>) or Secrets Store CSI Driver (<code>sscsi</code>). <code>secretArn</code> Secrets Manager ARN Required if using Secrets Store CSI Driver (<code>sscsi</code>). <code>secretKeyMappings</code> Dictionary of key mappings If you are using key names in your secrets that differ from the defaults, you can provide a custom mapping <p><code>secretKeyMappings</code> Section</p> <p>This section lets you configure custom key mappings. If you are not using the above mentioned Json structure (i.e. <code>engine</code>, <code>host</code>, <code>username</code>, <code>password</code>, <code>dbname</code> and <code>port</code>) in your secret, they can be overridden by specifying them with the <code>*Key</code> attributes to override the defaults</p> Key Value <code>urlKey</code> Secret Key Name for URL <code>portKey</code> Secret Key Name for Port <code>dbNameKey</code> Secret Key Name for db name <code>userKey</code> Secret Key Name for username <code>passKey</code> Secret Key Name for password <p><code>connectionConfig</code> Section</p> <p>This section is used to directly specify DB connection configurations.</p> Key Value Required Notes <code>authentication</code> Define authentication mechanism Defaults to use 'password' authentication. This implies you cannot use <code>none</code> as the config source, as you need a password to be provided via a secure mechanism. <code>authentication.provider</code> Define authentication provider Required when using <code>iam</code> authentication. Only <code>aws</code> provider is currently supported. <code>url</code> Database hostname This value acts as an override and will be used even if the credential is provided in a secret object. <code>port</code> Database port This value acts as an override and will be used even if the credential is provided in a secret object. <code>dbName</code> Database name This value acts as an override and will be used even if the credential is provided in a secret object. <code>user</code> Database username This value acts as an override and will be used even if the credential is provided in a secret object."},{"location":"guide/smilecdr/database/#defaults-section","title":"<code>defaults</code> Section","text":"<p>To avoid duplication of configuration settings, it's possible to set defaults for the <code>connectionConfigSource</code> and <code>connectionConfig</code> sections.</p>"},{"location":"guide/smilecdr/database/#example-external-database-configurations","title":"Example External Database Configurations","text":""},{"location":"guide/smilecdr/database/#using-secret-store-csi-and-default-json-structure","title":"Using Secret Store CSI and default Json structure","text":"<pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        source: sscsi\n        provider: aws\n      connectionConfig:\n        authentication:\n          type: pwd\n    databases:\n    - name: clustermgrSecret\n      modules:\n      - clustermgr\n      connectionConfigSource:\n        secretName: clustermgrSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n    - name: persistenceSecret\n      modules:\n      - persistence\n      connectionConfigSource:\n        secretName: persistenceSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:persistenceSecret\n    - name: auditSecret\n      modules:\n      - audit\n      connectionConfigSource:\n        secretName: auditSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:auditSecret\n    - name: transactionSecret\n      modules:\n      - transaction\n      connectionConfigSource:\n        secretName: transactionSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:transactionSecret\n</code></pre>"},{"location":"guide/smilecdr/database/#using-custom-secret-json-structure","title":"Using Custom Secret Json structure","text":"<p>If the Json keys in your secret are different than above, they can be overridden by specifying them with the <code>*Key</code> attributes to override the defaults.</p> <p>The below is just an incomplete example to demonstrate how the Json keys can be overridden. You need to ensure that this matches the configuration of your secret and the keys it contains.</p> <p>Note: <code>clustermgrSecret</code> can be any friendly name, it's not important. The Kubernetes <code>Secret</code> resource will be named using this value.</p>"},{"location":"guide/smilecdr/database/#using-custom-secret-json-structure_1","title":"Using Custom Secret JSON structure","text":"<p>If the JSON keys in your secret are different than above, they can be overridden by specifying them with the <code>*Key</code> attributes to override the defaults.</p> <p>The below are just examples, to show how the JSON keys can be overridden. You need to ensure that this matches the configuration of your secret and the keys it contains.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        source: sscsi\n        provider: aws\n        secretKeyMappings:\n          urlKey: url-key-name\n          portKey: port-key-name\n          dbnameKey: dbname-key-name\n          userKey: user-key-name\n          passKey: password-key-name\n      connectionConfig:\n        authentication:\n         type: pwd\n    databases:\n    - name: clustermgrSecret\n      modules:\n      - clustermgr\n      connectionConfigSource:\n        secretName: clustermgrSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n</code></pre>"},{"location":"guide/smilecdr/database/#using-iam-authentication-with-secret-for-connection-details","title":"Using IAM Authentication with Secret for connection details","text":"<pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        source: sscsi\n        provider: aws\n      connectionConfig:\n        authentication:\n          type: iam\n          provider: aws\n    databases:\n    - name: clustermgrSecret\n      modules:\n      - clustermgr\n      connectionConfigSource:\n        secretName: clustermgrSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n    - name: persistenceSecret\n      modules:\n      - persistence\n      connectionConfigSource:\n        secretName: persistenceSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:persistenceSecret\n    - name: auditSecret\n      modules:\n      - audit\n      connectionConfigSource:\n        secretName: auditSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:auditSecret\n    - name: transactionSecret\n      modules:\n      - transaction\n      connectionConfigSource:\n        secretName: transactionSecret\n        secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:transactionSecret\n</code></pre>"},{"location":"guide/smilecdr/database/#using-iam-authentication-without-secret-for-connection-details","title":"Using IAM Authentication without Secret for connection details","text":"<pre><code>database:\n  external:\n    enabled: true\n    defaults:\n      connectionConfigSource:\n        source: none\n      connectionConfig:\n        authentication:\n          type: iam\n          provider: aws\n        url: shared-db-url\n        port: shared-db-port\n    databases:\n    - name: clustermgrSecret\n      modules:\n      - clustermgr\n      connectionConfig:\n        dbName: clustermgr-db-name\n        user: clustermgr-db-user\n    - name: persistenceSecret\n      modules:\n      - persistence\n      connectionConfig:\n        dbName: persistence-db-name\n        user: persistence-db-user\n    - name: auditSecret\n      modules:\n      - audit\n      connectionConfig:\n        dbName: audit-db-name\n        user: audit-db-user\n    - name: transactionSecret\n      modules:\n      - transaction\n      connectionConfig:\n        dbName: transaction-db-name\n        user: transaction-db-user\n</code></pre>"},{"location":"guide/smilecdr/database/#provide-connection-details-directly","title":"Provide connection details directly","text":"<p>If a required field is not included in the secret, you can specify it in a database connection section like so.</p> <pre><code>databases:\n  - name: clustermgrSecret\n    modules:\n    - clustermgr\n    connectionConfigSource:\n      secretName: clustermgrSecret\n      secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n    connectionConfig:\n      url: db-url # this is the actual url/hostname\n      port: 5432\n      dbname: dbname\n      user: username\n</code></pre> <p>NOTE: You cannot override the passKey value. The password will always come from the referenced secret unless you are using IAM.</p>"},{"location":"guide/smilecdr/database/#using-crunchydata-pgo-databases","title":"Using CrunchyData PGO Databases","text":"<p>This chart supports automatic creation of an in-cluster Postgres database using the CrunchyData Postgres Operator (PGO).</p> <p>In order to use this feature, you will need to ensure that your K8s cluster already has the operator installed (Operator installation instructions here).</p> <p>After the PGO operator is installed and configured in your Kubernetes cluster, you can enable this feature using the following yaml fragment for your database configuration:</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_1","title":"<code>my-values.yaml</code>","text":"<p><pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre> This will create a 2 instance HA PostgreSQL cluster, each with 1cpu, 2GiB memory and 10GiB storage. These defaults can be configured using <code>database.crunchypgo.config</code> keys.</p> <p>Backups are enabled by default as it's a feature of the Operator.</p>"},{"location":"guide/smilecdr/database/#configuring-multiple-databases","title":"Configuring Multiple Databases","text":"<p>This chart has support to use multiple databases. It is recommended (and in some cases, required) to configure Smile CDR this way, with a separate DB for the Cluster Manager, Audit logs, Transaction logs and for any Persistence Modules.</p> <p>NoteIf there is only one database configured then it will be used for all modules.</p>"},{"location":"guide/smilecdr/database/#module-autoconfiguration-of-databases","title":"Module Autoconfiguration of Databases","text":"<p>This Helm Chart will automatically configure any Smile CDR modules that use a database.</p> <p>If you configure multiple databases, the <code>module</code> key specified for each one is used to determine which Smile CDR module is using it. This key is important as it tells the Helm Chart which module uses this database.</p>"},{"location":"guide/smilecdr/database/#environment-variables","title":"Environment Variables","text":"<p>In the examples below, the <code>clustermgr</code>, <code>audit</code>, <code>transaction</code> and <code>persistence</code> modules will automatically have their own set of environment variables configured for DB connections as follows: <code>CLUSTERMGR_DB_*</code>, <code>AUDIT_DB_*</code>, <code>TRANSACTION_DB_*</code> and <code>PERSISTENCE_DB_*</code></p>"},{"location":"guide/smilecdr/database/#module-configuration","title":"Module Configuration","text":"<p>As the modules are configured automatically, you must NOT manually update your module configurations to point to these environment variable references.</p> <p>When a given module is configured, any <code>DB_*</code> placeholders in the Helm Values files are automatically replaced with the appropriate <code>&lt;modulename&gt;_DB_*</code> values.</p> <p>For example the default Cluster Manager values file has DB connection settings that look like this:</p> <p><pre><code>db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\n</code></pre> The Helm Chart will generate a Smile CDR properties file with automatically updated values to match the environment variables, like so:</p> <pre><code>module.clustermgr.config.db.url      = jdbc:postgresql://#{env['CLUSTERMGR_DB_URL']}:#{env['CLUSTERMGR_DB_PORT']}/#{env['CLUSTERMGR_DB_DATABASE']}?sslmode=require\nmodule.clustermgr.config.db.password = #{env['CLUSTERMGR_DB_PASS']}\nmodule.clustermgr.config.db.username = #{env['CLUSTERMGR_DB_USER']}\n</code></pre> <p>This will happen automatically for any module that references <code>DB_*</code> environment variables.</p> <p>With multiple databases, the examples given above may look like this:</p>"},{"location":"guide/smilecdr/database/#external-multiple-database-example","title":"External Multiple Database Example","text":""},{"location":"guide/smilecdr/database/#my-valuesyaml_2","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: smilecdr\n      module: clustermgr\n    - secretName: smilecdr-audit\n      module: audit\n    - secretName: smilecdr-txlogs\n      module: transaction\n    - secretName: smilecdr-pers\n      module: persistence\n</code></pre>"},{"location":"guide/smilecdr/database/#crunchydata-pgo-multiple-database-example","title":"CrunchyData PGO Multiple Database Example","text":"<p>NoteThe CrunchyData PGO is a little different from the above as it uses the concept of 'users' in the configuration to configure multiple databases. That is why we are specifying multiple users here:.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_3","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    users:\n    - name: smilecdr\n      module: clustermgr\n    - name: smilecdr-audit\n      module: audit\n    - name: smilecdr-txlogs\n      module: transaction\n    - name: smilecdr-pers\n      module: persistence\n</code></pre>"},{"location":"guide/smilecdr/database/#crunchydata-database-name-suffixes","title":"CrunchyData Database Name Suffixes","text":"<p>When using CrunchyData PGO for experimenting with different Smile CDR configurations, it is often convenient to experiment with a fresh (empty) database, or flip back and forth between multiple database configurations.</p> <p>There are multiple ways this could be done:</p> <ul> <li>Deprovision/Reprovision the Db cluster - Destructive and time consuming</li> <li>Manually drop and recreate databases - Destructive and time consuming. Also requires DB tooling and connectivity.</li> <li>Reconfigure the database definitions in the <code>crunchypgo</code> section of the values file</li> </ul> <p>These methods have shortcomings that can slow down progress of testing initiatives.</p> <ul> <li>All of these options can be time consuming and error prone</li> <li>Any dropped and recreated databases will naturally lose any data</li> <li>Manually reconfiguring, although non-destructive, can get tedious and error-prone when dealing with configurations that have many databases, especially if you wish to 'flip' back and forth between different database configurations.</li> </ul> <p>As a convenience function, it is possible to quickly alter the database names, either individually or as an entire group. This can be done by using <code>dbName</code> or <code>dbSuffix</code>. The default suffix is <code>-db</code> and <code>-</code> will be prefixed on any provided suffix.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_4","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    users:\n    # Unaltered DB name will be `clustermgr-db`\n    - name: smilecdr\n      module: clustermgr\n\n    # Overridden DB name with disabled suffix will be `my-persistence-db-name`\n    - name: smilecdr-pers\n      module: persistence\n      dbName: my-persistence-db-name\n      dbSuffix: \"\"\n\n    # Default DB name with overridden suffix will be `audit-mydbsuffix`\n    - name: smilecdr-audit\n      module: audit\n      dbSuffix: \"-mydbsuffix\"\n</code></pre> <p>To reduce needing to specify the db suffix for multiple databases, the default suffix can be changed by setting <code>defaultDbSuffix</code> at the root level of the <code>crunchypgo</code> configuration as follows:</p> <p>NoteYou can still override the suffix for a single DB, as can be seen below with the <code>audit</code> and <code>persistence</code> modules below.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_5","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    defaultDbSuffix: test2\n    users:\n    # DB name will be `clustermgr-test2`\n    - name: smilecdr\n      module: clustermgr\n\n    # DB name will be `audit-test1`\n    - name: smilecdr-audit\n      module: audit\n      dbSuffix: test1\n\n    # DB name will be `transaction-test2`\n    - name: smilecdr-txlogs\n      module: transaction\n\n    # DB name will be `persistence`\n    - name: smilecdr-pers\n      module: persistence\n      dbSuffix: \"\"\n</code></pre>"},{"location":"guide/smilecdr/envvars/","title":"Custom Environment Configuration","text":"<p>Sometimes you may have custom components in your Smile CDR deployment that need to have configurations provided to them.</p> <p>In Smile CDR, this can be done one in of two ways.</p> <ul> <li>Java System Property Substitution</li> <li>System Environment Variable Substitution</li> </ul> <p>You can read more about these methods in the official Smile CDR documentation here</p> <p>Currently, this Helm Chart only supports using the second mechanism - System Environment Variable Substitution</p>"},{"location":"guide/smilecdr/envvars/#passing-extra-environment-variables","title":"Passing Extra Environment Variables","text":"<p>In order to configure extra environment variables into the pod, use the <code>extraEnvVars</code> section in your values file as follows:</p> <pre><code>extraEnvVars:\n- name: MYENVVARNAME\n  value: my-env-var-value\n- name: MYOTHERENVVARNAME\n  value: my-other-env-var-value\n</code></pre> <p>This is a list of objects that follow the same <code>env</code> schema as the Kubernetes <code>podSpec.containers</code> See here</p> <p>Note: Although it is possible to use this to add secrets using <code>env.valueFrom.secretKeyRef</code>, it's recommended to instead use one of the alternative mechanisms to pass in secret data to your pod. See the Secrets section for more info on this.</p> <p>Warning: Do not use this mechanism to set the <code>JVMARGS</code> environment variable. If you attempt this, the chart will fail with a warning. If you need to pass extra arguments to the JVM, refer to the JVM Configuration section.</p>"},{"location":"guide/smilecdr/envvars/#multi-node-configuration","title":"Multi-node configuration","text":"<p>If deploying Smile CDR in a multi-node configuration, you may wish to have different environment variables for the different CDR Nodes.</p> <p>Note: For more details on how to deploy Smile CDR with multi-node configurations, please refer to the CDR Nodes section.</p> <pre><code>cdrNodes:\n  node1:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node1-env-var-value\n  node2:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node2-env-var-value\n  node3:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node3-env-var-value\n    - name: GLOBALENVVARNAME\n      value: node3-overridden-global-value\n\nextraEnvVars:\n- name: GLOBALENVVARNAME\n  value: my-global-env-var-value\n</code></pre> <p>In the above configuration example:</p> <ul> <li>Each node gets its own set of extra environment variables.</li> <li>Each node gets the <code>GLOBALENVVARNAME</code> variable set to <code>my-global-env-var-value</code> except...</li> <li>Node3 has overridden the <code>GLOBALENVVARNAME</code> variable to <code>node3-overridden-global-value</code></li> </ul> <p>This allows for flexible configuration of extra environment variables in any Smile CDR configuration.</p>"},{"location":"guide/smilecdr/ingress/","title":"Configuring Ingress","text":"<p>This chart enables flexible Kubernetes Ingress resource configuration, supporting multiple Ingress and IngressClass resources for versatile application traffic routing.</p> <p>Currently supported controllers include Nginx Ingress, AWS Load Balancer, and Azure Application Gateway.</p>"},{"location":"guide/smilecdr/ingress/#ingress-type","title":"Ingress Type","text":"<p>This chart uses a concept of ingress <code>Type</code> to determine what kind of Ingress Controller is being used. It should not be confused with the <code>IngressClass</code>.</p> <p>The following ingress types are currently supported:</p> <ul> <li><code>nginx-ingress</code> (Default)</li> <li><code>aws-lbc-alb</code></li> <li><code>azure-agic</code></li> </ul> <p>This setting is used to help automatically configure the <code>Service</code> and <code>Ingress</code> resources so that the configured Ingress Controller can configure infrastructure resources appropriately.</p> <p>The ingress type for any given ingress can be set with <code>ingresses.default.type: ingress-type</code>.</p> <p>NOTE: If migrating from Helm Chart versions older than <code>v1.0.0-pre.104</code>, you will need to adjust from the old config schema to the new one. This can be done by moving any configurations that were previously under <code>ingress.*</code> to <code>ingresses.default.*</code>. e.g. <code>ingress.type: aws-lbc-alb</code> would become <code>ingresses.default.type: aws-lbc-alb</code></p>"},{"location":"guide/smilecdr/ingress/#nginx-ingress","title":"Nginx Ingress","text":"<p>By default, this chart is configured to use the Nginx Ingress Controller. <code>ingresses.default.type</code> is already set to <code>nginx-ingress</code> so you do not need to do anything to use this Ingress Controller.</p> <p>The behavior of the Nginx Ingress controller differs based on the cloud provider being used. When used in conjunction with the AWS Load Balancer Controller, the Nginx Ingress will be backed by an AWS NLB (Network Load Balancer).</p> <p>The <code>Service</code> objects will be set as <code>ClusterIP</code> rather than <code>NodePort</code>. This increases the security stance of the deployment as Kubernetes does not expose these services externally to the cluster. All traffic comes from the Nginx Ingress Controller pod, directly to the application pods.</p>"},{"location":"guide/smilecdr/ingress/#tls-encryption","title":"TLS Encryption","text":"<p>By default the Nginx ingress uses a self-signed certificate. If this is sufficient for your needs (as the fronting AWS NLB does not need to verify the certificates), nothing else needs to be done.</p> <p>If, however, you need to use a publicly-signed TLS certificate in the Nginx ingress (for example, using TLS passthrough on the fronting load balancer) then you can configure Nginx Ingress to use cert-manager and Let's Encrypt to automatically provision a suitable TLS certificate.</p> <p>For a detailed explanation of how this solution works, refer to the ingress-nginx and cert-manager documentation.</p> <p>This can currently be configured using the Helm Chart using two methods</p>"},{"location":"guide/smilecdr/ingress/#option-1","title":"Option 1","text":"<p>Create Let's Encrypt Staging <code>Issuer</code> Resource</p> <p>Step 1</p> <p>Use the below <code>values.yaml</code> snippet to enable the Helm Chart to generate a Let's Encrypt issuer. <pre><code>tls:\n  certificateIssuers:\n    default:\n      enabled: true\n      signingMethod: public-signed\n      acmeSpec:\n        email: test@example2.com\n</code></pre></p> <p>Step 2</p> <p>Update your ingress configuration to enable TLS and use the issuer configuration defined above. <pre><code>ingresses:\n  default:\n    tlsConfig:\n      enabled: true\n      issuerConfiguration: default\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#option-2","title":"Option 2","text":"<p>Use pre-existing <code>Issuer</code> Resource</p> <p>Step 1</p> <p>Create a suitable public-signing <code>Issuer</code> resource. Refer to the cert-manager documentation for instructions on doing this.</p> <p>In the rest of this example, we will assume that this existing <code>Issuer</code> resource is named <code>my-existing-lets-encrypt-issuer</code></p> <p>Step 2</p> <p>Use the below <code>values.yaml</code> snippet to define the pre-existing Issuer in a way that it can be used by multiple sections of the Helm Chart. <pre><code>tls:\n  certificateIssuers:\n    myExistingLetsEncryptIssuer:\n      enabled: true\n      signingMethod: public-signed\n      existingIssuer: my-existing-lets-encrypt-issuer\n</code></pre></p> <p>Note: Although the above is not technically mandatory for Ingress TLS configuration, it's advisable to use this mechanism. By doing so, the same issuer can easily be used to create other certificates that can be used in the Smile CDR deployment.</p> <p>Step 3</p> <p>Update your ingress configuration to enable TLS and use the issuer defined above. Note that you can either reference the above-defined <code>tls.certificateIssuers</code> configuration, or you can directly reference the pre-existing <code>Issuer</code> resource that you created. <pre><code>ingresses:\n  default:\n    tlsConfig:\n      enabled: true\n      issuerConfiguration: myExistingLetsEncryptIssuer\n      # If directly referencing the pre-existing `Issuer` resource that you created, use the following instead\n      # existingIssuer: my-existing-lets-encrypt-issuer\n</code></pre></p> <p>Note: For brevity, these examples do not enable back-end encryption to the Smile CDR pods. Please see the TLS Encryption section for more information on enabling back-end encryption, configuring Issuers, Certificates and using them in Smile CDR.</p>"},{"location":"guide/smilecdr/ingress/#byo-certificate","title":"BYO Certificate","text":"<p>At this time, the Helm Chart does not support providing your own externally-provisioned TLS certificate. This feature will be added at a future date.</p>"},{"location":"guide/smilecdr/ingress/#dedicated-nginx-ingress","title":"Dedicated Nginx Ingress","text":"<p>By default, this option uses the <code>nginx</code> ingress class. If multiple ingresses all use the same default IngressClass, then they will share the same underlying NLB.</p> <p>If you need to use a dedicated (or multiple) NLBs for this deployment, you can do so by first creating any required Nginx Ingress Controllers with a different IngressClass name. You can then specify this ingress class with <code>ingresses.default.ingressClassNameOverride</code>.</p>"},{"location":"guide/smilecdr/ingress/#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>To directly use the AWS Load Balancer Controller set <code>ingresses.default.type</code> to <code>aws-lbc-alb</code>. By default, this option uses the <code>alb</code> ingress class.</p> <p>This automatically adds appropriate default <code>Ingress</code> annotations for the AWS Load Balancer Controller. The controller will then create an AWS ALB (Application Load Balancer).</p> <p>You will still need to add some extra annotations, such as <code>alb.ingress.kubernetes.io/certificate-arn</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#known-problems","title":"Known Problems","text":"<p>There is currently a problem with the AWS Load Balancer Controller configuration where the health checks do not function as expected. This is somewhat mitigated by the fact that the <code>Service</code> objects are using <code>NodePort</code>. This will be addressed in a future release of this chart.</p>"},{"location":"guide/smilecdr/ingress/#azure-application-gateway-ingress-controller","title":"Azure Application Gateway Ingress Controller","text":"<p>If you wish to use the Azure Application Gateway Ingress Controller (AGIC), set <code>ingresses.default.type</code> to <code>azure-agic</code>. By default, this option uses the <code>azure/application-gateway</code> ingress class.</p> <p>When using this method, the chart will automatically add <code>Ingress</code> annotations for the Azure Application Gateway Controller. The controller will then create an Azure Application Gateway to be used as ingress.</p> <p>You will still need to add some extra annotations, such as <code>appgw.ingress.kubernetes.io/appgw-ssl-certificate</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#multiple-ingress-resources","title":"Multiple Ingress Resources","text":"<p>This chart allows for configurations using an arbitrary number of ingress resources and ingress classes.</p> <p>This enables the implementation of architectures that require multiple routes for accessing for your environment. For example, you may require some services available publicly while others, such as the Admin Web Console, may only permit access from a private network.</p>"},{"location":"guide/smilecdr/ingress/#default-ingress","title":"Default Ingress","text":"<p>In order to simplify deployment of certain architectures, this chart supports the concept of a 'default' Ingress resource.</p> <p>The default Ingress resource will be used for any Smile CDR modules that have not explicitly defined <code>service.ingresses</code> in their configuration.</p>"},{"location":"guide/smilecdr/ingress/#the-pre-defined-default-ingress-configuration","title":"The pre-defined Default Ingress Configuration","text":"<p>If no changes are made to the <code>ingresses</code> section of your Helm Values, a pre-defined Ingress Configuration is created, that effectively looks like this: <pre><code>ingresses:\n  default:\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n    ingressClassName: nginx\n</code></pre> With the above pre-defined ingress, any module with an endpoint-enabled service will have rules injected into the <code>Ingress</code> resource created by this configuration.</p>"},{"location":"guide/smilecdr/ingress/#the-defaultingress-setting","title":"The <code>defaultIngress</code> setting","text":"<p>This setting tells an Ingress resource to serve as the active default ingress. This setting can only be enabled for a single Ingress Configuration at a time.</p> <p>If you are defining a custom default Ingress Configuration and do not wish to use the pre-defined one, you need to disable it like so:</p> <pre><code>ingresses:\n  default:\n    enabled: false\n  myCustomDefaultIngress\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n    ...\n</code></pre> <p>NOTE: If there are no Ingress Configurations with this setting enabled, then any Smile CDR modules that HAVE NOT explicitly defined <code>service.ingresses</code> in their configuration will not be exposed externally to the K8s cluster.</p>"},{"location":"guide/smilecdr/ingress/#configuring-the-pre-defined-default-ingress-configuration","title":"Configuring the pre-defined Default Ingress Configuration","text":"<p>The pre-defined Ingress Configuration can be reconfigured as follows: <pre><code>ingresses:\n  default:\n    type: azure-agic\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#disabling-the-pre-defined-default-ingress-configuration","title":"Disabling the pre-defined Default Ingress Configuration","text":"<p>If you are creating multiple custom Ingress Configurations, you may wish to disable the pre-defined default ingress. This can be done as follows: <pre><code>ingresses:\n  default:\n    enabled: false\n</code></pre></p> <p>NOTE: If you disable the default ingress, then you must define at least one alternative Ingress Configuration if you wish to allow access from outside the Kubernetes cluster.</p>"},{"location":"guide/smilecdr/ingress/#defining-custom-ingress-configurations","title":"Defining Custom Ingress Configurations","text":"<p>To allow for complex ingress architectures, you may define an arbitrary number of Ingress Configurations like so <pre><code>ingresses:\n  default:\n    enabled: false\n  myPrivateNginx:\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n  myPublicNginx:\n    enabled: true\n    type: nginx-ingress\n    ingressClassNameOverride: nginx-public\n\n# These two are just to demonstrate using the\n#  AWS Load Balancer Controller for Ingress\n\n  myPrivateALB:\n    enabled: true\n    type: aws-lbc-alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internal\n  myPublicALB:\n    enabled: true\n    type: aws-lbc-alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n</code></pre> In the above example we are:</p> <ul> <li>Disabling the default Ingress Configuration (We do this, because we will be defining a new default Ingress Configuration)</li> <li>Creating a <code>myPrivateNginx</code> default Ingress Configuration.</li> <li>Using <code>defaultIngress</code> creates the default Ingress resource. This will be used by any Smile CDR modules that do not explicitly define <code>service.ingresses</code>.</li> <li>The Ingress resource will use the Nginx Ingress Controller via the <code>ingressClass</code> resource named <code>nginx</code></li> <li>Creating a <code>myPublicNginx</code> Ingress Configuration.</li> <li>This ingress will not be used unless a module specifies that it should use this ingress.</li> <li>The Ingress resource will use the Nginx Ingress Controller via the <code>ingressClass</code> resource named <code>nginx-public</code>. This additional Nginx Ingress Controller will need to be deployed before trying to use this configuration.</li> </ul> <p>The following two are just to demonstrate how you would use the AWS Load Balancer Controller as an alternative mechanism.</p> <ul> <li>Creating a <code>myPrivateALB</code> Ingress Configuration.</li> <li>This will use the AWS Load Balancer Controller with an IngressClassName of <code>alb</code>.</li> <li>It uses AWS Load Balancer Controller annotations to set this ingress to use an internal only AWS Application Load Balancer.</li> <li>Creating a <code>myPublicALB</code> Ingress Configuration.</li> <li>This will use the Nginx Ingress controller with an IngressClassName of <code>alb</code>.</li> <li>It uses AWS Load Balancer Controller annotations to set this ingress to use an internet facing AWS Application Load Balancer.</li> </ul> <p>NOTE: There are no restrictions on mixing the <code>type</code> for Ingress Configurations. For example, you may have some Ingress Configurations use <code>nginx-ingress</code> and others use <code>aws-lbc-alb</code>.</p>"},{"location":"guide/smilecdr/ingress/#configure-ingress-for-modules","title":"Configure Ingress for Modules","text":"<p>Smile CDR modules will automatically be configured to use whichever ingress has the <code>defaultIngress</code> setting enabled.</p> <p>If you do not want a module to use the default Ingress resource, or if there is no default Ingress Configuration defined, then you need to explicitly configure the <code>service.ingresses</code> for any Smile CDR modules that need to be accessed from outside the Kubernetes cluster.</p> <p>To configure a Smile CDR module to use a specific ingress, specify it in the <code>moduleSpec.service.ingresses</code> map as follows: <pre><code>modules:\n  fhirweb_endpoint:\n    service:\n      hostName: myFhirwebHost.example.com\n      ingresses:\n        myPublicNginx:\n          enabled: true\n</code></pre> In the above example we are telling the module named <code>fhirweb_endpoint</code> to use the <code>myPublicNginx</code> ingress specified previously mentioned. With the sample 'Multiple Ingress' configurations provided in on this page, all Smile CDR modules will only be accessible via the <code>myPrivateNginx</code> ingress, except for the <code>fhirweb_endpoint</code> module which will be available via the public access route.</p> <p>NOTE: Although you can configure a module to use multiple Ingress resources, be careful doing this unless you are using a split-dns configuration that preserves the host name for all ingresses.</p> <p>Having an endpoint be accessible via multiple hostnames can cause issues with incorrect links to resources generated by the application. It's advisable to only have a given module be accessible via a single ingress/hostname.</p>"},{"location":"guide/smilecdr/ingress/#extra-annotations","title":"Extra Annotations","text":"<p>Depending on the ingress type you select, the chart will automatically add a set of default annotations that are appropriate for the ingress type being used.</p> <p>However, it is not possible for the chart to automatically include all annotations as some need to be specified in your configuration.</p> <p>To add any extra annotations, or override existing ones, include them in your values file like so:</p> <pre><code>ingresses:\n  default:\n    annotations:\n      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm::01234567890:certificate/abcdef\n      alb.ingress.kubernetes.io/inbound-cidrs: ['0.0.0.0/0']\n</code></pre> <p>or <pre><code>ingresses:\n  default:\n    annotations:\n      appgw.ingress.kubernetes.io/appgw-ssl-certificate: mysslcert\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#ingress-class-name","title":"Ingress Class Name","text":"<p>This chart uses the following default class names for the different ingress types</p> Selected <code>ingresses.default.type</code> Default <code>ingressClassName</code> <code>nginx-ingress</code> <code>nginx</code> <code>aws-lbc-alb</code> <code>alb</code> <code>azure-agic</code> <code>azure/application-gateway</code> <p>If you have configured your Ingress Controller with a different <code>ingressClass</code> name, you can override it using <code>ingresses.default.ingressClassNameOverride</code>.</p> <p>For example, if you had a dedicated Nginx Ingress Controller with the <code>IngressClass</code> of <code>nginx-dedicated</code>, you would include it in your values file like so:</p> <pre><code>ingresses:\n  default:\n    ingressClassNameOverride: nginx-dedicated\n</code></pre>"},{"location":"guide/smilecdr/ingress/#disabling-ingress","title":"Disabling Ingress","text":"<p>In some scenarios, you may wish to disable external ingress for certain modules. For example, if you have a FHIR Rest Endpoint module that is behind a FHIR Gateway module, you may not want to expose the FHIR Rest endpoint externally to the cluster.</p> <p>In this case, you can disable the ingress for a given service like so: <pre><code>modules:\n  fhir_endpoint:\n    service:\n      ingresses:\n        default:\n          enabled: false\n</code></pre></p> <p>Configuring your module like this will prevent any rules from being added to the default Ingress resource that is generated.</p> <p>If your module is a FHIR Rest Endpoint module, the <code>base_url.fixed</code> setting will be automatically configured appropriately and there is no need for you set this in your <code>moduleSpec.config</code>.</p>"},{"location":"guide/smilecdr/ingress/#service-type","title":"Service Type","text":"<p>The appropriate type for the <code>Service</code> resources depend on which Ingress type is being used. The default <code>Service</code> created by this chart is <code>ClusterIP</code>. This is the preferred option as it does not expose the Services externally to the cluster.</p> <p>When using the AWS Load Balancer Controller, or Azure Application Gateway Controller, the service objects are instead set to <code>NodePort</code>.</p> <p>This can be overridden using <code>service.type</code> in your values file, but it is not recommended and may cause unpredictable behaviour.</p>"},{"location":"guide/smilecdr/install/","title":"Installing Smile CDR","text":""},{"location":"guide/smilecdr/install/#prepare-values-file","title":"Prepare Values File","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p> <p>Refer to the section on Values Files Management for more info on how to organise your values files. You can start out with one of the values files in the Examples section, or create your own from scratch using techniques from the configuration section.</p> <p>For the remainder of this section, we will assume the same values file that was used in the QuickStart guide.</p>"},{"location":"guide/smilecdr/install/#install-the-helm-chart","title":"Install the Helm Chart","text":"<p>With your custom values file(s) you install using the latest version of the Helm Chart as follows: <pre><code>$ helm upgrade -i my-smile-env -n my-namespace -f my-values.yaml smiledh-stable/smilecdr\n</code></pre></p> <p>To install a specific version of the Helm Chart, include the <code>--version</code> option as follows: <pre><code>$ helm upgrade -i my-smile-env -n my-namespace -f my-values.yaml smiledh-stable/smilecdr --version 2.0.0-next-major.1\n</code></pre></p> <p>Smile, we're up and running! :)</p> <p>If your cluster has spare capacity available, all pods should be in the <code>Running</code> state after about 2-3 minutes. If your cluster needs to auto-scale to provision enough resources, it may take longer while the Kubernertes worker nodes get created. <pre><code>$ kubectl get pods -n my-namespace\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL.</p> <p>You can now continue to reconfigure it using this guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env -n my-namespace\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"guide/smilecdr/logging/","title":"Smile CDR Application Logging","text":"<p>Smile CDR uses the Logback logging framework to collect application logs and can be configured based on individual requirements.</p> <p>Full documentation about the logging system is available here.</p>"},{"location":"guide/smilecdr/logging/#custom-log-configuration","title":"Custom Log Configuration","text":"<p>As per the Smile CDR docs, custom log configurations can be used by providing a custom <code>logback-smile-custom.xml</code> file.</p> <p>While you could create this file manually and include it in the Helm Chart deployment by using one of the file copy methods, editing this file and configuring the copying can be troublesome and error prone.</p>"},{"location":"guide/smilecdr/logging/#automatic-log-configuration","title":"Automatic Log Configuration","text":"<p>As an alternative to the above, you can specify common configurations for the custom logging directly in your Helm values file. This eliminates the need to to perform any extra steps. This can also be helpful for automating log configuration changes.</p> <p>All of the below mentioned techniques may be combined in a single configuration.</p>"},{"location":"guide/smilecdr/logging/#enable-troubleshooting-loggers","title":"Enable Troubleshooting Loggers","text":"<p>As per the Smile CDR documentation, you can enable Troubleshooting Logs of various types.</p> <p>The same configurations can be enabled in your Helm Values file like so:</p> <p>This snippet shows how you would enable DEBUG logs for the HTTP Troubleshooting Log <pre><code>logging:\n  troubleshootingLoggers:\n    http_troubleshooting:\n      enabled: true\n      level: debug\n</code></pre></p> <p>Any of the troubleshooting loggers listed in the official docs can be enabled in the same way.</p>"},{"location":"guide/smilecdr/logging/#set-arbitrary-loggers","title":"Set Arbitrary Loggers","text":"<p>You can enable logging at any log level for any of the classes implemented within Smile CDR.</p> <p>In this example, we demonstrate how to quiesce some noisy log messages that may be flooding the logs.</p> <pre><code>logging:\n  setLoggers:\n    thymeleaf:\n      description: Mute Thymeleaf errors that were flooding the logs with errors.\n      level: \"OFF\"\n      paths:\n      - org.thymeleaf.standard.processor.AbstractStandardFragmentInsertionTagProcessor\n      - org.thymeleaf.standard.processor.StandardIncludeTagProcessor\n</code></pre> <p>Again, any Java class path can be enabled at any log level.</p>"},{"location":"guide/smilecdr/logging/#create-custom-loggers","title":"Create Custom Loggers","text":"<p>If you have the requirement to store logs in an arbitrary log format, you can create a custom logger.</p> <p>The following defines a custom logger for a fictional Smile CDR component, 'x', and saves the logs to <code>myApp.log</code></p> <pre><code>logging:\n  customLoggers:\n    myCustomLogger:\n      path: cdr.component.x\n      enabled: true\n      level: DEBUG\n      pattern: \"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level M:%X{moduleId} R:%X{requestId} %logger{36} [%file:%line] %msg%n\"\n      target: myApp.log\n</code></pre> <p>You can also set <code>logging.customLoggers.&lt;loggerName&gt;.target: STDOUT</code> to log to <code>stdout</code> instead of writing to a log file.</p>"},{"location":"guide/smilecdr/logging/#provide-raw-custom-log-configuration","title":"Provide Raw Custom Log Configuration","text":"<p>Finally, if you need to provide custom Logback configurations instead of using the above techniques to generate it automatically, you can do so directly in your Helm values file like so:</p> <pre><code>logging:\n  rawLogConfig: |-\n    &lt;included&gt;\n      &lt;!--\n      Custom logging config:\n      This will override and replace any auto-generated logger configurations defined above.\n      --&gt;\n    &lt;/included&gt;\n</code></pre>"},{"location":"guide/smilecdr/logging/#log-collection-and-aggregation","title":"Log Collection and Aggregation","text":"<p>As with any application running in Kubernetes, you should stream logs from the Pods and use an aggregation solution to persist them in a single location.</p> <p>There are numerous solutions available to perform this task. There is no single 'correct' solution as different organisations may have differing requirements for the persisting of application logs.</p> <p>At a high level, here are some foundational concepts that should be reviewed so that a suitable logging solution may be devised.</p> <p>Kubernetes Logging Architecture</p>"},{"location":"guide/smilecdr/logging/#current-helm-chart-support","title":"Current Helm Chart Support","text":"<p>Currently this Helm Chart does not provide any functionality or guidance on how to perform these tasks.</p> <p>It is up to the architect/implementer to devise and implement a suitable solution.</p> <p>Some recommended solutions that are known to work are:</p> <ul> <li>OpenTelemetry + Loki + Grafana</li> <li>EKS Container Insights</li> <li>DataDog</li> </ul>"},{"location":"guide/smilecdr/logging/#future-helm-chart-support","title":"Future Helm Chart Support","text":"<p>There is a soon-to-be released feature of this Helm Chart that will automate the provisioning of a complete Observability Suite alongside your Smile CDR deployment.</p> <p>This feature will enable and configure all of the followinf just by enabling a few simple options inside your Helm values file.</p> <ul> <li>Full OpenTelemetry instrumentation (Metrics, Traces and Logs)</li> <li>OpenTelemetry Collector to aggregate telemetry data and send to backends</li> <li>Prometheus back-end for Metrics telemetry</li> <li>Loki back-end for Log file aggregation</li> <li>Tempo back-end for Trace telemetry</li> <li>Grafana with default dashboards connecting all of the above and providing overall view of Smile CDR cluster.</li> </ul>"},{"location":"guide/smilecdr/messagebroker/","title":"Message Broker Configuration","text":"<p>This Helm Chart support configuring Smile CDR to work with a Kafka or ActiveMQ message broker as described in the Smile CDR documentation</p> <p>While you can configure any of the message broker settings available in the <code>clustermgr</code> module, this process can be complicated and error prone. This Helm Chart simplifies this process by automatically configuring Smile CDR message broker and Kafka settings.</p> <p>You can either configure Smile CDR to use a message broker that has already been provisioned, or if available, you can make use of the Strimzi Operator to provision Kafka inside the K8s cluster.</p>"},{"location":"guide/smilecdr/messagebroker/#configuring-external-message-broker","title":"Configuring external message broker","text":"<p>Use the <code>messageBroker.external</code> section to configure an external message broker like so (in this example we use Kafka):</p> <pre><code>messageBroker:\n  external:\n    enabled: true\n    type: kafka\n    config:\n      ...\n</code></pre> <p>The remaining configuration differs for Kafka and ActiveMQ as described in the sections below.</p>"},{"location":"guide/smilecdr/messagebroker/#configuring-kafka","title":"Configuring Kafka","text":"<p>To enable a default Kafka configuration, you will need to provide connection and authentication details. For more information on the settings being used inside the Smile CDR configuration, please refer to the documentation here</p>"},{"location":"guide/smilecdr/messagebroker/#tls-connectivity","title":"TLS Connectivity","text":"<p>When connecting to Kafka clusters, it is advised to use TLS connections. In a default configuration, Smile CDR will be configured to use TLS.</p> <p>If you wish to run without enabling encryption (i.e. for testing purposes), you can also specify <code>plaintext</code> like so.</p> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: plaintext\n</code></pre> <p>Note: You cannot use mTLS or IAM authentication methods if you disable TLS</p>"},{"location":"guide/smilecdr/messagebroker/#using-custom-certificate-authority","title":"Using Custom Certificate Authority","text":"<p>If your external Kafka cluster is configured with a TLS certificate that is signed with a public Certificate Authority (CA) then no further steps are required as the truststore that is included in the Java distribution will be used.</p> <p>However, if you need to provide a custom CA certificate or if you do not wish to use the default trust store included with the JVM, you can do so by providing a <code>caCert</code> configuration in the connection settings.</p> <p>This certificate can be provided using either the <code>k8sSecret</code> or <code>sscsi</code> secret mechanisms. See the secrets section for more info.</p> <p>Using <code>k8sSecret</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: k8sSecret\n          secretName: my-kafka-ca-cert\n</code></pre></p> <p>Using <code>sscsi</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkacacert\n</code></pre> CA Cert Secret Format</p> <p>The certificate passed in to the chart must have 2 values, with the appropriate keys.</p> Key name Key value ca.p12 Trust store containing the CA certificate. Must be provided in the PKCS12 (<code>.p12</code>) format ca.password Password for verifying the contents of the trust store."},{"location":"guide/smilecdr/messagebroker/#authentication","title":"Authentication","text":"<p>This chart currently supports either Mutual TLS (mTLS) or IAM authentication, depending on the type of Kafka cluster that is being used.</p> Cluster Type mTLS IAM External (Generic) Amazon MSK  (With Private CA) Amazon MSK Serverless In-cluster (Strimzi)  (Default) <p>Note: Additional authentication mechanisms may be added at a later date.</p>"},{"location":"guide/smilecdr/messagebroker/#mtls-authentication","title":"mTLS Authentication","text":"<p>To configure mTLS authentication you need to do the following</p> <ul> <li> <p>Configure Kafka cluster for mTLS.</p> <p>NOTE: If using mTLS with Amazon MSK (Not Serverless) then you will also need to create an AWS Private Certificate Authority, to create any client certificates. This will incur additional expense. Configuration of MSK and user certificates is out of scope of this documentation, please refer to the AWS user guide.</p> </li> <li> <p>Have access to the certificate for the configured user.</p> </li> <li>Configure the connection type to use TLS (See above).</li> <li>Configure the authentication type to use TLS.</li> <li>Configure the userCert secret.</li> </ul> <p>Now you can provide the client certificate using <code>k8sSecret</code> or <code>sscsi</code> as follows:</p> <p>Using <code>k8sSecret</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: k8sSecret\n          secretName: my-kafka-ca-cert\n      authentication:\n        type: tls\n        userCert:\n          type: k8sSecret\n          secretName: my-kafka-client-cert\n</code></pre></p> <p>Using <code>sscsi</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkacacert\n      authentication:\n        type: tls\n        userCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkaclientcert\n</code></pre></p> <p>User Cert Secret Format</p> <p>The user certificate passed in to the chart must have 2 values, with the appropriate keys.</p> Key name Key value user.p12 Trust store containing the user certificate and private key. Must be provided in the PKCS12 (<code>.p12</code>) format user.password Password for decrypting the private key."},{"location":"guide/smilecdr/messagebroker/#iam-authentication-amazon-msk-only","title":"IAM Authentication (Amazon MSK only)","text":"<p>If you are using Amazon MSK as your message broker, IAM is the preferred method of authentication.</p> <p>NOTE: Amazon MSK Serverless ONLY supports IAM authentication.</p>"},{"location":"guide/smilecdr/messagebroker/#iam-prerequisites","title":"IAM Prerequisites","text":"<p>Before configuring Smile CDR to use this authentication method, you need to ensure that the following pre-requisites are in place:</p> <ul> <li>Configure IRSA for the Smile CDR application. See the Service Accounts section for more info on this.</li> <li>Ensure that your Smile CDR IAM role has a suitable MSK authorization policy attached. See the AWS Documentation for more information on how to create a suitable IAM authorization policy for MSK.</li> </ul>"},{"location":"guide/smilecdr/messagebroker/#client-configuration","title":"Client Configuration","text":"<p>The AWS documentation details the steps to configure clients to use IAM, which can be fairly complicated.</p> <p>To simplify this, the required configuration is automatically applied when enabling IAM authentication in this Helm Chart and there is nothing further to do on the client side.</p> <p>To enable IAM authentication: <pre><code>messageBroker:\n  external:\n    enabled: true\n    config:\n      connection:\n        type: tls\n        bootstrapAddress: my-msk-bootstrap-address1.amazon.com:9098\n      authentication:\n        type: iam\n</code></pre></p> <p>Note: You do not need to provide a trust certificate if using the Amazon MSK generated endpoints, as they use publicly signed TLS certificates which are trusted by default.</p> <p>If you plan to manually copy the required IAM authentication Jar file into a custom image, then you can disable the automatic file copying like so: <pre><code>messageBroker:\n  external:\n    enabled: true\n    config:\n      connection:\n        type: tls\n        bootstrapAddress: my-msk-bootstrap-address1.amazon.com:9098\n      authentication:\n        type: iam\n        iamConfig:\n          autoJarCopy: false\n          # adminAutoJarCopy: false # Optional: See note below\n</code></pre></p> <p>Note: If you are using the Kafka Admin pod and you wish to also disable the automated jar copying for this, then you will need to provide a custom Kafka image.</p>"},{"location":"guide/smilecdr/messagebroker/#consumer-producer-properties","title":"Consumer &amp; Producer properties","text":"<p>Custom consumer properties and producer properties can be configured using the <code>messageBroker.clientConfiguration</code> section as follows:</p> <pre><code>messageBroker:\n  clientConfiguration:\n    consumerProperties:\n      max.poll.records: 20\n    producerProperties: {}\n</code></pre> <p>Note: By default, the consumer properties are configured with the Kafka documented defaults prior to Kafka 3.0</p>"},{"location":"guide/smilecdr/messagebroker/#topic-auto-creation-validation","title":"Topic Auto Creation &amp; Validation","text":"<p>This chart simplifies Kafka topic management by autoconfiguring topic related settings based on your message broker configuration.</p> <p>Topic Auto Creation By default, the <code>auto.create.topics.enable</code> option on Kafka clusters is often set to <code>true</code>. With this setting enabled, Kafka will automatically create topics when consumers or producers try to use a topic.</p> <p>As a general Kafka best practice in production environments, topic auto creation should be disabled by setting this option to <code>false</code>. When configured this way, any required topics should be created through some other process.</p> <p>Topic Validation If topic auto creation is disabled in your Kafka cluster, the following option should also be set in Smile CDR:</p> <p><code>kafka.validate_topics_exist_before_use</code></p> <p>This option prevents Smile CDR from trying to send messages to topics that do not exist, which will flood the logs with errors.</p> <p>As a convenience, the <code>autoCreateTopics</code> and <code>validateTopics</code> options are auto-configured based on the Kafka configurations provided in your values file as per the below table.</p> Message Broker Config Topic Auto Creation Validate Topics <code>external.type: kafka</code> <code>external.type: msk</code> <code>external.type: msk-serverless</code> <code>strimzi.enabled</code> and <code>manageTopics: false</code> <code>strimzi.enabled</code> and <code>manageTopics: true</code> <p>In certain circumstances, you may wish to override the auto-configured defaults above behaviour by setting <code>messageBroker.autoCreateTopics</code> and/or <code>messageBroker.validateTopics</code>.</p> <p>Note: You cannot override topic auto creation for Amazon MSK Serverless as it does not support topic auto creation.</p> <p>See here for more information on these Kafka settings in Smile CDR.</p>"},{"location":"guide/smilecdr/messagebroker/#amazon-msk","title":"Amazon MSK","text":"<p>When creating an Amazon MSK cluster, topic auto creation is disabled by default, which is the recommended configuration for production environments.</p> <p>If you wish to enable topi cauto creation for development or testing environments, you will need to do the following:</p> <ul> <li>Create an MSK Custom Configuration that sets <code>auto.create.topics.enable</code> to <code>true</code></li> <li>Apply this configuration to your MSK cluster.</li> <li>Update your values file to override topic validation like so:     <pre><code>messageBroker:\n  validateTopics: false\n</code></pre></li> </ul> <p>More information on MSK Custom Configurations is available on the AWS Documentation</p>"},{"location":"guide/smilecdr/messagebroker/#amazon-msk-serverless","title":"Amazon MSK Serverless","text":"<p>Unlike provisioned Amazon MSK, the serverless variant does not allow for topic auto creation. When using Amazon MSK Serverless, you must create topics using another mechanism.</p>"},{"location":"guide/smilecdr/messagebroker/#strimzi","title":"Strimzi","text":"<p>If using the Strimzi Operator and configuring this Helm Chart to deploy the Kafka cluster, it is possible to also define the required topics in the values file. When doing this, the <code>auto.create.topics.enable</code> will be automatically set to <code>false</code> to prevent collisions between topics created by the broker and topics created by the Strimzi Operator. The <code>kafka.validate_topics_exist_before_use</code> Smile CDR configuration will also be set to <code>true</code>.</p>"},{"location":"guide/smilecdr/messagebroker/#topic-pre-provisioning","title":"Topic Pre-Provisioning","text":"<p>If you are using a Kafka cluster with <code>auto.create.topics.enable</code> to be set to <code>false</code> as mentioned above, then you will need to create the topics using some other process. If you do not already have a process in place for creating Kafka topics, then there are a couple of options.</p> <ul> <li>Follow the official Kafka documentation for creating topics.This usually involves using a workstation or server with the Kafka binaries and configuration to talk to your Kafka cluster. This can be a tricky process, depending on network security requirements and connectivity/authentication configuration.</li> <li>Use a declarative approach to Kafka topic management. Your topics will be defined in code and can be applied to the Kafka cluster in a highly repeatable fashion.</li> </ul> <p>If using either of the above techniques, the default configuration of Smile CDR deployed by this Helm Chart will require the following two topics to be created:</p> <ul> <li><code>batch2.work.notification.Masterdev.persistence</code></li> <li><code>subscription.matching.Masterdev.persistence</code></li> </ul> <p>If you change the <code>nodeId</code> (in <code>cdrNodes</code>) or alter the default module configuration, the above names may change and would need to be determined for your configuration.</p> <p>As a convenience, this Helm Chart provides methods to help with this.</p>"},{"location":"guide/smilecdr/messagebroker/#strimzi_1","title":"Strimzi","text":"<p>If using the Strimzi Operator, initial <code>batch2</code> and <code>subscription</code> topics will automatically be created from the <code>KafkaTopic</code> CRDs that get created. You can add or override topics in a declarative fashion using the <code>messageBroker.topics</code> section.</p> <p>Note: You can disable topic management by the Helm Chart &amp; Strimzi by setting <code>messageBroker.manageTopics</code> to <code>false</code>.</p> <pre><code>messageBroker:\n  manageTopics: true\n  topics:\n    batch2:\n      name: \"batch2.work.notification.Masterdev.persistence\"\n      partitions: 10\n    subscription:\n      name: \"subscription.matching.Masterdev.persistence\"\n      partitions: 10\n</code></pre> <p>When using this method, the Helm Chart will create a <code>KafkaTopic</code> resource for each of the provided topics. Topic creation, configuration and deletion will then be managed by the Strimzi Topic Operator. The Kafka brokers will be configured with <code>auto.create.topics.enable</code> set to <code>false</code> as per best practice for production environments.</p> <p>Using this method allows you to define the configuration of your Kafka topics in code for increased repeatability and reliability.</p>"},{"location":"guide/smilecdr/messagebroker/#admin-pod","title":"Admin Pod","text":"<p>This experimental feature will let you create a <code>Kafka Admin</code> pod in the same namespace as your Smile CDR instance. It can be enabled as follows:</p> <pre><code>messageBroker:\n  adminPod:\n    enabled: true\n</code></pre> <p>When enabled, there will be a new deployment created that creates a single ephemeral pod. This pod is automatically configured to use the same connectivity (Including required certificates) as the main Smile CDR pods.</p> <p>You can use this pod to inspect the Kafka cluster, or perform tasks such as creating/deleting topics. This is a convenience feature that should only be used during the development phase and not be used in production environments.</p> <ul> <li> <p>Connect to the Kafka Admin pod <pre><code>kubectl exec -ti &lt;admin-pod-name&gt; -- sh\n</code></pre></p> </li> <li> <p>Check available topics <pre><code>./bin/kafka-topics.sh --list\n</code></pre></p> </li> <li> <p>Check consumer groups <pre><code>./bin/kafka-consumer-groups.sh --describe --group smilecdr\n</code></pre></p> </li> </ul> <p>Note: You do not need to provide a config file or bootstrap address on the command-line as it is auto configured.</p>"},{"location":"guide/smilecdr/messagebroker/#provisioning-kafka-with-strimzi","title":"Provisioning Kafka with Strimzi","text":"<p>If you have the Strimzi Operator installed in your cluster, you can use the following values file section to automate provisioning of a Kafka cluster. Your Smile CDR instance will then be automatically configured to use this Kafka cluster. <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n</code></pre></p>"},{"location":"guide/smilecdr/messagebroker/#configuring-kafka-via-strimzi","title":"Configuring Kafka via Strimzi","text":"<p>With the configuration provided above, you will have a production-like Kafka cluster with the followinf configuration:</p> <ul> <li>3 ZooKeeper nodes with the following specifications<ul> <li>0.5cpu</li> <li>512MiB memory</li> <li>10GiB storage</li> </ul> </li> <li>3 Kafka Broker nodes with the following specifications<ul> <li>0.5cpu</li> <li>1GiB memory</li> <li>10GiB storage</li> </ul> </li> <li>TLS connection enabled by default</li> <li>mTLS authentication enabled by default</li> </ul> <p>All of the Kafka configurations can be configured using <code>messageBroker.strimzi.config</code> like so:</p> <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n    kafka:\n      connection:\n        type: tls\n      authentication:\n        type: tls\n      version: \"3.3.1\"\n      protocolVersion: \"3.3\"\n\n      replicas: 4\n      volumeSize: 20Gi\n      resources:\n        requests:\n          cpu: 0.5\n          memory: 4Gi\n        limits:\n          memory: 4Gi\n    zookeeper:\n      replicas: 2\n      volumeSize: 10Gi\n      resources:\n        requests:\n          cpu: 0.5\n          memory: 512Mi\n        limits:\n          memory: 512Mi\n</code></pre>"},{"location":"guide/smilecdr/messagebroker/#deprecated-strimzi-schema","title":"Deprecated Strimzi Schema","text":"<p>If you are updating from version v1.0.0-pre.106 of the Helm Chart or earlier, you will need to alter your Strimzi spec from the old schema below, which has been deprecated.</p> <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n    config:\n      connection:\n        type: tls\n      authentication:\n        type: tls\n      version: \"3.3.1\"\n      protocolVersion: \"3.3\"\n      kafka:\n        replicas: 4\n      zookeeper:\n        replicas: 2\n</code></pre> <ul> <li>Anything under <code>messageBroker.strimzi.config.kafka</code> should be moved to <code>messageBroker.strimzi.kafka</code></li> <li>Anything under <code>messageBroker.strimzi.config.zookeeper</code> should be moved to <code>messageBroker.strimzi.zookeeper</code></li> <li>Anything remaining under <code>messageBroker.strimzi.config</code> should be moved to <code>messageBroker.strimzi</code></li> </ul> <p>You will receive a deprecation warning so that you have time to update your configurations before support for the old schema is removed.</p> <p>For more details on how to configure Kafka using Strimzi, please consult the Strimzi Operator documentation here</p>"},{"location":"guide/smilecdr/requirements/","title":"Requirements and Dependencies","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Helm Chart. Due to the complicated nature of configuring the product, and enforcing strong security practices, there is no quickstart option without ensuring some, or all, of these pre-requisites have been met.</p>"},{"location":"guide/smilecdr/requirements/#minimum-requirements","title":"Minimum Requirements","text":"<p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes addons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>One of the following supported database options:<ul> <li>Externally provisioned database in the official Smile CDR supported databases list here</li> <li>CrunchyData Postgres Operator installed in cluster. See Extra Requirements below if you follow this option.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#recommended-requirements","title":"Recommended Requirements","text":"<p>These dependencies are recommended in order to follow security best practices. These are in addition to those listed above.</p> <ul> <li>Kubernetes/EKS/AKS cluster should be configured with best practices in mind.<ul> <li>Kubernetes best practices</li> <li>Amazon EKS best practices</li> <li>Azure AKS best practices</li> </ul> </li> <li>Kubernetes cluster should, at the very least, have the following configurations<ul> <li>Secret Encryption (EKS Secret Encryption)</li> <li>Storage Class with encryption enabled if using persistent storage (PostgreSQL or Kafka)</li> <li>Enforce all pods should set resource requests</li> </ul> </li> <li>AWS IAM Role for the Smile CDR application.<ul> <li>Should follow the principle of least privilege and only have access to required AWS services</li> </ul> </li> <li>Secrets Store CSI Driver + Provider<ul> <li>Only the AWS Provider is supported at this time</li> <li>AWS IAM Role needs access to read &amp; decrypt the secrets in AWS Secrets Manager</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#extra-requirements","title":"Extra Requirements","text":"<ul> <li>Strimzi Kafka Operator<ul> <li>Allows you to install a production ready Kafka cluster as a part of the Smile CDR deployment.</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/tls-encryption/","title":"Configuring TLS Encryption","text":"<p>This Helm Chart supports multiple mechanisms for enabling encryption of data in transit. This feature is currently under active development in order to simplify the configuration of end-to-end encryption.</p> <p>End-to-end encryption is currently an opt-in feature that needs to be explicitly enabled.</p>"},{"location":"guide/smilecdr/tls-encryption/#default-configuration","title":"Default Configuration","text":"<p>Currently, even though the Helm Chart defaults enable TLS encryption for the end-user, it is not enabled by default directly on the Smile CDR HTTP endpoints used internally within the cluster.</p> <p>This results in the following encryption configurations between the client and Smile CDR, depending on your choice of ingress.</p>"},{"location":"guide/smilecdr/tls-encryption/#using-ingress-nginx-for-ingress-default","title":"Using ingress-nginx for ingress (Default)","text":"<p>If you have left your ingress definition to use the default of <code>nginx-ingress</code> (See Here), encryption will be configured as follows:</p> <ul> <li>TLS encryption between client and the AWS Network Load Balancer using ACM certificate provided when configuring the ingress-nginx controller.</li> <li>TLS encryption from the NLB to the Nginx ingress controller using self-signed certificate automatically generated by the ingress controller.</li> <li>Unencrypted from the Nginx ingress controller to the Smile CDR pods.</li> </ul>"},{"location":"guide/smilecdr/tls-encryption/#using-aws-application-load-balancer-for-ingress","title":"Using AWS Application Load Balancer for ingress","text":"<p>If you have set your ingress definition to use <code>aws-lbc-alb</code> (See Here), encryption will be configured as follows:</p> <ul> <li>TLS encryption between client and the AWS Application Load Balancer using ACM certificate specified in your <code>Ingress</code> resource.</li> <li>Unencrypted from the ALB to the Smile CDR pods.</li> </ul>"},{"location":"guide/smilecdr/tls-encryption/#using-azure-application-gateway-ingress-controller-for-ingress","title":"Using Azure Application Gateway Ingress Controller for ingress","text":"<p>If you have set your ingress definition to use <code>azure-agic</code> (See Here), encryption is not currently supported.</p>"},{"location":"guide/smilecdr/tls-encryption/#enabling-tls","title":"Enabling TLS","text":"<p>This helm chart leverages the cert-manager X.509 certificate controller to provision and manage any required certificates. As such, this controller is a requirement to enable TLS beyond the default configurations mentioned above.</p> <p>TLS certificate generation is enabled by configuring <code>Issuer</code> and <code>Certificate</code> resources in the cluster. For convenience, the Helm Chart will create default issuers and certificates and auto-configure Smile CDR to use them. It is also possible to disable these defaults to define your own.</p>"},{"location":"guide/smilecdr/tls-encryption/#enable-default-cluster-signed-certificates","title":"Enable Default 'cluster-signed' certificates","text":"<p>To enable default end-to-end encryption, add the following to your <code>values.yaml</code> file.</p> <pre><code>tls:\n  certificateIssuers:\n    default:\n      enabled: true\n  certificates:\n    default:\n      enabled: true\n  defaultEndpointConfig:\n    enabled: true\n</code></pre> Explanation <p>Simply adding the above code will result in the following encryption configurations between the client and Smile CDR, depending on your choice of ingress.</p>"},{"location":"guide/smilecdr/tls-encryption/#certificateissuers","title":"<code>certificateIssuers</code>","text":"<p>The <code>certificateIssuers</code> configuration is used to define cert-manager <code>Issuer</code> resources. The provided default issuer configuration will create the following resources in the same namespace as the Smile CDR deployment:</p> <ul> <li>A self-signed Issuer that is used for signing the cluster-local Issuer that will act as the CA for this namespace.</li> <li>A cluster-local Issuer that is used for signing certificates to be used by Smile CDR<ul> <li>This <code>Issuer</code> resource is designated as the default issuer for any <code>Certificate</code> resources that do not explicitly specify an <code>Issuer</code>.</li> </ul> </li> </ul> <p>This default issuer can be used, or it can be disabled if you wish to define your own issuers.</p>"},{"location":"guide/smilecdr/tls-encryption/#certificates","title":"<code>certificates</code>","text":"<p>The <code>certificates</code> configuration is used to define cert-manager <code>Certificate</code> resources. The provided default certificate configuration will create the following resources in the same namespace as the Smile CDR deployment:</p> <ul> <li>A certificate that is used as the default for any resources auto-configured to use TLS encryption</li> <li>A certificate that is used for enabling TLS encryption on Smile CDR HTTP Endpoint modules<ul> <li>This certificate is signed by whichever <code>Issuer</code> resource is designated as the default.</li> <li>This <code>Certificate</code> is designated as the default for any resources auto-configured to use TLS encryption</li> </ul> </li> </ul> <p>This default certificate can be used, or it can be disabled if you wish to define your own certificates.</p>"},{"location":"guide/smilecdr/tls-encryption/#defaultendpointconfig","title":"<code>defaultEndpointConfig</code>","text":"<p>The <code>defaultEndpointConfig</code> configuration is used to set the default auto-configuration of the Smile CDR HTTP Endpoint modules. By enabling this option, all HTTP endpoint modules will use whichever <code>Certificate</code> is designated as the default.</p> <p>Individual modules may also be configured to use their own certificate configurations, this will override the <code>defaultEndpointConfig</code></p>"},{"location":"guide/smilecdr/tls-encryption/#using-ingress-nginx-for-ingress-default_1","title":"Using ingress-nginx for ingress (Default)","text":"<p>If you have left your ingress definition to use the default of <code>nginx-ingress</code> (See Here), default end-2-end encryption will be configured as follows:</p> <ul> <li>TLS encryption between client and the AWS Network Load Balancer using ACM certificate provided when configuring the ingress-nginx controller.</li> <li>TLS encryption from the NLB to the Nginx ingress controller using self-signed certificate automatically generated by the ingress controller.<ul> <li>See below note on Nginx TLS configuration</li> </ul> </li> <li>TLSv1.3 encryption from the Nginx ingress controller to the Smile CDR pods using <code>cluster-signed</code> certificate.<ul> <li>Uses <code>nginx.ingress.kubernetes.io/configuration-snippet</code> annotation.</li> <li>If this annotation is disabled, then the Nginx ingress controller should be configured to use TLSv1.3</li> </ul> </li> <li>Kubernetes Readiness Probes updated to use HTTPS</li> <li>Smile CDR module endpoints configured as follows:<ul> <li>TLS Protocol restricted to <code>TLSv1.3</code></li> <li>TLS restricted to the following recommended ciphers, which are known to work with the ELB Security Policy and the Kubernetes Kubelet:<ul> <li><code>TLS_AES_128_GCM_SHA256</code></li> <li><code>TLS_AES_256_GCM_SHA384</code></li> <li><code>TLS_CHACHA20_POLY1305_SHA256</code></li> </ul> </li> </ul> </li> </ul> <p>Note: If you wish to wish to avoid the use of the default self-signed certificate that is automatically provisioned by the Nginx Ingress controller, you can configure it to use one of the cert-manager <code>Issuer</code> resources to automatically provision a more appropriate certificate. Please see the TLS Encryption section of the Ingress documentation.</p>"},{"location":"guide/smilecdr/tls-encryption/#using-aws-application-load-balancer-for-ingress_1","title":"Using AWS Application Load Balancer for ingress","text":"<p>If you have set your ingress definition to use <code>aws-lbc-alb</code> (See Here), default end-to-end encryption will be configured as follows:</p> <ul> <li>TLS encryption between client and the AWS Application Load Balancer using ACM certificate specified in your <code>Ingress</code> resource.</li> <li>ELB Security policy is set to a secure default that supports TLSv1.3 (<code>ELBSecurityPolicy-TLS13-1-2-FIPS-2023-04</code>)</li> <li>TLS encryption from the ALB to the Smile CDR pods using <code>cluster-signed</code> certificate.</li> <li>AWS ALB Health Probes configured to use HTTPS</li> <li>Smile CDR module endpoints configured as follows:</li> <li>TLS Protocol restricted to <code>TLSv1.3</code></li> <li>TLS restricted to the following recommended ciphers, which are known to work with the ELB Security Policy and the Kubernetes Kubelet:<ul> <li><code>TLS_AES_128_GCM_SHA256</code></li> <li><code>TLS_AES_256_GCM_SHA384</code></li> <li><code>TLS_CHACHA20_POLY1305_SHA256</code></li> </ul> </li> <li><code>tls_debug_disable_sni_check</code> set to true.</li> </ul> <p>Note: In order for the HTTPS healthchecks to work when using the AWS ALB, the disable sni check option must be set. Currently, this will result in a warning in the Smile CDR console suggesting to disable the option. This is unavoidble at this time.</p> <p>These defaults may be updated via your Helm Values file. See the Advanced Configuration below.</p>"},{"location":"guide/smilecdr/tls-encryption/#using-azure-application-gateway-ingress-controller-for-ingress_1","title":"Using Azure Application Gateway Ingress Controller for ingress","text":"<p>If you have set your ingress definition to use <code>azure-agic</code> (See Here), encryption is not currently supported.</p>"},{"location":"guide/smilecdr/tls-encryption/#advanced-configuration","title":"Advanced Configuration","text":"<p>If the above defaults are not suitable, much of the functionality can be customized.</p>"},{"location":"guide/smilecdr/tls-encryption/#currently-configurable-options","title":"Currently configurable options","text":"<p>The following can be updated.</p> <p>Note: If you need assistance while the advanced configuration documentation is still being created, please contact us.</p> <ul> <li>Create custom cluster-local issuers</li> <li>Create custom certificates</li> <li>Manually configure individual HTTP Endpoint modules to use different certificates</li> <li>Configure using mixed-schema endpoints (i.e. some endpoints use HTTPS and some use HTTP). Note that multiple Ingress resources are required for this scenario</li> </ul>"},{"location":"guide/smilecdr/tls-encryption/#create-lets-encrypt-issuer-resources","title":"Create Let's Encrypt <code>Issuer</code> resources","text":"<p>This Helm Chart can be configured to easily and automatically provision a lets-encrypt, or other ACME compliant issuer.</p> <p>In order to enable this feature, your <code>certificateIssuer</code> configuration must include at the minimum the following:</p> <ul> <li><code>signingMethod: public-signed</code></li> <li><code>acmeSpec.email: your@email.com</code></li> </ul> <p>Note: Although cert issuance functions with an invalid e-mail address, it is used by the provider to track your certificates and provide notifications about the status of your certificates. It's advisable to use a correct e-mail address.</p> <p>Here is how that would look in your values file. <pre><code>tls:\n  certificateIssuers:\n    acmeStaging:\n      enabled: true\n      signingMethod: public-signed\n      acmeSpec:\n        email: your@email.com\n</code></pre></p> <p>By default, the above will use the Let's Encrypt Staging servers and produce an <code>Issuer</code> resource with the following spec:</p> <pre><code>spec:\n  acme:\n    email: your@email.com\n    privateKeySecretName:\n      name: release-name-smilecdr-acmestaging-key\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    solvers:\n    - http01:\n        ingress:\n          ingressClassName: nginx\n</code></pre> <p>For convenience, you may set <code>acmeSpec.server</code> to some pre-defined friendly names. You can also set it directly to an ACME server of your choosing.</p> <p>The following convenience server names are provided:</p> <ul> <li><code>lets-encrypt-staging</code> - <code>https://acme-staging-v02.api.letsencrypt.org/directory</code> (Default)</li> <li><code>lets-encrypt-prod</code> - <code>https://acme-v02.api.letsencrypt.org/directory</code></li> </ul>"},{"location":"guide/smilecdr/tls-encryption/#using-acme-issuer-with-aws-alb","title":"Using ACME Issuer with AWS ALB","text":"<p>At this time, the automatically generated ACME Issuer uses HTTP01 Challenges and will only work if your environment is configured to use the <code>ingress-nginx</code> Ingress Controller. If you wish to use ACME certificates with AWS ALB, there are some challenges as the cert-manager <code>Challenge</code> resource creates an Ingress resource with <code>IngressClass: alb</code>.</p> <p>As this challenge mechanism relies on creating an ingress route that is resolvable from the environment's URL, it needs to use the same AWS ALB instance. In order to achieve this, the following steps need to be taken:</p> <ul> <li>Re-deploy your ALB ingress using the <code>alb.ingress.kubernetes.io/group.name: tlstest</code> annotation.</li> <li>Note that if you are adding this value, your existing ALB may be destroyed replaced by thwe AWS Load Balancer Controller. Any DNS entries pointing to the old one may need to be updated.</li> <li>Define and manually create an <code>Issuer</code> resource that uses an <code>ingressTemplate</code> that includes the same group name annotation.</li> <li>Configure an Issuer (in your values file) that references the above Issuer resource that you manually created.</li> <li>See the pre-existing Issuers section below for more info on using existing Issuers or ClusterIssuers.</li> </ul> <p>Now, when the <code>Certificate</code> resource is created, an <code>HTTP01</code> challenge route will be created on the existing AWS ALB and the certificate will be provisioned.</p> <p>The following snippets demonstrate how to achieve this:</p> <p>In your <code>values.yaml</code> file <pre><code>ingresses:\n  default:\n    type: aws-lbc-alb\n    annotations:\n      alb.ingress.kubernetes.io/group.name: tlstest\n\ntls:\n  certificateIssuers:\n    default:\n      enabled: true\n      signingMethod: public-signed\n      existingIssuer: my-existing-le-issuer-with-alb\n  certificates:\n    default:\n      enabled: true\n  defaultEndpointConfig:\n    enabled: true\n</code></pre></p> <p>Manually created <code>Issuer</code> resource: <pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: my-existing-le-issuer-with-alb\nspec:\n  acme:\n    email: test2@exampledomain.com\n    privateKeySecretName:\n      name: smilecdr-my-existing-le-issuer-key\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n\n    solvers:\n    - http01:\n        ingress:\n          ingressClassName: alb\n          ingressTemplate:\n            metadata:\n              annotations:\n                alb.ingress.kubernetes.io/group.name: tlstest\n                alb.ingress.kubernetes.io/listen-ports: \"[{\\\"HTTP\\\":80},{\\\"HTTPS\\\":443}]\"\n                alb.ingress.kubernetes.io/scheme: internet-facing\n                alb.ingress.kubernetes.io/ssl-redirect: \"443\"\n</code></pre></p> <p>Note that it is important for all of the above annotations to be present so that the <code>HTTP01</code> challenge gets successfully set up. The ingress resource that is created is transient and will only be present for the duration of the certificate ordering process.</p> <p>For more information on any other settings that may be used in the <code>acmeSpec</code>, please refer to the official cert-manager Issuer documentation here.</p>"},{"location":"guide/smilecdr/tls-encryption/#use-pre-existing-cert-manager-issuer-or-clusterissuer-resources","title":"Use pre-existing cert-manager <code>Issuer</code> or <code>ClusterIssuer</code> resources","text":"<p>If you have already provisioned one or more cert-manager issuers through other mechanisms, you can use them instead of having the Helm Chart create them for you.</p> <p>To do this, you will still need to create an issuer configuration under <code>certificateIssuers</code> so that the Helm Chart knows what kind of issuer it is working with.</p> <pre><code>tls:\n  certificateIssuers:\n    myExistingIssuer:\n      enabled: true\n      existingIssuer: theIssuerResourceName\n      # existingClusterIssuer: theClusterIssuerResourceName\n      # signingMethod: public-signed\n</code></pre> <p>By default, this issuer will be treated as a 'cluster-signed' issuer. This means that any certificate requests may include Kubernetes service entries in the SAN list of the cert.</p> <p>If you wish to use an existing public-signed issuer (such as an ACME provider like Let's Encrypt) then you need to set <code>signingMethod: public-signed</code>. This will prevent the creation of certificates that contain host entries that cannot be included when using these issuers.</p> <p>If your existing issuer is a namespace-local issuer, use <code>existingIssuer</code>. If it is a Cluster-wide issuer, use <code>existingClusterIssuer</code>. Using the correct type here ensures that certificate requests use the correct one.</p> <p>When definging certificates to use your existing issuer, they should reference the configuration key under <code>certificateIssuers</code> rather than the Issuer resource name. To use the above existing certificate issuer, your certificate definition may look like this:</p> <pre><code>tls:\n  certificates:\n    certWithExistingIssuer:\n      enabled: true\n      issuerConfigName: myExistingIssuer\n</code></pre>"},{"location":"guide/smilecdr/tls-encryption/#adding-global-endpoint-module-configurations","title":"Adding global endpoint module configurations","text":"<p>If you wish to set extra configurations for TLS endpoints, you may do so using the <code>defaultEndpointConfig</code> configuration like so:</p> <pre><code>tls:\n  defaultEndpointConfig:\n    extraCdrConfig:\n      tls.protocol.cipher_blacklist: \"cipher1,cipher2\"\n</code></pre> <p>In the above example, maybe you wish to set specific TLS cipher restrictions.</p> <p>By adding the configuration here, you do not need to manually add it to every endpoint module. This allows for simple configuration of TLS endpoints with reduced complexity and scope for errors.</p> <p>Note: Exercise caution when manipulating the allowed ciphers, as incorrect choices may prevent the readiness probe or ALB health checks from functioning correctly.</p> <p>More information on available cipher/protocol restriction settings is available here</p>"},{"location":"guide/smilecdr/tls-encryption/#currently-unsupported-options","title":"Currently unsupported options","text":"<ul> <li>Use pre-existing cert-manager <code>Certificate</code> resources</li> <li>Configure <code>ingress-nginx</code> to use <code>Ingress.tls</code> to configure its TLS termination, instead of the default <code>self-signed</code> cert</li> <li>Securely import externally provisioned certificate material</li> <li>TLS encryption on Azure</li> </ul>"},{"location":"guide/smilecdr/updating/","title":"Deploying changes","text":"<p>When you make changes to the Helm Chart configuration, you need to apply them using the <code>helm upgrade</code> command. This chart has been designed in such a way that there should not be any outages during updates.</p> <p>There are multiple components that a configuration can affect. Broadly, it can affect the configuration of the Smile CDR app itself, or it can affect the surrounding infrastructure.</p> <p>In the event that a configuration change affects the Smile CDR application, then this chart will update any configuration files and create new application pods with zero-outage.</p>"},{"location":"guide/smilecdr/updating/#rolling-deployments","title":"Rolling Deployments","text":"<p>We achieve zero-outage by making use of Rolling Deployments in Kubernetes. The rolling deployment has been configured to create one new Pod with the new configuration at a time. Once each new Pod has successfully started up and is able to accept traffic, Kubernetes will start routing requests to it and then terminate one of the pods with the older configuration.</p> <p>The result of this is that the changes will be rolled out over the entire cluster in a controlled fashion over a few minutes, without any downtime or outage.</p> <p>This is a conservative rolling deployment model, but it means that if pods with the new configuration fail to come up without error, then the existing deployment will remain unaffected.</p>"},{"location":"guide/smilecdr/updating/#making-a-config-change-with-rolling-deployments","title":"Making a config change with Rolling Deployments","text":"<p>There is nothing you need to do to make use of this rolling deployment mechanism. If your chart configuration changes include something that will update the Smile CDR configuration, and if you have a sufficient number of replicas, then this will happen automatically.</p> <p>All changes other than those listed here will cause a rolling deployment of the application</p> <ul> <li><code>replicaCount</code> or <code>autoScaling</code> changes</li> <li><code>ingress</code> configuration - i.e. switching to a different ingress provider.</li> <li>CrunchyPGO database infrastructure configuration<ul> <li>Updating <code>users</code> config WILL cause a rolling deployment</li> </ul> </li> <li>Strimzi Kafka infrastructure resource configuration<ul> <li>Updating protocol/connection config will cause a rolling deployment</li> </ul> </li> </ul> <p>The method used to apply your updates will depend on how you have deployed the Helm Chart. If you have used a code reconciliation system or some other automation, you should not need to do anything.</p> <p>If using native Helm commands, you would use the same command you used to install the chart, like so:</p> <p><code>helm upgrade -i my-smile-env -f my-values.yaml smiledh/smilecdr</code></p>"},{"location":"guide/smilecdr/updating/#automatic-deployment-of-config-changes","title":"Automatic Deployment of Config Changes","text":"<p>Normally, changes that do not directly affect the Pod definition of a Deployment in Kubernetes will not trigger a deployment. Typically, this means that manual recycling of Pods may be required to force updates.</p> <p>To ensure that all changes are automatically deployed, the Smile CDR Helm Chart uses a unique <code>sha256</code> hash to identify any <code>ConfigMap</code> objects. This means that any configuration changes will be detected and automatically deployed without interruption using the Rolling Deployment strategy.</p> <p>NOTE: An extra benefit of this technique is that if a new configuration has an error and the pods fail to come up, then the existing Pods will still use their original configuration, even if they need to be restarted.</p> <p>This feature can be disabled if required by setting <code>autoDeploy</code> to <code>false</code></p>"},{"location":"guide/smilecdr/updating/#argocd-considerations","title":"ArgoCD Considerations","text":"<p>If you ArgoCD to deploy your charts, then this mechanism would cause previous versions of the <code>ConfigMap</code> to be deleted after you perform configuration changes. This interferes with the ability for the existing <code>ReplicaSet</code> to scale or self-heal. To avoid this issue, you should set <code>argocd.enabled</code> to true to prevent this issue. By doing this, it will add annotations to any <code>ConfigMap</code> resources that are identified by their hash, so that ArgoCD does not prune the resources.</p>"},{"location":"guide/smilecdr/updating/#long-running-processes","title":"Long Running Processes","text":"<p>Although these techniques will avoid any disruption to the application availability, any long running processes may be interrupted. Remember to design any workflows to be able to handle unexpected disruption, using retry mechanisms for any tasks that do not complete correctly due to transient infrastructure interruption.</p>"},{"location":"guide/smilecdr/modules/","title":"Configuring Smile CDR Modules","text":"<p>This guide covers configuration details for the Smile CDR Helm Chart</p>"},{"location":"guide/smilecdr/modules/#smile-cdr-configuration-options","title":"Smile CDR Configuration Options","text":"<p>Please note that for details on the Smile CDR configuration options, you should consult the official product documentation on the Smile CDR website</p>"},{"location":"guide/smilecdr/modules/cdrnode/","title":"Smile CDR Cluster Configuration","text":"<p>Smile CDR is designed to be installed with flexible cluster configurations, as per the documentation here.</p> <p>Using this Helm Chart, it is possible to configure your Smile CDR cluster with a flexible architecture design:</p> <ul> <li> <p>Single Node All modules are contained in a single Node configuration.</p> </li> <li> <p>Multi Node Modules can be split amongst an arbitrary number of Node configurations.</p> </li> </ul> <p>When using a multi-node architecture, each Node will be deployed using separate Kubernetes resources. All configurations, such as the number of replicas, resource requests &amp; limits, probes, autoscaling configurations, mapped files and CDR module configurations, can be configured separately for each Node.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#single-node-design","title":"Single Node Design","text":"<p>By default, Smile CDR will be installed with a single-node configuration. No special actions are required to run Smile CDR with this configuration.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#overriding-default-nodeid","title":"Overriding Default NodeID","text":"<p>When using the default configuration, a single Smile CDR Node will be created with a <code>NodeID</code> of <code>Masterdev</code>. If required this can be changed using one of two methods.</p> <p>Method 1 - Override name in default configuration</p> <p>This is the default configuration which results in <code>node.id</code> being set to <code>Masterdev</code> <pre><code>cdrNodes:\n  masterdev:\n    name: Masterdev\n</code></pre></p> <p>To override this name, simply alter the <code>name:</code> field in your <code>values.yaml</code> file.</p> <p>The following configuration results in <code>node.id</code> being set to <code>MyNodeID</code> <pre><code>cdrNodes:\n  masterdev:\n    name: MyNodeID\n</code></pre></p> <p>Method 2 - Disable default configuration</p> <p>The above mentioned settings use the <code>cdrNode.masterdev</code> YAML dictionary that is predefined in the default <code>values.yaml</code> file. This dictionary contains a number of default node settings as follows: <pre><code>cdrNodes:\n  masterdev:\n    name: Masterdev\n    enabled: true\n    config:\n      locked: true\n      troubleshooting: false\n    security:\n      strict: false\n</code></pre></p> <p>If you wish to start with a fresh node configuration, you can disable the default <code>cdrNodes.masterdev</code> and configure your own item that does not inherit any default values.</p> <p>Add the following snippet to your <code>values.yaml</code> file to disable the default configuration and define your own node using the <code>mynode</code> key, with a <code>node.id</code> of <code>MyNode</code> <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  mynode:\n    enabled: true\n    name: MyNode\n</code></pre></p>"},{"location":"guide/smilecdr/modules/cdrnode/#multi-node-design","title":"Multi Node Design","text":"<p>WARNING: Multi-Node configuration using this Helm Chart is still a new and developing feature. It's not recommended at this time to use it in a production environment and should only be used for evaluation purposes.</p> <p>Building on the section above, it's simple to create a multi-node configuration by adding multiple node specifications under the <code>cdrNodes</code> map entry.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#sample-architecture","title":"Sample Architecture","text":"<p>For this example, we will replicate the Sample Architecture described in the Smile CDR documentation here</p> <p></p>"},{"location":"guide/smilecdr/modules/cdrnode/#basic-configuration","title":"Basic Configuration","text":"<p>The below example configuration will install a Smile CDR cluster with two nodes, as per the above diagram.</p> <p>Note: Do NOT use this configuration, it is merely used to demonstrate the mechanisms available for defining Smile CDR Nodes. Use it only as a guideline for configuring your own multi-node cluster.</p> <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  admin:\n    name: AdminNode\n    enabled: true\n    modules:\n      clustermgr:\n        # AdminNode overrides for Cluster Manager Module...\n      admin_json:\n        # JSON Admin API Module Spec...\n      admin_web:\n        # Web Admin Console Module Spec...\n      # Disabling unused default modules.\n      fhir_endpoint:\n        enabled: false\n      fhirweb_endpoint:\n        enabled: false\n      persistence:\n        enabled: false\n      subscription:\n        enabled: false\n      smart_auth:\n        enabled: false\n      package_registry:\n        enabled: false\n      audit:\n        enabled: false\n      license:\n        enabled: false\n      transaction:\n        enabled: false\n  fhir:\n    name: FhirNode\n    enabled: true\n    resources:\n      requests:\n        cpu: \"4\"\n      limits:\n        memory: 8Gi\n    modules:\n      useDefaultModules: false\n      clustermgr:\n        # FhirNode overrides for Cluster Manager Module...\n      persistence:\n        # Persistence Module Spec...\n      fhir_endpoint:\n        # Fhir Endpoint Module Spec...\n\n# Any configurations in the root context will be used as defaults by all enabled\n# nodes in `cdrNodes`. Configurations in `cdrNodes` will have priority, so this\n# is a useful mechanism for defining global defaults to reduce config duplication.\nmodules:\n  clustermgr:\n    # Global Cluster Manager Module Spec...\n  local_security:\n    # Global Security Module Spec...\n\nresources:\n  requests:\n    cpu: \"1\"\n  limits:\n    memory: 4Gi\n</code></pre> <p>Various concepts from the above will be covered in the below sections.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>When defining a Node as per above, the Helm Chart will merge configurations from multiple locations to determine the correct value for any given setting. This applies to any of the Helm Chart settings that can be set in your <code>values.yaml</code> file.</p> <p>Configuration values are effectively determined by using the first entry found when looking in the following locations in order:</p> <ol> <li>Local Node spec in <code>cdrNodes</code> entry</li> <li>Values file root context</li> <li>Default values file</li> </ol> <p>The result of this is that any settings in the root context of the values file can be used as a mechanism for defining global defaults.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#example-explanation","title":"Example Explanation","text":"<p>In the example configuration above, we can see that this concept was utilized for specifying resource usage quotas.</p> <ul> <li>The root context defined <code>resources.requests.cpu: 1</code> and <code>resources.limits.memory: 4Gi</code>. These values become the default for all nodes</li> <li>The FhirNode overrides these values to <code>4</code> and <code>8Gi</code> respectively, thus overriding the global defaults.</li> <li><code>replicaCount</code> has neither been defined in the root context, nor in the FhirNode Spec. As such, the value for <code>replicaCount</code> will be determined from the default values file.</li> <li>There are modules defined both in the root context, as well as the Node specs under <code>cdrNodes</code>. This will be discussed in the section below.</li> </ul>"},{"location":"guide/smilecdr/modules/cdrnode/#per-node-module-definitions","title":"Per-node Module Definitions","text":"<p>When using a multi-node configuration such as above there are multiple ways that you can configure the modules for each node.</p> <ul> <li>Use default modules - Although this may work, do not do this, as all nodes would have the same set of modules, negating the purpose of having a multi-node cluster in the first place ;)</li> <li>Use default modules, but disable modules that are not required on a given node. This is a reasonable option if you wish to use the default module definitions. It could get confusing if you have a lot of module definitions as it may become unclear which modules are defined where.</li> <li>Disable default modules and define all required modules yourself. This is probably the most complicated solution, but offers improved manageability as all your modules will be defined in one location.</li> </ul> <p>In either of the options above, the same inheritance process will be used to determine the final module configuration.</p> <p>Module Inheritance with Default Modules Enabled</p> <p>If you are using the default modules, module configurations will be determined in the following order:</p> <ol> <li>Modules section of Node spec in <code>cdrNodes</code> entry</li> <li>Modules section of root context</li> <li>Default Modules file</li> </ol> <p>Module Inheritance with Default Modules Disabled</p> <p>If you have disabled the default modules, module configurations will be determined in the following order:</p> <ol> <li>Modules section of Node spec in <code>cdrNodes</code> entry</li> <li>Modules section of root context</li> </ol>"},{"location":"guide/smilecdr/modules/cdrnode/#example-explanation_1","title":"Example Explanation","text":"<p>In the example configuration above, we can see that both of these methods were demonstrated.</p> <p>AdminNode uses method 1, leaving default modules enabled.</p> <ul> <li>AdminNode leaves default modules enabled, but explicitly disables any modules that are not required in the node</li> <li>AdminNode uses global overrides for the <code>clustermgr</code> and <code>local_security</code> modules.</li> <li>AdminNode overrides values for the <code>clustermgr</code>, <code>admin_json</code> and <code>admin_web</code> modules locally.</li> </ul> <p>AdminNode uses method 2, disabling the default modules.</p> <ul> <li>FhirNode disables default modules.</li> <li>FhirNode uses global defaults for the <code>clustermgr</code> and <code>local_security</code> modules.</li> <li>FhirNode overrides values for the <code>clustermgr</code>, <code>persistence</code> and <code>fhir_endpoint</code> modules locally.</li> </ul> <p>In reality, you would likely choose one option or the other for consistency. Both were used here just for demonstration purposes.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#multi-node-considerations","title":"Multi Node Considerations","text":"<p>When running Smile CDR in a multi-node configuration, there are some things to consider. Please study the documentation for designing a cluster</p>"},{"location":"guide/smilecdr/modules/cdrnode/#clustermgr-module-configuration","title":"ClusterMgr Module Configuration","text":"<p>It is important that the configuration for the cluster manager module is mostly the same amongst the different nodes.</p> <p>Note: Despite the clustermgr configuration being identical, the Database configuration may differ slightly. See the Database Configuration section below.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#batch-job-visibility","title":"Batch Job Visibility","text":"<p>As per the documentation here, the Sample Architecture above will not have the ability to display Batch jobs in the Web Admin Console.</p> <p>A solution to this is to also configure any <code>persistence</code> modules in your AdminNode (Or any such node that has the <code>admin_web</code> module configured). If doing this, take special note of the considerations mentioned in the linked docs.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#database-configuration","title":"Database Configuration","text":"<p>If multiple modules are configured to point to the same database, consider the following:</p> <ul> <li>Only one node should have the <code>schema_update_mode</code> set to <code>UPDATE</code>. All others should have it set to <code>NONE</code></li> <li>When configuring multiple nodes/modules to point to the same persistence database (e.g. for viewing Batch Jobs as mentioned above) then:<ul> <li><code>suppress_scheduled_maintenance_jobs</code> should be set to true on all but one node</li> <li><code>read_only_mode.enabled</code> should be set to true on the AdminNode</li> <li>The <code>maxidle</code> and <code>maxtotal</code> db connections can be reduced on the AdminNode</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/modules/cdrnode/#readiness-probes","title":"Readiness Probes","text":"<p>With a default single node configuration of Smile CDR, the Kubernetes readiness probe is set up to use the health-check of the FHIR Endpoint module. Although this is a reasonable compromise when it comes to the question of \"What do I monitor\", it falls short if you need to ensure that the Web Admin Console is always available.</p> <p>With a multi-node configuration this issue is solved as you will now have a separate readiness probe configuration for each node. This means your AdminNode could use the Web Admin Console health-check and your FhirNode could use the health-check of the FHIR Endpoint.</p> <p>With such a configuration, both the Web Admin Console and the FHIR Endpoint will gain resilience and self-healing benefits from the Kubernetes control plane.</p>"},{"location":"guide/smilecdr/modules/cdrnode/#multi-node-example-configuration","title":"Multi Node Example Configuration","text":"<p>Below is a realistic example configuration for a multi-node cluster. It's based on the Sample Architecture further up in this page, but includes the following:</p> <ul> <li>Realistic comprehensive module configurations using global defaults</li> <li>Includes <code>audit</code>, <code>transaction</code> &amp; <code>license</code> modules</li> <li>Includes ability to view Batch jobs from the Web Admin Console</li> </ul>"},{"location":"guide/smilecdr/modules/cdrnode/#my-multi-node-valuesyaml","title":"<code>my-multi-node-values.yaml</code>","text":"Click to expand <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  admin:\n    name: AdminNode\n    enabled: true\n    modules:\n      clustermgr:\n        config:\n          db.schema_update_mode: UPDATE\n      audit:\n        config:\n          db.schema_update_mode: UPDATE\n      transaction:\n        config:\n          db.schema_update_mode: UPDATE\n          # transactionlog.show_request_body.enabled: true\n      persistence:\n        config:\n          suppress_scheduled_maintenance_jobs: true\n          read_only_mode.enabled: true\n          db.connectionpool.maxidle: 2\n          db.connectionpool.maxtotal: 4\n      admin_web:\n        name: Web Admin\n        enabled: true\n        type: ADMIN_WEB\n        enableReadinessProbe: true\n        service:\n          enabled: true\n          svcName: admin-web\n          hostName: default\n        requires:\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: \"\"\n          port: 9100\n          # tls.enabled: false\n          # https_forwarding_assumed: true\n          # respect_forward_headers: true\n      admin_json:\n        name: JSON Admin Services\n        enabled: false\n        type: ADMIN_JSON\n        service:\n          enabled: true\n          svcName: admin-json\n        requires:\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: json-admin\n          port: 9000\n          tls.enabled: false\n          anonymous.access.enabled: true\n          security.http.basic.enabled: true\n          https_forwarding_assumed: true\n          respect_forward_headers: true\n  fhir:\n    name: FHIRNode\n    enabled: true\n    modules:\n      persistence:\n        config:\n          db.schema_update_mode: UPDATE\n      fhir_endpoint:\n        name: FHIR Service\n        enabled: true\n        type: ENDPOINT_FHIR_REST\n        enableReadinessProbe: true\n        service:\n          enabled: true\n          svcName: fhir\n          hostName: default\n        requires:\n          PERSISTENCE_ALL: persistence\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: fhir_request\n          port: 8000\n          security.http.basic.enabled: true\n\nmodules:\n  useDefaultModules: false\n  clustermgr:\n    name: Shared Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  NONE\n      audit_log.db.always_write_to_clustermgr: false\n      audit_log.request_headers_to_store: Content-Type,Host\n      transactionlog.enabled: false\n      retain_transaction_log_days: 7\n\n  local_security:\n    name: Shared Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n\n  audit:\n    name: Shared Audit DB Config\n    enabled: true\n    type: AUDIT_LOG_PERSISTENCE\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  NONE\n\n  license:\n    name: Shared License Module Config\n    type: LICENSE\n    enabled: true\n\n  transaction:\n    name: Shared Transaction Log DB\n    enabled: true\n    type: TRANSACTION_LOG_PERSISTENCE\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.username: \"#{env['DB_USER']}\"\n      db.password: \"#{env['DB_PASS']}\"\n      db.schema_update_mode: NONE\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode: NONE\n</code></pre>"},{"location":"guide/smilecdr/modules/hl7v2/","title":"HL7 v2.x Listening Endpoint Module","text":"<p>To use the HL7 v2.x Listening Endpoint module with this Helm Chart, special configuration is required. For more info on this module, please refer to the official Smile CDR documentation here</p> <p>This module supports 2 transport mechanisms:</p> <ul> <li><code>HL7_OVER_HTTP</code></li> <li><code>MLLP_OVER_TCP</code></li> </ul> <p>Currently, the Helm Chart only supports using the <code>HL7_OVER_HTTP</code> transport protocol. <code>MLLP_OVER_TCP</code> support may be added in the future.</p>"},{"location":"guide/smilecdr/modules/hl7v2/#prerequisites","title":"Prerequisites","text":"<p>To use this module, you need to configure an additional DNS entry. This is because this module will only function using the root context path (i.e. <code>https://hl7endpoint.mydomain.com/</code>) which prevents it from running on the same hostname as the other Smile CDR endpoints.</p> <p>The DNS entry created for this module should point to the same load balancer that is used for the other Smile CDR endpoints.</p>"},{"location":"guide/smilecdr/modules/hl7v2/#configuring-module","title":"Configuring Module","text":"<p>To configure the HL7 v2.x Listening Endpoint module to use the above domain, you need to add a <code>hostName</code> entry to the <code>service</code> section of your module definition.</p> <p>Use the following module definition to enable this module and ingress route.</p> <pre><code>modules:\n  hlendpoint:\n    name: hl7v2\n    enabled: true\n    type: ENDPOINT_HL7V2_IN\n    service:\n      enabled: true\n      svcName: hl7v2\n      hostName: hl7endpoint.mydomain.com\n    requires:\n      PERSISTENCE_ALL: persistence\n    config:\n      port: 8008\n      store_original_message: true\n      transport: HL7_OVER_HTTP\n</code></pre>"},{"location":"guide/smilecdr/modules/license/","title":"Configuring Smile CDR License","text":"<p>Some components of Smile CDR require an additional license in order for them to function. This guide shows how to enable your license in a secure fashion.</p>"},{"location":"guide/smilecdr/modules/license/#prerequisites","title":"Prerequisites","text":"<p>The recommended way to configure your Smile CDR license is by importing it from a secure secrets vault. Currently this chart only supports AWS Secrets Manager using the Secrets Store CSI Driver. For more information on these pre-requisites and how secrets are handled in this chart, please refer to the Secrets Handling section of this guide.</p>"},{"location":"guide/smilecdr/modules/license/#configure-using-secrets-store-csi","title":"Configure using Secrets Store CSI","text":""},{"location":"guide/smilecdr/modules/license/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>The Smile CDR License is a regular JWT token. When storing it in an AWS Secrets Manager Secret, store it under the <code>jwt</code> key in the JSON object.</p>"},{"location":"guide/smilecdr/modules/license/#values-file","title":"Values File","text":"<p>Add a snippet in your values file like so</p> <pre><code>license:\n  type: sscsi\n  provider: aws\n  secretArn: arn:aws:secretsmanager:us-east-1:111111111111:secret:my-smile-license\n</code></pre> <p>Note: This uses the <code>secretSpec</code> schema defined here</p>"},{"location":"guide/smilecdr/modules/license/#alternative-methods","title":"Alternative methods","text":"<p>If you do not wish to use the above method, you can also include your license file using the existing method for including files as described in the Including Extra Files section of this guide.</p> <p>If you use this method, you will also need to update your module configuration so that <code>license.config.jwt_file</code> points to the correct file.</p> <p>This chart does not currently support providing the licence directly via Kubernetes <code>Secret</code> objects.</p> <p>WARNING: Be aware that your license should be considered sensitive material. If you do use this method, your license may show up in your infrastructure logs that are used to provision this Helm Chart.</p>"},{"location":"guide/smilecdr/modules/modules/","title":"Module Configuration","text":"<p>Configuring modules is fairly straight forward, but somewhat different than the existing methods used for configuring Smile CDR which uses the <code>cdr-config-Master.properties</code> file.</p> <p>This file is still used behind the scenes, but is automatically generate by the Helm Chart and cannot be modified directly.</p> <p>NOTE: When using Helm Charts, they become the 'single source of truth' for your configuration. This means that repeatable, consistent deployments become a breeze. It also means you should not edit your configuration options in the Smile CDR web admin console.</p> <p>You can define your modules in your main values file, or you can define them in separate files and include them using the helm <code>-f</code> command. This is possible because Helm accepts multiple values files</p> <p>We recommend defining them in one or more separate files, as this allows you to manage common settings as well as per-environment overlays. We will discuss this further down in the Advanced Configuration section below.</p>"},{"location":"guide/smilecdr/modules/modules/#mapping-traditional-smile-cdr-configuration-to-helm","title":"Mapping traditional Smile CDR configuration to Helm","text":"<p>Mapping existing configurations to values files is relatively straight forwards:</p>"},{"location":"guide/smilecdr/modules/modules/#identify-the-module-configuration-parameter","title":"Identify the module configuration parameter.","text":"<p>e.g. Concurrent Bundle Validation Config.properties format: <code>module.persistence.config.dao_config.concurrent_bundle_validation = false</code></p>"},{"location":"guide/smilecdr/modules/modules/#specify-them-in-the-values-yaml-file-format","title":"Specify them in the values yaml file format:","text":"<p><pre><code>modules:\n  persistence:\n    config:\n      dao_config.concurrent_bundle_validation: \"false\"\n</code></pre> The same effective mapping can be used for any module configurations supported by Smile CDR.</p>"},{"location":"guide/smilecdr/modules/modules/#included-pre-defined-module-definitions","title":"Included pre-defined Module Definitions","text":"<p>This chart includes a set of pre-defined module configurations that closely matches the default <code>cdr-config-Master.properties</code> configuration that is included with Smile CDR.</p> <p>Some of these module configurations are slightly different in order to better accommodate deploying in Kubernetes. If you wish to review this default configuration, or use it as a baseline for your own custom set of module configurations, you can get it here</p> <p>WARNING: If you use a copy of the pre-defined modules, then be aware that future versions of this Helm Chart may introduce breaking changes that will require you to review changes in the default modules, so that you can update your custom module definitions.</p>"},{"location":"guide/smilecdr/modules/modules/#configuring-endpoint-modules","title":"Configuring Endpoint Modules","text":"<p>Many Smile CDR modules provide services that can be consumed via some API. These modules implement listeners that expose them to other parts of your infrastructure. In order for these <code>endpoint</code> modules to be exposed, additional Kubernetes resources need to be created in the cluster.</p>"},{"location":"guide/smilecdr/modules/modules/#configure-service","title":"Configure Service","text":"<p>This Helm Chart will configure these resources based on the configurations provided in the module's <code>service</code> configuration.</p> <p>e.g. This snippet will create a <code>Service</code> resource and a <code>rule</code> in the default <code>Ingress</code> resource for the Web Admin Console: <pre><code>modules:\n  admin_web:\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    config:\n      port: 9100\n</code></pre></p> <ul> <li><code>moduleSpec.service.enabled</code> must be set to <code>true</code> to enable the service</li> <li><code>moduleSpec.service.svcName</code> represents the name of the created resources and must be unique for each module</li> <li><code>moduleSpec.service.hostName</code> can be overridden with an explicit host name. If the value <code>default</code> is used, it will derive the hostName from <code>specs.hostname</code></li> <li><code>moduleSpec.service.config.port</code> must be set to a unique value for each module that defines an ingress.</li> </ul>"},{"location":"guide/smilecdr/modules/modules/#configure-ingress","title":"Configure Ingress","text":"<p>As this Helm Chart supports multiple Ingress resources, you may need to specify which ingress is used by any modules that need to be exposed externally to the Kubernetes cluster.</p> <p>See the Ingress section for more information on Ingress Configurations.</p>"},{"location":"guide/smilecdr/modules/modules/#default-ingress-configuration","title":"Default Ingress Configuration","text":"<p>In a default Helm Chart installation, only a single Ingress resource is created and any modules with enabled <code>service</code> configurations will use this ingress by default. No extra module configuration is required in this scenario.</p>"},{"location":"guide/smilecdr/modules/modules/#custom-ingress-configuration","title":"Custom Ingress Configuration","text":"<p>If you wish to use a non-default Ingress resource, then this needs to be specified on a per-module basis.</p> <p>Before specifying any ingresses here, a custom Ingress Configuration needs to be created.</p> <p>In the following snippet, we will configure the Web Admin Console to use a custom internal Ingress resource and the FHIR endpoint to use a custom public Ingress resource. All other modules will use the default Ingress Resource.</p>"},{"location":"guide/smilecdr/modules/modules/#my-module-valuesyaml","title":"<code>my-module-values.yaml</code>","text":"<pre><code>modules:\n  admin_web:\n    service:\n      ingresses:\n        myPrivateNginx:\n          enabled: true\n  fhir_endpoint:\n    service:\n      ingresses:\n        myPublicNginx:\n          enabled: true\n</code></pre> <p>NOTE: When defining a custom ingress here, you do not need to explicitly disable the default ingress as it's disabled automatically.</p>"},{"location":"guide/smilecdr/modules/modules/#module-definition-considerations","title":"Module definition considerations","text":"<p>Here are some additional fields/considerations that need to be included in your module definitions files:</p> <ul> <li>Though not strictly required by the <code>yaml</code> spec, all values should be quoted.   You may run into trouble with some values if you do not quote them.   Specifically, values starting with <code>*</code> or <code>#</code> will fail if not quoted.</li> <li>The <code>module id</code> is taken from the yaml key name.</li> <li>Modules can be defined, but disabled. They need to be enabled with the <code>enabled: true</code> entry. Disabled modules will not be included in the generated <code>cdr-config-*.properties</code> file</li> <li>Modules other than the cluster manager need to define <code>type</code>. A list of module types is available here</li> <li>Modules which expose an endpoint need to de defined with a <code>service</code> entry, which defines infrastructure resources that are required to access the module.</li> <li>DB credentials/details can be referenced from your module configurations via <code>DB_XXX</code> environment variables.</li> </ul> <p>Any configurations you specify will merge with the defaults, priority going to the values file.</p>"},{"location":"guide/smilecdr/modules/modules/#disabling-included-pre-defined-module-definitions","title":"Disabling included pre-defined module definitions","text":"<p>If you wish to disable any of the pre-defined default modules, you can do so individually, or you can disable all default modules and define your own from scratch.</p> <p>If you do the latter, it may be easier to determine the exact modules you have defined just by looking at your values files.</p> <p>NOTE: If doing this, you may need to review upstream changes when moving to a newer version of the Helm Chart.</p> <p>You can disable all default modules using: <pre><code>modules:\n  useDefaultModules: false\n</code></pre> You can reference the <code>default-modules.yaml</code> file as a reference by untarring the Helm Chart or viewing it directly from the repository.</p> <p>Here is an example of what your module definition may look like when configuring Smile CDR with the <code>clustermgr</code>, <code>persistence</code>, <code>local_security</code>, <code>fhir_endpoint</code> and <code>admin_web</code> modules.</p>"},{"location":"guide/smilecdr/modules/modules/#my-module-valuesyaml_1","title":"<code>my-module-values.yaml</code>","text":"Click to expand <pre><code>modules:\n  useDefaultModules: false\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      password_encoding_type: BCRYPT_12_ROUND\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n</code></pre>"},{"location":"guide/smilecdr/modules/modules/#define-readiness-probe","title":"Define Readiness Probe","text":"<p>As Kubernetes only supports a single readiness probe per container, you need to define which endpoint module Kubernetes should use to consider the 'readiness' of your installation.</p> <p>The default modules included with this chart are configured so that the <code>fhir_endpoint</code> module is used for the readiness probe. This is done by setting the <code>enableReadinessProbe</code> key to <code>true</code> in the module definition.</p> <p>If you wish to use a different module for the readiness probe, you must disable it for the <code>fhir_endpoint</code> module and enable it for the module of your choice. e.g.</p>"},{"location":"guide/smilecdr/modules/modules/#my-module-valuesyaml_2","title":"<code>my-module-values.yaml</code>","text":"<p><pre><code>modules:\n  fhir_endpoint:\n    enableReadinessProbe: false\n  my_fhir_endpoint:\n    enableReadinessProbe: true\n    enabled: true\n    ...\n</code></pre> Alternatively, you may disable the included default modules as described above, and then enable the probe on one of your custom defined modules.</p> <p>Note: You must enable the readiness probe for exactly one endpoint module. If you specify none, or more than one, the Helm Chart will return an error.</p>"},{"location":"guide/smilecdr/modules/modules/#install-smile-cdr-with-extra-modules-definition-files","title":"Install Smile CDR with extra modules definition files","text":"<p>When splitting your configuration into multiple <code>values</code> files, pass them in to your <code>helm upgrade</code> commandline like so: <pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml -f my-module-values.yaml smiledh/smilecdr\n</code></pre></p>"},{"location":"guide/smilecdr/modules/modules/#experimentalunsupported-features","title":"Experimental/Unsupported Features","text":"<p>There are scenarios where you may wish to update Smile CDR module configurations directly in the Web console.</p> <ul> <li>You need to do some realtime troubleshooting that requires live updates of module configuration</li> <li>You are working in a development environment and you do not have a suitable code pipeline in place to enable fast iteration of changes</li> </ul> <p>In these cases, there are two settings that you can use to update configuration live in the Smile CDR Web Admin console. Using these settings will alter the values of <code>node.config.locked</code> and <code>node.propertysource</code> in the resulting Smile CDR configuration. Re fer to the Smile CDR Docs for more info on Module Property Sources.</p>"},{"location":"guide/smilecdr/modules/modules/#troubleshooting-mode","title":"Troubleshooting Mode","text":"<p>You may enable troubleshooting mode for a given Smile CDR Node as follows:</p> <p>Note: If you have defined a different <code>cdrNodes</code> configuration, please alter the code below accordingly.</p> <p><pre><code>cdrNodes:\n  masterdev:\n    config:\n      troubleshooting: true\n</code></pre> Enabling this option lets you update module configurations in the console for troubleshooting/testing/experimenting.</p> <ul> <li>Your changes will be lost if the pod restarts or if another pod joins the cluster.</li> <li>It sets <code>node.config.locked</code> to <code>false</code> and sets <code>node.propertysource</code> to <code>PROPERTIES_UNLOCKED</code></li> </ul>"},{"location":"guide/smilecdr/modules/modules/#database-mode","title":"Database Mode","text":"<p>The troubleshooting mode may not meet your requirements in some scenarios:</p> <ul> <li>You need these changes to persist for a longer time period and your underlying compute resources could be interrupted. For example:<ul> <li>You are using ephemeral compute resources such as AWS EC2 Spot instances which can go away with short notice.</li> <li>Your infrastructure needs to be scaled down when not actively working on it.</li> </ul> </li> <li>You wish to restart the Smile CDR pods while maintaining your manual configuration changes.</li> <li>New Kubernetes Pods may come online (If you are testing HPA or HA, for example.)</li> </ul> <p>In these scenarios, it would be a more robust solution to regularly mirror your configuration changes to your Helm <code>values</code> file and reconcile.</p> <p>In the event that this is not possible, and your manually entered configurations must persist in the above scenarios, you may use the <code>database</code> mode as follows:</p> <p>Note: If you have defined a different <code>cdrNodes</code> configuration, please alter the code below accordingly.</p> <pre><code>cdrNodes:\n  masterdev:\n    config:\n      database: true\n</code></pre> <p>When enabling this mode, consider the following:</p> <ul> <li>This is an experimental feature and is unsupported. Use at your own risk.</li> <li>It sets <code>node.config.locked</code> to <code>false</code></li> <li>It sets <code>node.propertysource</code> to <code>DATABASE</code></li> <li>The Helm Chart will still create surrounding Kubernetes resources (Ingress, Service, Extra files, Mapped secrets etc) based on the contents of the Helm <code>values</code> file.</li> <li>If you add a new module from the Smile CDR Web console and that module has an endpoint configuration, you will NOT be able to access it. No Ingress or Service objects will be created unless you also create the module using the Helm Chart.</li> <li>Any changes made in the Smile CDR Web console that do not match the Helm <code>values</code> settings will lead to configuration drift that may cause unpredictable behaviour the next time you deploy the Helm Chart.</li> <li>In the event of such drift occurring, reverting this mode to <code>disabled</code> may then lead to unpredictable behaviour that could result in modules being incorrectly configured, resulting to critical system or data integrity faults. <p>!!!DO NOT USE THIS EXPERIMENTAL UNSUPPORTED FEATURE IN NON-DEVELOPMENT ENVIRONMENTS!!!</p> </li> </ul>"},{"location":"guide/smilecdr/operations/smileutil/","title":"Using The Smile CLI Tool (<code>smileutil</code>)","text":"<p>Many operational tasks within Smile CDR can be performed using the <code>smileutil</code> tool that is included when installing using this Helm Chart. Please refer to the <code>smileutil</code> documentation for more information on what functions it supports.</p> <p>In a local or VM installation, it is a common practice to run this command directly inside a running instance of Smile CDR.</p> <p>The Smile CDR helm chart currently has limited support for using the Smile CLI as this is an incubating feature set.</p>"},{"location":"guide/smilecdr/operations/smileutil/#using-smile-cli-in-kubernetes","title":"Using Smile CLI in Kubernetes","text":"<p>There are 2 ways that such operational tasks could be performed on a Pod in Kubernetes.</p> <p>Exec into Pod</p> <p>The simple way to perform ad-hoc operational tasks is to directly <code>exec</code> into the pod using the <code>kubectl</code> tool.</p> <p>When following best security practices, it is not advisable to allow administrators to directly connect to running Pods to perform operational tasks, as doing so may expose a number of security requirements and concerns:</p> <ul> <li>User must have <code>kubectl</code> tooling on their workstation, along with any required credentials for the cloud account and Kubernetes cluster.</li> <li>User must have <code>exec</code> RBAC permissions in the cluster</li> <li>User may be able to view secret material mounted in the running environment</li> <li>User may be able to interrupt the running application</li> <li>No way to control which <code>smileutil</code> commands can be run</li> <li>High chance of error when typing commands by hand.</li> <li>Low repeatability and re-use of commands and parameters</li> </ul> <p>Currently this is the only mechanism available for running the <code>smileutil</code> command when installing using the Smile CDR Helm Chart</p> <p>Use <code>smileutil</code> Job*</p> <p>Use a Kubernetes <code>Job</code> that performs the <code>smileutil</code> command for you in a codified and repeatable manner.</p> <p>By using a Kubernetes Job, the options passed to <code>smileutil</code> can be codified resulting in low manual effort and high repeatability.</p> <p>Note: Currently, the Smile CDR Helm Chart does not support this functionality. Until this feature is implemented, the exec into pod method must be used.</p>"},{"location":"guide/smilecdr/operations/smileutil/#preparing-pod-and-running-smileutil","title":"Preparing Pod and Running <code>smileutil</code>","text":"<p>Depending on the particular Smile CLI command you plan to run, you may need to adjust the available resources in your Pod.</p>"},{"location":"guide/smilecdr/operations/smileutil/#prepare-pod-resources","title":"Prepare Pod resources","text":"<p>Memory</p> <p>When running <code>smileutil</code>, it will run its own JVM, with its own memory heap. It may be required to increase the amount of unallocated memory in the Pod if there is not enough. This can be done in multiple ways:</p> <ul> <li>Increase the Pod resource limits and reduce the <code>memoryFactor</code> in the JVM Heap Auto-Sizing.</li> <li>Specify a Pod memory limit that is larger than the Pod memory request. The difference between these two values signifies the amount of unallocated memory available to other commands like <code>smileutil</code></li> </ul> <p>Ephemeral Volumes</p> <p>In some cases, the <code>smileutil</code> command may need to write temporary file. Alternatively you may need to upload files to the pod in order to run your command. In either of these scenarios, you may need to adjust the size limit of some of the ephemeral volumes, such as <code>/home/smile/smilecdr/tmp</code> or <code>/home/smile/smilecdr/customerlib</code>.</p> <p>Refer to the volume configuration and adding files sections for information on how to do this.</p>"},{"location":"guide/smilecdr/operations/smileutil/#exec-into-pod","title":"Exec into pod","text":"<p>Once your Pod is prepared, you can <code>exec</code> into your running pod using the <code>kubectl</code> command:</p> <pre><code>kubectl exec -ti &lt;podname&gt; -n &lt;namespacename&gt; bash\n</code></pre>"},{"location":"guide/smilecdr/operations/smileutil/#supported-smile-cli-tool-commands","title":"Supported Smile CLI Tool Commands","text":"<p>The following <code>smileutil</code> commands are currently supported.</p>"},{"location":"guide/smilecdr/operations/smileutil/#upload-terminology","title":"<code>upload-terminology</code>","text":"<p>There are two ways to run the upload terminology command.</p>"},{"location":"guide/smilecdr/operations/smileutil/#run-from-external-location","title":"Run from external location","text":"<p>When running this command from an external location, the <code>smileutil</code> utility will upload the zip file to the Smile CDR Pod, which will keep a copy of the zip file in memory before unzipping it to the temp dir and processing the records.</p> <p>For this to work, the pod must be configured as follows:</p> <ul> <li>JVM heap size must be sufficiently sized. This process requires a lot of memory when dealing with very large zip files. For example, a 600MB zip file will require about 4GB more heap size than normal.</li> <li>Temp directory must be large enough to hold the uncompressed data. For example, a typical 600MB terminology zip file may unzip to ~4GB. The temp directory should be set to at least 4GB</li> </ul> <p>Run the following command from your external location: <pre><code>./smileutil upload-terminology -d /path/to/terminology.zip -s 1GB -v r4 -b user:password -t \"https://myEnvironment.example.com/fhir_request/\" -u \"http://snomed.info/sct\"\n</code></pre></p> <p>Note: It is important to specify the <code>-s</code> option to be a value larger than the size of the zip file you are uploading. This forces the command to run in 'remote' mode where it copies the file to the Smile CDR server before unzipping and processing.</p>"},{"location":"guide/smilecdr/operations/smileutil/#run-from-within-a-smile-cdr-pod","title":"Run from within a Smile CDR Pod","text":"<p>To reduce resource usage, this command can be run from inside the Smile CDR pod. In this case, you will need to somehow copy the file into the Pod. This can be done either manually using <code>kubectl cp</code> or using the add files functionality to copy the file from S3 to the <code>customerlib</code> or <code>classes</code> directory. The <code>smileutil upload-terminology</code> command will then copy the file to the temp directory and the Smile CDR application will reference the file locally and unzip the file, also to the temp directory.</p> <p>For this to work, the pod must be configured as follows:</p> <ul> <li>JVM heap size may need to be increased a little. There is still some overhead required in the Smile CDR JVM, as well as the Smile CLI JVM. A 600MB zip file seems to require ~2GB extra on both.</li> <li>Temp directory must be large enough to hold a copy of the zip file as well as all of the uncompressed data. For example, a typical 600MB terminology zip file may unzip to ~4GB. The temp directory should be set to at least 4GB</li> <li>The <code>customerlib</code> or <code>classes</code> directory used to upload the file will need to be large enough to hold the zip file.</li> </ul> <p>Review the Helm Values example snippet to prepare your environment as described above.</p> <p>Use <code>kubectl</code> to <code>exec</code> into your running pod: <pre><code>kubectl exec -ti &lt;podname&gt; -n &lt;namespacename&gt; bash\n</code></pre></p> <p>Run the following command from inside the pod: <pre><code>JAVA_OPTS=-Xmx2g /home/smile/smilecdr/bin/smileutil upload-terminology -d /home/smile/smilecdr/customerlib/terminologyfile.zip -v r4 -b user:password -t \"http://localhost:8000/fhir_request/\" -u \"http://snomed.info/sct\"\n</code></pre></p> <p>Note 1: This assumes a default Smile CDR install with the FHIR endpoint running on port 8000</p> <p>Note 2: It is important to remove the <code>-s</code> option or set it to be a value LOWER than the size of the zip file you are uploading. This forces the command to run in 'local' mode where it copies the file to the temp directory and instructs the Smile CDR server to reference the local file before unzipping and processing.</p>"},{"location":"guide/smilecdr/storage/files/","title":"Including Extra Files","text":"<p>It is often required to include extra files into your Smile CDR instance. This could be to provide updated configuration changes (e.g. a modified <code>logback.xml</code>), provide <code>.js</code> scripts, <code>.jar</code> files and other libraries to extend the functionality of Smile CDR.</p> <p>Rather than having to build a custom Smile CDR container image to include these files, it is possible to include them using this Helm Chart.</p>"},{"location":"guide/smilecdr/storage/files/#available-methods","title":"Available Methods","text":"<p>There are two mechanisms available to load files.</p> <ul> <li>Including files in the Helm deployment</li> <li>Pulling files from an external location</li> </ul> <p>Each of these mechanisms has its own advantages.</p>"},{"location":"guide/smilecdr/storage/files/#choosing-which-method-to-use","title":"Choosing Which Method To Use","text":""},{"location":"guide/smilecdr/storage/files/#helm-chart-method","title":"Helm Chart Method","text":"<p>Using the Helm Chart method is ideal when:</p> <ul> <li>The files are text based and under 1MiB in size<ul> <li>Config files and small scripts are good examples</li> <li>Not ideal for binary files, even if small</li> </ul> </li> <li>You do not have many files to add<ul> <li>Although there is no limit, your configuration will get very hard to manage if you use too many</li> <li>Between 5 &amp; 10 would be a good limit, but this is just a suggestion</li> </ul> </li> <li>You don't have a mechanism in place to stage the files somewhere (i.e. Amazon S3)<ul> <li>This method provides a simple deployment solution as it has no external dependencies</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/storage/files/#external-pull-method","title":"External Pull Method","text":"<p>Using the External Pull method is ideal when:</p> <ul> <li>You have binary files or large files<ul> <li>Any file over 1MiB requires you use this method</li> </ul> </li> <li>You have many files<ul> <li>This mechanism will copy files recursively without clogging up your configuration</li> </ul> </li> <li>You are able to stage your files and file updates on Amazon S3<ul> <li>Currently only S3 is supported, but other external file sources will be added as required</li> </ul> </li> <li>You wish to pull files that are publicly hosted (e.g. public <code>.jar</code> files)</li> </ul>"},{"location":"guide/smilecdr/storage/files/#using-both-methods","title":"Using Both Methods:","text":"<p>Using both methods is possible too:</p> <ul> <li>If you had a set of <code>.jar</code> files and scripts being staged on S3, you could still add files using the Helm chart method if it makes for a simpler workflow</li> <li>Be wary that having it split up like this could make your configuration more confusing (i.e. \"Where was that file copied from again?\")</li> <li>Files copied using the Helm Chart method will take precedence over any files copied from an external source.</li> </ul>"},{"location":"guide/smilecdr/storage/files/#using-the-helm-chart-method","title":"Using the Helm Chart Method","text":"<p>To pass in files using the Helm Chart, there are two things you need to do: 1. Use a Helm commandline option to load the file into the deployment 2. Reference and configure the file in your values file.</p>"},{"location":"guide/smilecdr/storage/files/#include-file-in-helm-deployment","title":"Include File in Helm Deployment","text":"<p>To include a file in the deployment, use the following commandline option: <pre><code>helm upgrade -i my-smile-env --devel -f my-values.yaml --set-file mappedFiles.logback\\\\.xml.data=logback.xml smiledh/smilecdr\n</code></pre></p> <p>WARNING: Pay special attention to the escaping required to include the period in the filename. You need to use <code>\\\\.</code> when running this from a shell. This is just the way this works.</p> <p>This will encode the file and load it into the provided values under the <code>mappedFiles.logback.xml.data</code> key.</p>"},{"location":"guide/smilecdr/storage/files/#include-file-in-values-file","title":"Include File in Values File","text":"<p>The included file also needs to be referenced from your values file so that the chart knows where to mount the file in the application's Pod: <pre><code>mappedFiles:\n  logback.xml:\n    path: /home/smile/smilecdr/classes\n</code></pre> As the result of the above, a <code>ConfigMap</code> will be created and mapped into the pod at <code>/home/smile/smilecdr/classes/logback.xml</code> using <code>Volume</code> and <code>VolumeMount</code> resources. If the content of the file is changed, then it will be automatically picked up on the next deployment. (See Automatic Deployment of Config Changes for more info on this)</p>"},{"location":"guide/smilecdr/storage/files/#using-the-external-pull-method","title":"Using the External Pull Method","text":"<p>The external pull method can be used to pull files from Amazon S3 or from public websites that publish resources (e.g. Maven).</p>"},{"location":"guide/smilecdr/storage/files/#how-it-works","title":"How It Works","text":""},{"location":"guide/smilecdr/storage/files/#shared-volumes","title":"Shared Volumes","text":"<p>Pod-local shared volumes are used for the <code>classes</code> and <code>customerlib</code> directories so that the files can be copied there before the main Smile CDR container starts up.</p> <p>These volumes are only accessible to containers running inside the pods and are deleted when the pod is terminated so they are not accessible outside the pod's lifecycle. If the underlying Kubernetes node volume uses encrypted storage, then these volumes will also be encrypted.</p>"},{"location":"guide/smilecdr/storage/files/#init-containers","title":"Init Containers","text":"<p>Kubernetes init containers are then used to pull files from S3, or some other location.</p> <p>It uses multiple Kubernetes init containers to synchronize and pull files to these shared volumes during pod startup.</p> <p>This feature has been implemented to support Amazon S3 and curl. Other mechanisms may be introduced in a future version of this chart.</p> <p>The init containers are auto-configured based on the provided <code>copyFiles</code> settings. They function as follows:</p> <p><code>init-sync-classes</code></p> <ul> <li>This container copies the default files from the classes directory from the Smile CDR base image to a <code>classes</code> shared volume that is local to the pod.</li> <li>The <code>init-pull-classes</code> container will overwrite any of these files with the same names.</li> <li>This is a required step if you wish to retain the default files. As such, it's enabled by default</li> <li>If you require a 'clean' <code>classes</code> directory, this step can be disabled using <code>copyFiles.classes.disableSyncDefaults: true</code>.<ul> <li>If disabled, you will need to provide all <code>classes</code> files that are required for Smile CDR to start up (With the exception of the config properties file which is generated by this Helm Chart).</li> </ul> </li> </ul> <p><code>init-sync-customerlib</code></p> <ul> <li>This container copies the default files from the customerlib directory from the Smile CDR base image to a <code>customerlib</code> shared volume that is local to the pod.</li> <li>When using the default Smile CDR image, no files will be copied as the directory is empty.</li> <li>If using a customised image with files preloaded into the <code>customerlib</code> directory, this step is necessary to prevent those files being clobbered. As such, it's enabled by default.</li> <li>If you require a 'clean' <code>customerlib</code> directory, this step can be disabled using <code>copyFiles.classes.disableSyncDefaults: true</code>.</li> </ul> <p><code>init-pull-classes-*</code></p> <ul> <li>These containers copy files from the specified location to the classes shared volume</li> <li>Currently they support pulling files from Amazon S3 or downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> </ul> <p><code>init-pull-customerlib-*</code></p> <ul> <li>This container copies files from the specified location to the customerlib shared volume</li> <li>Currently they support pulling files from Amazon S3 or downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> </ul>"},{"location":"guide/smilecdr/storage/files/#s3-prerequisites","title":"S3 Prerequisites","text":"<p>To pass in files from an Amazon S3 bucket, you need the following prerequisites in place:</p> <ul> <li>An S3 bucket with:<ul> <li>A folder containing your <code>classes</code> files</li> <li>A folder containing your <code>customerlib</code> files</li> <li>Ideally, these should be in a higher level folder to control versioning<ul> <li>e.g. <code>v1</code>, <code>v2</code> or a <code>UID</code></li> </ul> </li> <li>Bucket should not be publicly accessible<ul> <li>It will work with public buckets too, but this is a bad security practice</li> </ul> </li> <li>Bucket should use encryption<ul> <li>Again, it will work without, but it's good security practice to encrypt everything by default</li> </ul> </li> <li>The mechanism to copy the files into this bucket is out of the scope of this Helm Chart</li> </ul> </li> <li>Service Account must be enabled and configured to use IRSA. See here for more info on this</li> <li>The IAM Role used for the Service Account must have read access to the S3 bucket</li> </ul> Required IAM policy actions for S3 copyFile configurations <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjectsInBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::bucket-name\"]\n        },\n        {\n            \"Sid\": \"GetObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:GetObject\"], ## or s3:GetObjectVersion\n            \"Resource\": [\"arn:aws:s3:::bucket-name/*\"]\n        }\n    ]\n}\n</code></pre>"},{"location":"guide/smilecdr/storage/files/#a-note-on-file-versioning","title":"A Note On File Versioning","text":"<p>Though not required, it is recommended to include some versioning structure in your S3 bucket.</p> <p>While already running pods cannot be affected by this (As they have already copied their files), any new pods that start up (e.g in scaling or reconciliation events) may be adversely affected if files have been unexpectedly changed or deleted.</p> <p>By including a new version whenever a given set of files is updated, previous deployments of the application will remain unaffected. This is also beneficial during rollbacks as the previous set of files will remain.</p> <p>This does introduce challenges of file duplication and managing multiple old versions. As the number of files included is typically low, this should not be of huge concern.</p>"},{"location":"guide/smilecdr/storage/files/#configure-helm-values-file","title":"Configure Helm Values File","text":"<p>To enable this feature, add the following snipped to your values file. Replace the bucket name and path to match your environment.</p> <pre><code>copyFiles:\n  classes:\n    sources:\n    # Copies files recursively from S3 to the classes directory\n    - type: s3\n      # disableSyncDefaults: true &lt;- Optional. Use with caution! (See above)\n      bucket: s3-bucket-name\n      path: /path-to/classes\n      # Example versioned locations.\n      # path: /v1/classes\n      # path: /v1.1/classes\n      # path: /v2/classes\n      # path: /&lt;sha256-of-file-content&gt;/classes &lt;- You could generate a sha256 hash of the entire file contents.\n      # path: /&lt;UID&gt;/classes &lt;- You could generate a unique UID for each new version\n  customerlib:\n    sources:\n    # Copies files recursively from S3 to the customerlib directory\n    - type: s3\n      bucket: s3-bucket-name\n      path: /path-to/customerlib-src\n    # Downloads a single file using curl to the customerlib directory (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n    - type: curl\n      fileName: elastic-apm/elastic-apm-agent-1.13.0.jar\n      url: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre> <p>Note: The Service Account configurations have been left out for clarity. Please refer to the Service Account guide for instructions on enabling IRSA and IAM roles.</p>"},{"location":"guide/smilecdr/storage/volumeConfig/","title":"Configure Ephemeral Volumes","text":"<p>The <code>customerlib</code>, <code>classes</code>, <code>log</code> and <code>tmp</code> directories are all mounted using temporary ephemeral volumes that only exist for the duration of the Pod.</p> <p>Currently they are backed by the underlying disk of the Kubernetes worker node that they are running on. Due to this, the default size limit for these volumes is kept to a minimum so as to reduce the disk requirements of the underlying worker node.</p> <p>Note: A future feature of this Helm Chart may allow for configuring generic ephemeral volumes which can utilized networked storage, thus eliminating the need for these volumes to require disk from the underlying worker node.</p>"},{"location":"guide/smilecdr/storage/volumeConfig/#default-configuration","title":"Default Configuration","text":"<p>The following ephemeral volumes are defined by default to have the following default size limits:</p> Name Path Default Size Limit Read/Write Notes tmp <code>/home/smile/smilecdr/tmp</code> 1Mi Y Always mounted log <code>/home/smile/smilecdr/log</code> 10Gi Y Always mounted customerlib <code>/home/smile/smilecdr/customerlib</code> 500Mi N Note1 classes <code>/home/smile/smilecdr/classes</code> 500Mi N Note1 amq <code>/home/smile/smilecdr/activemq-data</code> 10Mi Y Note2 <p>Note 1: The <code>customerlib</code> and <code>classes</code> ephemeral volumes only get created if adding extra files using the copyFiles feature. Otherwise, the existing directories from the container image remain in place.</p> <p>Note 2: The ActiveMQ data ephemeral volume only gets created if the Smile CDR is run in embedded ActiveMQ mode. If an external message broker has been enabled, then this volume is not created.</p>"},{"location":"guide/smilecdr/storage/volumeConfig/#increase-size-limit","title":"Increase size limit","text":"<p>The configured size limit is a 'soft' limit. This means that is is possible to continue writing more data to a volume, but if you do exceed the limit, Kubernetes will 'evict' the running pod. This may lead to unexpected termination of your running pod in the event that you write excessive data to these volumes.</p> <p>In some circumstances, it may be required to increase the size limit for these volumes. For example: * More space required for files being copied to <code>customerlib</code> or <code>classes</code> directories * Using Smile CDR functionality that writes data to the <code>tmp</code> directory.</p> <p>To allow for this, you can configure the ephemeral volume size limit as follows:</p> <pre><code>volumeConfig:\n  cdr:\n    tmp:\n      sizeLimit: 100Mi\n    classes:\n      sizeLimit: 1Gi\n    customerlib:\n      sizeLimit: 1Gi\n    log:\n      sizeLimit: 100Mi\n    amq:\n      sizeLimit: 20Mi\n</code></pre> <p>Warning: When setting the size limit too high, the underlying host machine may run out of disk space, leading to instability of the Kubernetes worker node and any pods running on it. Use caution when increasing these values.</p>"},{"location":"guide/smilecdr/tuning/jvm/","title":"Configuring Smile CDR JVM","text":""},{"location":"guide/smilecdr/tuning/jvm/#jvm-auto-configuration","title":"JVM Auto Configuration","text":"<p>The Smile CDR Helm Chart performs some automatic configuration of the Smile CDR JVM by generating and including the <code>JVMARGS</code> environment that is used by the <code>smilecdr</code> startup script.</p> <p>This is done so that the following settings can be automatically configured:</p> <ul> <li>JVM Heap Parameters - Based upon the configured <code>resources.requests.memory</code> and <code>resources.limits.memory</code> settings</li> <li>JVM Temporary directory location - Hard coded to <code>/home/smile/smilecdr/tmp</code> which is mounted as an ephemeral read-write volume.</li> <li>JVM extra arguments - Some extra jvm arguments are provided by default so match those added by the <code>/home/smile/smilecdr/bin/setenv</code> script. You may also add extra JVM arguments (See below)</li> </ul> <p>Due to this, it is not possible to manually configure the <code>JVMARGS</code> environment variable using the <code>extraEnvVars</code> configuration. However it is still possible to alter the behaviour of the heap auto-configuration as well as adding additional JVM arguments.</p>"},{"location":"guide/smilecdr/tuning/jvm/#jvm-heap-auto-sizing","title":"JVM Heap Auto Sizing","text":"<p>When running Java applications in Kubernetes, the max heap size should be lower than <code>resources.limits.memory</code>. This is due to the fact that the JVM uses memory for multiple purposes, not just the heap.</p> <p>The exact difference between the max heap and the available memory is not an exact science and will depend on the kind of workload you are running. As combinations of Smile CDR are almost unlimited, it is hard to determine a on-size-fits-all approach to this sizing.</p> <p>For a default installation of Smile CDR, without making any configuration changes, the Java heap size should be set to about 50-75% of the total available memory in the pod (i.e. <code>resources.limits.memory</code>).</p>"},{"location":"guide/smilecdr/tuning/jvm/#jvm-memory-factor","title":"JVM Memory Factor","text":"<p>The auto-generated value for the heap size is determined by multiplying the <code>resources.limits.memory</code> setting by the <code>jvm.memoryFactor</code>. By default, this value is set conservatively to <code>0.5</code>. With the default <code>resources.limits.memory</code> value of 4Gib, the chart sets Java <code>-Xmx</code> to <code>2048m</code>. This default results in less efficient use of the available memory, but the chance of running into an OOM (Out-Of-Memory) eviction is extremely minimal.</p> <p>By using this memory factor mechanism, you are able to easily perform vertical scaling tasks (i.e. re-configure the pod to use a higher <code>resources.limits.memory</code>) without needing to manually calculate and reconfigure the max heap. This increases the operational efficiency when running Smile CDR in a scalable fashion.</p> <p>If you need to adjust the max heap space relative to the <code>resources.limits.memory</code> setting, then you can do so by adjusting this value. like so:</p> <p><pre><code>jvm:\n  memoryFactor: 0.8\n</code></pre> With the default <code>resources.limits.memory</code> value of 4Gib, this value would cause the chart to set Java <code>-Xmx3276m</code> .</p> <p>Adjusting this value should be done with caution as the closer you get to <code>1</code>, the higher the likelihood that the pod will use all of the available memory, causing Kubernetes to terminate it with an <code>OOM Killed</code> error.</p> <p>Using metrics and analyzing your workload is necessary to find the right balance to efficiently make use of the available memory without getting pod terminations.</p>"},{"location":"guide/smilecdr/tuning/jvm/#setting-heap-minimum-size","title":"Setting Heap Minimum Size","text":"<p>If you were to set <code>jvm.memoryFactor</code> to <code>1</code> your pod is almost guaranteed to be terminated with an <code>OOM Killed</code> error, but it will happen at an unpredictable time as the heap slowly grows to a certain point.</p> <p>This can increase difficulty of troubleshooting due to the unpredictable timing. It may fail in a few minutes, or a few hours/days/weeks/never depending on the workload characteristics.</p> <p>To reduce the likelihood of such unpredictable <code>OOM Killed</code> errors, the minimum heap setting (<code>-Xms</code>) is automatically set to be the same value as <code>-Xmx</code>. If this needs to be disabled, you can do so by by setting <code>jvm.xms</code> to <code>false</code></p>"},{"location":"guide/smilecdr/tuning/jvm/#jvm-arguments","title":"JVM Arguments","text":"<p>By default, the <code>JVMARGS</code> environment will contain those settings mentioned above, as well as some extra arguments from the <code>setenv</code> script.</p> <p>If you need to pass in more arguments to the JVM, you can do so using the <code>jvm.args</code> section as follows:</p> <pre><code>jvm:\n  args:\n    -Dmyarg=myvalue\n</code></pre>"},{"location":"guide/smilecdr/tuning/podscheduling/","title":"Pod Scheduling","text":"<p>Kubernetes supports a number of mechanisms to control how pods are scheduled on to different worker nodes.</p> <p>When deploying Smile CDR using this Helm Chart, sensible defaults are used to ensure that the Smile CDR pods are appropriately spread across failure domains.</p> <p>For more information on assigning pods to nodes, study the official documentation here.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#supported-pod-assignment-configurations","title":"Supported Pod Assignment Configurations","text":"<p>The following configurations can be directly provided in your Helm Values file.</p> <ul> <li><code>nodeSelector</code></li> <li><code>affinity</code></li> <li><code>nodeName</code></li> <li><code>topologySpreadConstraints</code></li> </ul>"},{"location":"guide/smilecdr/tuning/podscheduling/#default-topology-constraints","title":"Default Topology Constraints","text":"<p>By default, this Helm Chart will include a <code>topologySpreadConstraints</code> based on the examples given in the Kubernetes docs on Topology Spread Constraints</p> <p>These defaults will only take meaningful effect when your deployment is scaled to 3 replicas or higher.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#topology-zone-constraint","title":"Topology Zone Constraint","text":"<p>The following default constraint will be configured: <pre><code>topologySpreadConstraints:\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: ScheduleAnyway\n</code></pre></p> <p>Pods from the same <code>replicaSet</code> will be forced to be scheduled in different topology zones. (e.g. Availability Zones in AWS parlance) This ensures high availability in the event of a zonal failure in the cloud provider's infrastructure.</p> <p>With <code>maxSkew: 1</code> it is possible that with only 2 replicas, they may both be scheduled in the same zone.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#kubernetes-node-constraint","title":"Kubernetes Node Constraint","text":"<p>The following default constraint will be configured: <pre><code>topologySpreadConstraints:\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: ScheduleAnyway\n</code></pre></p> <p>Pods from the same <code>replicaSet</code> will be forced to be scheduled on different Kubernetes Nodes. (i.e. Different hosts) This ensures high availability in the event of a host failure.</p> <p>With <code>maxSkew: 1</code> it is possible that with only 2 replicas, they may both be scheduled on the same host. If there are insufficient hosts available to satisfy this constraint, then they will still be scheculed on a node based on other pod assignment strategies.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#disabling-default-topology-constraints","title":"Disabling Default Topology Constraints","text":"<p>The above examples are just sensible defaults to cover some common scenarios and to increase the availability of an out-of-the box deployment.</p> <p>The subject of pod scheduling and allocation can get very complicated and there may be nuances or requirements in your architectural design that these defaults do not satisfy.</p> <p>If you wish to provide your own, fine tuned, pod allocation strategies, then the above topology constraints can be disabled by adding the following in your values file:</p> <pre><code>disableDefaultTopologyConstraints: true\n</code></pre> <p>Note: This can be added in the root context as a global setting, or in a <code>cdrNode</code> context if you are using multiple CDR Nodes and wish to set this on a per-cdrNode basis.</p> <p>You may then provide your own configuration as described below.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#custom-configuration","title":"Custom Configuration","text":"<p>You can provide any of the following pod allocation configurations:</p> <ul> <li><code>nodeSelector</code></li> <li><code>affinity</code></li> <li><code>nodeName</code></li> <li><code>topologySpreadConstraints</code></li> </ul> <p>As the configuration of pod allocation strategies can get very complicated depending on the architecture, this Helm Chart does not currently perform any validation or auto-configuration (Aside from the default <code>topologySpreadConstraints</code> mentioned above).</p> <p>If you provide a configuration for <code>topologySpreadConstraints</code>, then the defaults will not be used.</p> <p>Any provided configurations are passed through to the <code>podSpec</code> that is generated, without any alterations.</p> <p>Configurations may be provided in the root context as a global setting, or in a <code>cdrNode</code> context if you are using multiple CDR Nodes and wish to set this on a per-cdrNode basis.</p> <p>Warning: Do not use these examples as-is. They are merely to demonstrate how to configure pod allocation strategies.</p>"},{"location":"guide/smilecdr/tuning/podscheduling/#custom-example-1","title":"Custom Example 1","text":"<p>A single CDR Node configuration using a new <code>affinity</code> rule and custom <code>topologySpreadConstraints</code>:</p> <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: topology.kubernetes.io/zone\n          operator: In\n          values:\n          - us-east-1a\n          - us-east-1b\ntopologySpreadConstraints:\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 3\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: DoNotSchedule\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 2\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: DoNotSchedule\n</code></pre> <p>This will result in a deployment which will ensure that pods: * Only run in the <code>us-east-1a</code> and <code>us-east-1b</code> availability zones * Use the provided <code>topologySpreadConstraints</code></p>"},{"location":"guide/smilecdr/tuning/podscheduling/#custom-example-2","title":"Custom Example 2","text":"<p>A multi CDR Node configuration with the following design:</p> <ul> <li>Global settings using a <code>nodeSelector</code> rule to use Spot instances and does not use the default <code>topologySpreadConstraints</code> config.</li> <li>Admin CDR Node using the global defaults.</li> <li>FHIR CDR Node using a different <code>nodeSelector</code> rule to use On Demand instances and does use the default <code>topologySpreadConstraints</code> config.</li> </ul> <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n\n  admin:\n    name: AdminNode\n    enabled: true\n\n  fhir:\n    name: FhirNode\n    enabled: true\n    disableDefaultTopologyConstraints: false\n    nodeSelector:\n      karpenter.sh/capacity-type: on-demand\n\n# GLobal defaults\ndisableDefaultTopologyConstraints: true\n\nnodeSelector:\n  karpenter.sh/capacity-type: spot\n</code></pre> <p>The resulting deployments would have <code>podSpecs</code> that include the following:</p> <p>Admin Node <pre><code>nodeSelector:\n  karpenter.sh/capacity-type: spot\n</code></pre></p> <p>FHIR Node <pre><code>nodeSelector:\n  karpenter.sh/capacity-type: on-demand\ntopologySpreadConstraints:\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n      smilecdr/nodeName: fhir\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: ScheduleAnyway\n- labelSelector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: smilecdr\n      smilecdr/nodeName: fhir\n  matchLabelKeys:\n  - pod-template-hash\n  maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: ScheduleAnyway\n</code></pre></p>"},{"location":"guide/smilecdr/tuning/resources/","title":"Configuring Compute Resources","text":""},{"location":"guide/smilecdr/tuning/resources/#kubernetes-memory-requests-vs-limits","title":"Kubernetes Memory Requests vs Limits","text":"<p><code>resources.requests.memory</code> will be set to the same value as <code>resources.limits.memory</code> unless you override it.</p> <p>The values you should use for CPU resources will depend on the number of cores you are licenced for with Smile CDR.</p> <p>Your total cores can be calculated by:</p> <p><code>replicas * resources.limits.cpu</code></p> <p>If you are using Horizontal Pod Autoscaling then it can be calculated by:</p> <p><code>autoscaling.maxReplicas * resources.limits.cpu</code></p>"},{"location":"guide/smilecdr/tuning/resources/#pod-sizing","title":"Pod Sizing","text":"<p>As Smile CDR is a high performance Java based application, special consideration needs to be given to the resource settings.</p> <p>Typical cloud best practices suggest starting small and increasing resources as workload increases. We have tested Smile CDR in its default module configuration and determined that the max heap size should be no smaller than 2GB. When smaller than this, there are excessive GC events or heapspace errors in the JVM, which is not ideal.</p> <p>When configuring more modules in Smile CDR, it may require more memory/cpu. If you split up the cluster into multiple nodes, then each node may be able to run with less memory/cpu, though total cluster may end up higher depending on your architecture. You will need to analyze resource usage in your configured environment to determine the ideal settings.</p>"},{"location":"guide/smilecdr/tuning/resources/#jvm-auto-configuration","title":"JVM Auto Configuration","text":"<p>This Helm Chart will automatically configure the Smile CDR JVM settings based on the configured resource allocation. See the JVM Configuration for more information on this and how to configure it.</p>"},{"location":"guide/smilecdr/tuning/resources/#storage-configuration","title":"Storage Configuration","text":"<p>Depending on the type of workload, you may need to adjust storage configuration. Please refer to the Storage Configuration for more information.</p>"},{"location":"quickstart/","title":"Deployment Quickstart","text":"<p>This section of the documentation will get you up and running quickly to show how the chart works. For any real deployments, please look through the advanced deplopyments in the User Guide and Examples sections to design a solution that works for your environment.</p>"},{"location":"quickstart/#preparation","title":"Preparation","text":"<p>To deploy Smile CDR using these Helm Charts, you will need to do the following:</p> <ul> <li>Ensure all requirements and dependencies are in place</li> <li>Prepare a set of configurations to suit your planned installation architecture</li> <li>Perform the deployment</li> </ul> <p>The following pages will guide you through the above steps to so that you can gain familiarity with how these Helm Charts function.</p>"},{"location":"quickstart/#advanced-deployment","title":"Advanced Deployment","text":"<p>The Quickstart shows you a basic install that does not follow security best practices. To install with best practices in mind, refer to the advanced configurations in the User Guide section which goes into detail on all available configuration options.</p>"},{"location":"quickstart/helm-repo/","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>In this Quickstart, we will use the native <code>helm</code> command, but you may wish to deploy using alternative tooling in your environment. Please check the User Guide for more info on this.</p>"},{"location":"quickstart/helm-repo/#add-repository","title":"Add repository","text":"<p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh-stable https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"quickstart/install-smilecdr/","title":"Install smilecdr","text":""},{"location":"quickstart/install-smilecdr/#install-the-helm-chart","title":"Install the Helm Chart","text":"<pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre> <p>Smile, we're up and running! :)</p> <p>After about 2-3 minutes, all pods should be in the <code>Running</code> state with <code>1/1</code> containers in the <code>Ready</code> state. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL. You can try re-configuring it using the instructions in the User Guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"quickstart/requirements/","title":"Quickstart Requirements","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Quickstart guide.</p> <p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes addons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment</li> <li>This is used for the Quickstart as it is the easiest way to get up and running without   having to provision an external database and configure credentials and connectivity</li> </ul> </li> <li>Persistent Volume provider that can be used to create <code>PersistentVolume</code> resources for the database</li> </ul>"},{"location":"quickstart/values-file/","title":"Create a Helm values file for your environment","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p>"},{"location":"quickstart/values-file/#a-note-on-creating-values-files","title":"A note on creating values files","text":"<p>Do not copy the default <code>values.yaml</code> file from the Helm Chart, start from a fresh empty file instead.</p> <p>See the section on Values Files Management for more info on this.</p>"},{"location":"quickstart/values-file/#example-values-file","title":"Example Values File","text":"<p>The following example will work in any Kubernetes environment that has the following components installed.</p> <ul> <li>Nginx Ingress</li> <li>CrunchyData PGO</li> <li>A suitable Persistent Volume storage provider (For the database).</li> </ul> <p>You will need to update the values specific to your environment and include credentials for a container repository that contains the Smile CDR Docker images.</p> <p>WARNING: The following method of providing Docker credentials in the values file is insecure and only shown in this quick-start demonstration to show the chart in action. You should instead use an alternative such as an external secret vault. See the secrets section for more info.</p> <p><code>my-values.yaml</code> file <pre><code>specs:\n  hostname: smilecdr.mycompany.com\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: values\n    registry: docker.smilecdr.com\n    username: &lt;DOCKER_USERNAME&gt;\n    password: &lt;DOCKER_PASSWORD&gt;\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre></p>"},{"location":"quickstart-aws/","title":"AWS Quickstart","text":"<p>This section of the documentation shows how to install Smile CDR on Amazon EKS using the Helm Chart.</p>"},{"location":"quickstart-aws/#preparing-dependencies","title":"Preparing Dependencies","text":"<p>When installing Smile CDR securely on Amazon EKS, a number of dependencies need to be in place beforehand. These include, but are not limited to:</p> <ul> <li>Kubernetes cluster</li> <li>Various controllers/operators installed on Kubernetes cluster</li> <li>Various AWS resources such as IAM roles, secrets, S3 buckets, DNS entries etc.</li> </ul>"},{"location":"quickstart-aws/#terraform-module","title":"Terraform Module","text":"<p>To simplify the provisioning of these dependencies, Smile Digital Health provides a Smile CDR Dependencies Terraform module that eases the deployment and management of these dependencies.</p> <p>This quickstart uses some example Terraform configurations that use this module to easily provision required dependencies with minimal manual configurations.</p> <p>Note: If installing Smile CDR without using this Terraform module, there may be a lot more effort required to determine and configure all of the required dependencies.</p>"},{"location":"quickstart-aws/#quickstart-steps","title":"Quickstart Steps","text":"<p>These quickstart instructions are split into 3 main sections:</p>"},{"location":"quickstart-aws/#prepare-eks-cluster","title":"Prepare EKS Cluster","text":"<p>The Prepare EKS Cluster section shows how to install a new EKS cluster or prepare an existing EKS cluster to be ready to install Smile CDR using the Helm Chart.</p> <p>If you already have an EKS cluster ready to go, you may skip this section and move on to the Smile Dependencies section.</p>"},{"location":"quickstart-aws/#prepare-aws-resources","title":"Prepare AWS Resources","text":"<p>The Prepare AWS Resources section details the AWS resources that need to be provisioned in your AWS account in order to deploy Smile CDR using the Helm Chart.</p> <p>Once you have these dependencies in place, you can proceed to deploying Smile CDR using the Terraform module and Helm Chart</p>"},{"location":"quickstart-aws/#deploy-smile-cdr-with-terraform","title":"Deploy Smile CDR with Terraform","text":"<p>The Deploy Smile CDR with Terraform section shows how to deploy Smile CDR via the provided Terraform module.</p> <p>If choosing to deploy directly with the Helm Chart instead, you will first need to deploy the Smile CDR dependency resources into your AWS account through other means. More details on these can be found in the Requirements section of the Smile CDR Helm Chart User Guide.</p> <p>Note: This method is out of scope for this quickstart guide.</p>"},{"location":"quickstart-aws/#whats-next","title":"What's Next?","text":"<p>You may use this quickstart as a basis for your deployment. Please look through the advanced deplopyments in the User Guide and Examples sections to design a solution that works for your architecture.</p>"},{"location":"quickstart-aws/aws-resources/","title":"Prepare AWS Resources","text":"<p>Before deploying Smile CDR using this Terraform Module and Helm Chart, you will still need to create some resources.</p> <p>Due to the nature of these resources, it's not feasible to include them in any automations at this time.</p>"},{"location":"quickstart-aws/aws-resources/#private-container-registry-credentials","title":"Private Container Registry Credentials","text":"<p>As the Smile CDR container images are hosted on a private container registry, credentials are required by Kubernetes so that it can pull images before starting pods. There are multiple options for exposing the Smile CDR images to your EKS cluster.</p> <p>If you use the Smile Digital Health container repository, or if you copy the Smile CDR images to your own password-protected container registry, then you will need to create a <code>dockerconfigjson</code> style secret in AWS Secrets Manager.</p> <p>There are two ways that this can be achieved.</p>"},{"location":"quickstart-aws/aws-resources/#account-wide-shared-repository-secret","title":"Account-wide shared repository secret","text":"<p>If you pre-provision an AWS Secrets Manager secret in the same AWS account, then you can provide the ARN of this secret when you configure your environment in the following section.</p> <p>The Terraform module will automatically add this Secret to the IAM Policies used by the Smile CDR pods.</p>"},{"location":"quickstart-aws/aws-resources/#per-environment-repository-secret","title":"Per environment repository secret","text":"<p>If you do not specify an AWS Secrets Manager Secret ARN, an empty secret will be automatically created in the same region.</p> <p>In this case, you will need to manually update the secret in the AWS Console before the Smile CDR pods will be able to pull the image and start.</p>"},{"location":"quickstart-aws/aws-resources/#repository-secret-format","title":"Repository Secret Format","text":"<p>In either of the above scenarios, the secret should use the standard <code>dockerconfigjson</code> format that is used by Kubernetes. More information on this is available here</p> <p>To create suitable output to paste into the secret, you can run the following command locally:</p> <pre><code>kubectl create secret docker-registry regcred --dry-run=client --docker-server=my.registry.com --docker-username=username --docker-password=password --docker-email=email@example.com -oyaml --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode\n</code></pre> <p>This will output the following, which can be pasted directly into an AWS Secrets Manager secret under the <code>dockerconfigjson</code> key.</p> <pre><code>{\"auths\":{\"my.registry.com\":{\"username\":\"username\",\"password\":\"password\",\"email\":\"email@example.com\",\"auth\":\"dXNlcm5hbWU6cGFzc3dvcmQ=\"}}}\n</code></pre>"},{"location":"quickstart-aws/aws-resources/#amazon-ecr","title":"Amazon ECR","text":"<p>If you choose to copy the Smile CDR images to Amazon ECR, you do not need to create any Secrets Manager secrets as authentication is performed using IAM Policies attached to IAM Roles used by the Pods.</p>"},{"location":"quickstart-aws/aws-resources/#dns-configuration","title":"DNS Configuration","text":"<p>If you wish to have the Terraform module automatically create DNS entries for your new environments, then you will need to have a Route53 Hosted Zone in the same AWS account that you are deploying Smile CDR inside.</p>"},{"location":"quickstart-aws/deploy-terraform/","title":"Deploy Smile CDR with Terraform","text":"<p>Although it's possible to deploy Smile CDR using the Helm Chart, it is advisable to use the provided Terraform module as this will install the required dependencies for you.</p>"},{"location":"quickstart-aws/deploy-terraform/#how-it-works","title":"How It Works","text":"<p>To ease configuration of complex environments, the Smile CDR Dependencies Terraform Module will configure the following, non-Kubernetes dependencies.</p> <ul> <li>RDS database (Optional)<ul> <li>Creates separate users &amp; databases for each module</li> <li>Creates DB connection credentials in AWS Secrets Manager, with secure password handling</li> <li>Configure users to use IAM authentication (Optional)</li> <li>Provides Helm Chart configuration values for the above</li> </ul> </li> <li>Create S3 bucket suitable for staging copied files (Optional)</li> <li>Smile CDR IAM role with least-privilege policies for the above resources</li> <li>Configures IRSA and Helm Chart configuration values</li> <li>AWS Secrets Manager secret for image repository (Optional)</li> <li>Creates Route53 DNS entry (optional)</li> </ul> <p>If using the Helm Chart directly, all of the above will need to be configured beforehand. This is not in scope of this quickstart.</p>"},{"location":"quickstart-aws/deploy-terraform/#pre-requisites","title":"Pre-requisites","text":"<p>It's important to ensure that any pre-requisites are in place before following these steps. If you have not done so already, please review the Prepare EKS Cluster and Prepare AWS Resources sections.</p>"},{"location":"quickstart-aws/deploy-terraform/#container-repository","title":"Container Repository","text":"<p>Review the section on Private Container Registry Credentials and ensure that you either:</p> <ul> <li>Have already created a suitable AWS Secrets Manager secret</li> </ul> <p>or</p> <ul> <li>Are prepared to edit the secret that will be created by this Terraform Module.</li> </ul> <p>Note: The Smile CDR pods will not start up unless a container repository is accessible by the cluster.</p>"},{"location":"quickstart-aws/deploy-terraform/#dns-configuration","title":"DNS configuration","text":"<p>Review the section on DNS Configuration and ensure that you either:</p> <ul> <li>Have access to add DNS entries in the Route53 Hosted Zone for your chosen parent domain</li> </ul> <p>or</p> <ul> <li>Have access to add DNS entries to whichever platform hosts your DNS entries</li> </ul> <p>Note:* Your Smile CDR instance will not be accessible until you create a DNS entry using one of the above mechanisms.</p>"},{"location":"quickstart-aws/deploy-terraform/#minimal-configuration","title":"Minimal Configuration","text":"<p>The minimum required configuration to install Smile CDR using the Terraform module:</p> <pre><code>module \"smile_cdr_dependencies\" {\n  source = \"git::https://gitlab.com/smilecdr-public/smile-dh-helm-charts//src/main/terraform/smile-cdr-deps?ref=terraform-module\"\n  name = \"myDeploymentName\"\n  eks_cluster_name = \"myClusterId\"\n\n  # If you pre-provisoned a shared Container Registry secret, uncomment this line and add the secret's ARN\n  # cdr_regcred_secret_arn = \"arn:aws:secretsmanager:&lt;region&gt;:012345678910:secret:shared/regcred/my.registry.com/username\"\n\n  prod_mode = false\n\n  ingress_config = {\n    public = {\n      parent_domain = \"example.com\"\n\n      # If you are not able to create Route53 DNS entries, then uncomment this line.\n      # You will need to create your DNS entry manually.\n      # route53_create_record = false\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#required-values","title":"Required Values","text":"<p>The only required options are:</p> <ul> <li><code>name</code> - A unique identifier for this environment. This will be used for other resources, (e.g. the Kubernetes <code>namespace</code>), unless overridden elswehere.</li> <li><code>eks_cluster_name</code> - The name of the EKS cluster that was already provisioned.</li> <li><code>ingress_config.public.parent_domain</code> - The parent domain for the public ingress.</li> </ul>"},{"location":"quickstart-aws/deploy-terraform/#default-configuration","title":"Default Configuration","text":"<p>Using the above code snippet, a default install of Smile CDR will be created in the EKS cluster. This will include:</p> <ul> <li>In-cluster Postgres database (Using CrunchyData PGO)</li> <li>Default Smile CDR configuration</li> <li>Single Ingress using the Nginx Ingress controller</li> <li>Single DNS entry in existing Route 53 Hosted Zone<ul> <li>Default HostName will be <code>&lt;name&gt;.&lt;parent_domain&gt;</code>.</li> <li>In the example above, it would be <code>mydeploymentname.example.com</code></li> </ul> </li> <li>Helm Release of Smile CDR</li> </ul>"},{"location":"quickstart-aws/deploy-terraform/#provider-configuration","title":"Provider Configuration","text":"<p>This module requires the following providers to be configured in your Terraform project in order to communicate with the EKS cluster:</p> <pre><code>provider \"aws\" {\n  region = local.region\n}\n\nprovider \"helm\" {\n\n  kubernetes {\n    host                   = module.smile_cdr_dependencies.eks_cluster.endpoint\n    cluster_ca_certificate = base64decode(module.smile_cdr_dependencies.eks_cluster.certificate)\n    token                  = module.smile_cdr_dependencies.eks_cluster.auth_token\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = module.smile_cdr_dependencies.eks_cluster.endpoint\n  cluster_ca_certificate = base64decode(module.smile_cdr_dependencies.eks_cluster.certificate)\n  token                  = module.smile_cdr_dependencies.eks_cluster.auth_token\n}\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#advanced-configurations","title":"Advanced Configurations","text":"<p>The configuration provided so far is not sufficient for a typical install of Smile CDR. A more common pattern would require more configurations to be provided.</p>"},{"location":"quickstart-aws/deploy-terraform/#helm-values-files","title":"Helm Values Files","text":"<p>When deploying Smile CDR using the Helm Chart, configuration is performed by updating the Helm Values file. See the Smile CDR Helm Chart User Guide for more information on how to create and organize your Values files.</p> <p>Once you have prepared your Values files, they can be referenced from this Terraform module using the <code>helm_chart_values</code> configuration. Multiple values files may be referenced, which can greatly help with organising configuration.</p> <pre><code>helm_chart_values = [\n  file(\"helm/smilecdr/values.yaml\"),\n  file(\"helm/smilecdr/feature1.yaml\")\n]\n</code></pre> <p>You can also override values directly from the Terraform module using the <code>helm_chart_values_set_overrides</code> configuration. This is helpful when you want to pass in infrastructure dependent values, rather than having to manually edit the values file separately.</p> <pre><code>helm_chart_values_set_overrides = {\n  \"replicaCount\" = 1\n}\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#terraform-module-helpers","title":"Terraform Module Helpers","text":"<p>This Terraform module also provides some helper configurations to simplify some configurations that would otherwise be troublesome to implement.</p>"},{"location":"quickstart-aws/deploy-terraform/#mapped-files","title":"Mapped Files","text":"<p>When using the Helm Chart Method for Including Extra Files, you would typically pass the files as commandline options like so: <pre><code>helm upgrade -i my-smile-env --devel -f my-values.yaml --set-file mappedFiles.logback\\\\.xml.data=logback.xml smiledh/smilecdr\n</code></pre></p> <p>As you are unable to manipulate the helm command when using the Terraform module, a helper configuration, <code>helm_chart_mapped_files</code>, has been provided to facilitate this functionality.</p> <p>Include files using this method like so: <pre><code>helm_chart_mapped_files = [\n  {\n    name = \"file1.txt\"\n    location = \"classes\"\n    data = file(\"files/classes/file1.txt\")\n  },\n  {\n    name = \"file2.txt\"\n    location = \"cutomerlib\"\n    data = file(\"files/cutomerlib/file2.txt\")\n  }\n]\n</code></pre></p>"},{"location":"quickstart-aws/deploy-terraform/#further-configuration","title":"Further Configuration","text":"<p>For further configuration options and examples, please refer to the Smile CDR Dependencies Terraform Module and Smile CDR Helm Chart User Guide sections.</p>"},{"location":"quickstart-aws/deploy-terraform/#terraform-quickstart-project","title":"Terraform 'Quickstart' Project","text":"<p>A Terraform project is provided in the examples section of the Smile CDR Helm Chart repository. It brings together all of the concepts mentioned above and can be used as a starting point to deploy Smile CDR.</p>"},{"location":"quickstart-aws/deploy-terraform/#download-terraform-quickstart-project","title":"Download Terraform Quickstart Project","text":"<p>In a terminal, change to a suitable folder to manage your project. <pre><code>mkdir -p ~/my-sdh-eks/\ncd ~/my-sdh-eks\n</code></pre></p> <p>Clone the Terraform Quickstart Project if you have not already done so.</p> <p>Note: If you followed the Prepare EKS Cluster section, you should have already completed this step.</p> <pre><code>git clone --depth 1 https://gitlab.com/smilecdr-public/smile-dh-helm-charts.git\n</code></pre> <p>Optionally make a copy of the project to work from <pre><code>cp -rp smile-dh-helm-charts/examples/terraform/workload/quickstart workload\ncd workload\n</code></pre></p>"},{"location":"quickstart-aws/deploy-terraform/#configure-the-project-for-your-environment","title":"Configure the project for your environment","text":"<p>Due to the pre-requisites, this project will NOT run without modification, you should update some of the Terraform <code>locals</code> to suit your environment.</p> <p>At a minimum, you MUST should configure the following:</p> <ul> <li><code>cdr_regcred_secret_arn</code> - The ARN for a pre-provisioned secret for the Container Registry. If not specified, then a blank secret will be created automatically.</li> <li><code>parent_domain</code> - Set this to the subdomain where you will create your DNS entry.</li> <li><code>route53_create_record</code> - Set this to <code>false</code> if you are not able to create Route53 DNS entries. You will need to create your DNS record using another mechanism.</li> </ul> <p>You should also consider updating the following, as they are based on the default cluster that was deployed in the Prepare EKS Cluster section.</p> <ul> <li><code>name</code> - A unique name that will be used for your Smile CDR deployment and any supporting resources (Default is <code>MyDeploymentName</code>)</li> <li><code>eks_cluster_name</code> - The name/id of the EKS cluster. (Default is <code>MyClusterName</code>)</li> <li><code>region</code> - The AWS region where you wish to deploy Smile CDR. This must be the same region that you deployed the EKS cluster. (Default is <code>us-east-1</code>)</li> </ul> <p>Note: It's advisable at this point to configure your Terraform remote state. For this guide, we will continue to use local state.</p> <p>Edit the <code>main.tf</code> file. At the top of the file, you will see the following <code>locals</code> block that you should update based on the above.</p> <pre><code>locals {\n    name = \"MyDeploymentName\"\n    eks_cluster_name = \"MyClusterName\"\n    cdr_regcred_secret_arn = null\n    parent_domain = \"example.com\"\n    # If you are not able to create Route53 DNS entries, then set to false\n    # You will then need to create your DNS entry manually.\n    route53_create_record = true\n    region=\"us-east-1\"\n}\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#prepare-terraform-project","title":"Prepare Terraform Project","text":"<p>Make sure that you have valid AWS credentials loaded and that you are able to authenticate against the AWS API.</p> <p><pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROAXAABBCCDDEEFFGG\",\n    \"Account\": \"012345678910\",\n    \"Arn\": \"arn:aws:sts::012345678910:role/MyAdminRole\"\n}\n</code></pre> Double check that you are using the correct AWS account and have a suitable IAM role/user that has Administrative privileges.</p> <p>Initialize the Terraform Project <pre><code>terraform init\n</code></pre></p> <p>After all of the Terraform modules have been installed, you should see the following message: <pre><code>Terraform has been successfully initialized!\n</code></pre></p>"},{"location":"quickstart-aws/deploy-terraform/#deploy-smile-cdr","title":"Deploy Smile CDR","text":"<p>Now you can plan, review and apply the Terraform project to create the Smile CDR environment and the required components.</p> <pre><code>terraform plan\n</code></pre> <p>Review the output to see what resources will be created.</p> <pre><code>terraform apply\n\nPlan: 9 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n</code></pre> <p>NOTE: If you did not provide an existing Container Registry credentials secret, you will need to manually update the secret that was created in AWS Secrets Manager after running <code>terraform apply</code>. If you do not do this, then the <code>apply</code> operation will not complete, as the Smile CDR pods will not come up until the credentials are available.</p> <p>Once completed, you should see output as follows:</p> <pre><code>Apply complete! Resources: 9 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nhelm_release_notes = &lt;&lt;EOT\n\n***************************\n**** NO CHART WARNINGS ****\n***************************\n\nThank you for installing Smile CDR by Smile Digital Health!\n\n                              ,.,;pppppQQQQQQQQppppppppppppp;,..\n                      .s#################################SlSSS#######pp,\n                 ;s######################################SlSSSSSSSS#######Np.\n              ;###Sl#####################################SlSSSSSSSS###########p.\n            ;#SGSSSl#####################################SlSSSSSSSS#############Qp\n          .SGGGGSSSl#####################################SlSSSSSSSS###############p\n         .lGGGGGSSSl#####################################SlSSSSSSSS###############bp\n         lSGGGGGSSSl#####################################SlSSSSSSSS###############bG\n         lGGGGGGSSSl#####################################SlSSSSSSSS###############bC\n         lGGGGSSS###########################################SSSSSSS###############bC\n        :GGSG#Tb\"\"^\"\"\"T88#####$8$$8888888##############$TTG$S#SSSSS#######$TT8@###b\n        ISGb^             ?@#               ^\"6@######p    !@#SSSSS#####b     @###b\n        $G      .;ppp;,.  ;#b     ,,,,,,,.      '8@###b    !$#SSSSS#####b     @###b\n       'G     ;##S$$$#######b     @#########p.    '8##p    !$#SSSSS#####b     @##b|\n       :G     8$#SSl########b     @###########N     8#b    !$#SSSSSS####b     @##b\n       !Sp     \"6@S#########b     @############b    '@p    l$##SSS######N     @##b\n       GG$p.      ^\"T8@#####b     @############N     GC     ^^^^^^^^^^^^^     @##b\n       GGGGSSp         '8@##b     @############b     GC                       @#bb\n      'GGGGGGGS##Sp       7@#     @############b     $C    ;#############     @#b\n      !GGGGGGGGGS$$##N.    l$     @############b    '@p    !@#SSSS$$$###b     @#b\n      GGGSGGGGGGSSSS$#N    '$     @###########b     $#b    !$#SSSSS#####b     @#b\n      GGGTT8$GSSS#####b    l#     @########b^     ,###b    !$#SSSSS#####b     @#~\n     !GGG     '^^^^^      ;##     '\"*\"^^'       ,#####b    !$#SSSSS#####b     $G\n     !GGG.             ,s###b               ,s########~    j@#SSSSS#####b     $G\n     !GGG$#SQppppppQ###################################QQQQ##SSSSSS#######QQQ##C\n     GGGGGGGGGGGGSS$$$##################################$$SSSSSSSSS########$##b\n    oGGGGGGGGGGGSSSl#####################################SlSSSSSSSS###########b\n    ^GGGGGGGGGGGSSSl#####################################SlSSSSSSSS##########bb\n     ?GGGGGGGGGGSSSl#####################################SlSSSSSSSS##########b\n      *GGGGGGGGGSSSl#####################################SlSSSSSSSS#########b\n        ?8GGGGGGSSSl#####################################SlSSSSSSSS#######b\n          '?GGGGSSSl#####################################SlSSSSSSSS###bb^\n              ^\"G8$$######################################llSll$GG\"*^\n                    ^^7T888888888888888#####88888888888TTT\"\"^^\n\nSmile, we're up and running! :)\n\n  You can access your Smile CDR instance at:\n\n  https://mydeploymentname.example.com/\n\nEOT\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#verify-helm-deployment","title":"Verify Helm Deployment","text":"<p>The Smile CDR Helm Chart provides some configuration feedback after installing. This feedback provides warnings and information about incorrect or deprecated configurations that may need to be updated.</p> <p>The provided example includes this output as can be seen above. It's vital that you review this output during installs and upgrades so that you can pre-emptively avoid any disruptions due to future configuration schema changes.</p> <p>If you do not include the above output section, or if you are unable to review your Terraform output, then you can manually check the Helm Chart notes as follows</p> <pre><code>helm get notes smilecdr\n</code></pre>"},{"location":"quickstart-aws/deploy-terraform/#destroy-environment","title":"Destroy Environment","text":"<p>You can destroy this environment and all related resources like so:</p> <pre><code>terraform destroy\n</code></pre> <p>Warning!: If you have deployed any in-cluster stateful resources, such as a Postgres cluster or Strimzi Kafka cluster, they will also be destroyed using this command. Data will be irreversibly lost unless you have configured backups.</p>"},{"location":"quickstart-aws/eks-cluster/","title":"Prepare EKS Cluster","text":"<p>To simplify creation of a suitable EKS cluster, this guide uses a simple Terraform project that leverages the Amazon EKS Blueprints for Terraform patterns.</p> <p>These patterns provide a well-curated baseline EKS configuration that can be customized based on the requirements for your environment.</p>"},{"location":"quickstart-aws/eks-cluster/#terraform-quickstart-project","title":"Terraform 'Quickstart' Project","text":"<p>A Terraform project is provided in the examples section of the Smile CDR Helm Chart repository. It can be used to create a complete EKS cluster that contains all of the required components to use the Smile CDR Helm Chart.</p>"},{"location":"quickstart-aws/eks-cluster/#included-components","title":"Included Components","text":"<p>The main components that are included in this Terraform Quickstart Project are as follows:</p> <ul> <li>EKS Cluster using best-practice defaults (e.g. etcd encrypted by default)</li> <li>Karpenter for provisioning compute resources on-demand, rather than pre-provisioning worker nodes. Can take advantage of Spot instances and is more granular and efficient than Autoscaling Groups. More Info</li> <li>AWS Load Balancer Controller for managing AWS Load Balancer. More Info</li> <li>Nginx Ingress Controller for managing Ingress resources. More Info</li> <li>Secrets Store CSI for securely managing Secrets (i.e. DB credentials). More Info</li> <li>CrunchyData Postgres Operator for managing in-cluster Postgres databases. More Info</li> <li>Strimzi Kafka Controller for managing in-cluster Kafka clusters. More Info</li> <li>Other supporting components that are not relevant to this guide.</li> </ul>"},{"location":"quickstart-aws/eks-cluster/#quickstart-variants","title":"Quickstart variants","text":"<p>This Terraform project is available in three variants to help satisfy common use-cases.</p> <ul> <li><code>karpenter</code> - Configures all of the above components, plus a new VPC.</li> <li><code>karpenter-fargate</code> - Same as above, but uses Fargate for the core cluster components to save cost under certain scenarios</li> <li><code>karpenter-novpc</code> - Same as the <code>karpenter</code> option above, but can be deployed to an existing VPC. Useful if you do not wish to create a VPC.</li> </ul>"},{"location":"quickstart-aws/eks-cluster/#terraform-state-management","title":"Terraform State Management","text":"<p>By default, this Terraform Quickstart Project uses a local Terraform state file.</p> <p>It's highly recommended to use a centrally managed remote state if you have one already available in your environment. If you do not have one and wish to configure one in the same AWS account that you plan to deploy to, then you can use the provided CDK project to provision an S3 bucket and DynamoDB table suitable for Terraform remote state management. This can be found in the state-s3 section of the Smile CDR Helm Chart repository.</p>"},{"location":"quickstart-aws/eks-cluster/#deployment-steps","title":"Deployment Steps","text":"<p>Let's follow the <code>karpenter</code> variant of the Terraform Quickstart Project...</p>"},{"location":"quickstart-aws/eks-cluster/#download-terraform-quickstart-project","title":"Download Terraform Quickstart Project","text":"<p>In a terminal, change to a suitable folder to manage your project. <pre><code>mkdir -p ~/my-sdh-eks/\ncd ~/my-sdh-eks\n</code></pre></p> <p>Clone the Terraform Quickstart Project <pre><code>git clone --depth 1 https://gitlab.com/smilecdr-public/smile-dh-helm-charts.git\n</code></pre></p> <p>Optionally make a copy of the project to work from <pre><code>cp -rp smile-dh-helm-charts/examples/terraform/cluster/complete/karpenter cluster\ncd cluster\n</code></pre></p>"},{"location":"quickstart-aws/eks-cluster/#configure-the-project-for-your-environment","title":"Configure the project for your environment","text":"<p>Although this project will run without modification, you should update some of the Terraform <code>locals</code> to suit your environment.</p> <p>At a minimum, you should configure the following:</p> <ul> <li><code>name</code> - A unique name that will be used for your EKS cluster and any supporting resources (Default is <code>MyClusterName</code>)</li> <li><code>region</code> - The AWS region where you wish to deploy the EKS cluster. (Default is <code>us-east-1</code>)</li> <li><code>acm_cert_arn</code> - The ARN for a default ACM certificate that will be used by the AWS Network Load Balancer that will be used by the Nginx Ingress Controller.</li> </ul> <p>Note: It's advisable at this point to configure your Terraform remote state. For this guide, we will continue to use local state.</p> <p>Edit the <code>main.tf</code> file. At the top of the file, you will see the following <code>locals</code> block that you should update.</p> <pre><code>locals {\n  name   = \"MyClusterName\"\n  region = \"us-east-1\"\n  acm_cert_arn = \"arn:aws:acm:us-east-1:012345678910:certificate/xxxx-yyyy-zzzz\"\n}\n</code></pre>"},{"location":"quickstart-aws/eks-cluster/#prepare-terraform-project","title":"Prepare Terraform Project","text":"<p>Make sure that you have valid AWS credentials loaded and that you are able to authenticate against the AWS API.</p> <p><pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROAXAABBCCDDEEFFGG\",\n    \"Account\": \"012345678910\",\n    \"Arn\": \"arn:aws:sts::012345678910:role/MyAdminRole\"\n}\n</code></pre> Double check that you are using the correct AWS account and have a suitable IAM role/user that has Administrative privileges.</p> <p>Initialize the Terraform Project <pre><code>terraform init\n</code></pre></p> <p>After all of the Terraform modules have been installed, you should see the following message: <pre><code>Terraform has been successfully initialized!\n</code></pre></p>"},{"location":"quickstart-aws/eks-cluster/#deploy-cluster","title":"Deploy Cluster","text":"<p>Now you can plan, review and apply the Terraform project to create the EKS cluster and the required components.</p> <pre><code>terraform plan\n# Review the output to see what resources will be created.\n\nterraform apply\n\nPlan: 110 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + configure_kubectl = \"aws eks --region us-east-1 update-kubeconfig --name MyClusterName\"\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n</code></pre> <p>Now is the time to grab that cuppa, as this process will take about 45 mins!</p> <p>Once completed, you should see output as follows:</p> <pre><code>Apply complete! Resources: 11 added, 0 changed, 2 destroyed.\n\nOutputs:\n\nconfigure_kubectl = \"aws eks --region us-east-1 update-kubeconfig --name MyClusterName\"\n</code></pre>"},{"location":"quickstart-aws/eks-cluster/#verify-eks-cluster","title":"Verify EKS Cluster","text":"<p>Now you can add the cluster to your local <code>kubectl</code> configuration by running the <code>configure_kubectl</code> that is returned.</p> <pre><code>aws eks --region us-east-1 update-kubeconfig --name MyClusterName\nAdded new context arn:aws:eks:us-east-1:012345678910:cluster/MyClusterName to ~/.kube/config\n</code></pre> <p>Using this context, you should now be able to inspect the cluster and view all of the core component pods.</p>"},{"location":"quickstart-aws/eks-cluster/#deploy-workloads","title":"Deploy Workloads","text":"<p>Your cluster is now ready and applications can be deployed.</p> <p>Before installing Smile CDR using the Helm Chart, you may still need to perform further configurations in your AWS account. Proceed to the Prepare AWS Resources section.</p>"},{"location":"quickstart-aws/eks-cluster/#destroy-eks-cluster","title":"Destroy EKS Cluster","text":"<p>When deleting this cluster, it's important to destroy the resources in a specific order. Failure to do this will likely leave the Terraform project in a 'stuck' state where it is unable to delete resources without tedious manual steps.</p> <ol> <li>Delete any application workloads or operators deployed on the cluster    <pre><code>terraform destroy -target module.eks_blueprints_addon_crunchypgo\nterraform destroy -target module.eks_blueprints_addon_strimzi\nterraform destroy -target module.eks_blueprints_addon_karpenter_provisioner_config\n</code></pre></li> <li>Destroy any ingress addons    <pre><code>terraform destroy -target module.eks_blueprints_addons_ingress\n</code></pre></li> <li>Destroy the core EKS Blueprint Addons    <pre><code>terraform destroy -target module.eks_blueprints_addons_core\n</code></pre></li> <li>Destroy the core EKS module    <pre><code>terraform destroy -target module.eks\n</code></pre></li> <li>Destroy the remaining resources    <pre><code>terraform destroy\n</code></pre></li> </ol>"},{"location":"quickstart-aws/helm-repo/","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>In this Quickstart, we will use the native <code>helm</code> command, but you may wish to deploy using alternative tooling in your environment. Please check the User Guide for more info on this.</p>"},{"location":"quickstart-aws/helm-repo/#add-repository","title":"Add repository","text":"<p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh-stable https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"quickstart-aws/install-smilecdr/","title":"Install smilecdr","text":""},{"location":"quickstart-aws/install-smilecdr/#install-the-helm-chart","title":"Install the Helm Chart","text":"<pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre> <p>Smile, we're up and running! :)</p> <p>After about 2-3 minutes, all pods should be in the <code>Running</code> state with <code>1/1</code> containers in the <code>Ready</code> state. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL. You can try re-configuring it using the instructions in the User Guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"quickstart-aws/requirements/","title":"Quickstart Requirements","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Quickstart guide.</p> <p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes addons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment</li> <li>This is used for the Quickstart as it is the easiest way to get up and running without   having to provision an external database and configure credentials and connectivity</li> </ul> </li> <li>Persistent Volume provider that can be used to create <code>PersistentVolume</code> resources for the database</li> </ul>"},{"location":"quickstart-aws/values-file/","title":"Create a Helm values file for your environment","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p>"},{"location":"quickstart-aws/values-file/#a-note-on-creating-values-files","title":"A note on creating values files","text":"<p>Do not copy the default <code>values.yaml</code> file from the Helm Chart, start from a fresh empty file instead.</p> <p>See the section on Values Files Management for more info on this.</p>"},{"location":"quickstart-aws/values-file/#example-values-file","title":"Example Values File","text":"<p>The following example will work in any Kubernetes environment that has the following components installed.</p> <ul> <li>Nginx Ingress</li> <li>CrunchyData PGO</li> <li>A suitable Persistent Volume storage provider (For the database).</li> </ul> <p>You will need to update the values specific to your environment and include credentials for a container repository that contains the Smile CDR Docker images.</p> <p>WARNING: The following method of providing Docker credentials in the values file is insecure and only shown in this quick-start demonstration to show the chart in action. You should instead use an alternative such as an external secret vault. See the secrets section for more info.</p> <p><code>my-values.yaml</code> file <pre><code>specs:\n  hostname: smilecdr.mycompany.com\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: values\n    registry: docker.smilecdr.com\n    username: &lt;DOCKER_USERNAME&gt;\n    password: &lt;DOCKER_PASSWORD&gt;\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre></p>"},{"location":"terraform/smilecdrdeps/","title":"Index","text":""},{"location":"terraform/smilecdrdeps/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.7 aws &gt;= 5.35.0 helm &gt;= 2.12.1 kubernetes &gt;= 2.25.0"},{"location":"terraform/smilecdrdeps/#providers","title":"Providers","text":"Name Version aws &gt;= 5.35.0 helm &gt;= 2.12.1 kubernetes &gt;= 2.25.0 random n/a"},{"location":"terraform/smilecdrdeps/#inputs","title":"Inputs","text":"Name Description Type Default Required aws_region Region of the EKS cluster <code>string</code> <code>null</code> no cdr_license_secret_arn ARN for AWS Secret for Smile CDR License. If not specified, a secret will be created <code>string</code> <code>null</code> no cdr_license_secret_kms_arn ARN for KMS key for existing AWS Secrets Manager secret for Smile CDR License. <code>string</code> <code>null</code> no cdr_license_secret_name Name to use when creating secret for Smile CDR License <code>string</code> <code>\"license\"</code> no cdr_license_secret_name_override Set true to prevent random suffix on Smile CDR License secret name <code>bool</code> <code>false</code> no cdr_regcred_secret_arn ARN for existing AWS Secrets Manager secret for registry credentials. If not specified, a secret will be created <code>string</code> <code>null</code> no cdr_regcred_secret_kms_arn ARN for KMS key for existing AWS Secrets Manager secret for registry credentials. <code>string</code> <code>null</code> no cdr_regcred_secret_name Name to use when creating secret for Docker pull <code>string</code> <code>\"regcred\"</code> no cdr_regcred_secret_name_override Set true to prevent random suffix on Docker pull secret name <code>bool</code> <code>false</code> no cdr_service_account_name Override auto-generated service account name <code>string</code> <code>null</code> no create_copyfiles_bucket Enable creation of S3 bucket suitable for CopyFiles function <code>bool</code> <code>false</code> no crunchy_pgo_config CrunchyData PGO backup &amp; restore configuration <pre>object({    enabled                       = optional(bool)    helm_autoconf                 = optional(bool,true)    pgbackrest                    = optional(object({      enabled                     = optional(bool,false)      volume                      = optional(object({        backupsSize               = optional(string,\"1Gi\")        retention_full            = optional(number,3)        retention_incremental     = optional(number)        retention_differential    = optional(number)        manual_backup             = optional(string)      }))      s3                          = optional(object({        enabled                   = optional(bool,false)        use_existing_bucket       = optional(bool,false)        bucket_name               = optional(string)        bucket_name_prefix        = optional(string)        bucket_prefix             = optional(string,\"pgbackrest\")        reponumber                = optional(number,2)        reponame                  = optional(string)        retain_on_destroy         = optional(bool,false)        retention_full            = optional(number,10)        retention_incremental     = optional(number)        retention_differential    = optional(number)        manual_backup             = optional(string)      }),{enabled=false})      schedules                   = optional(object({        full                      = optional(string)        incremental               = optional(string)        differential              = optional(string)      }))    }))    restore                       = optional(object({      enabled                     = optional(bool,false)      source                      = optional(string)      type                        = optional(string,\"time\")      restore_time                = optional(string)    }))    datasource                    = optional(any)  })</pre> <pre>{  \"pgbackrest\": {    \"enabled\": false  }}</pre> no db_instance_defaults Default configuration for databases. <pre>object({    db_subnet_discovery_tags      = optional(map(string))    db_subnet_discovery_enabled   = optional(bool,true)    db_subnet_ids                 = optional(list(string))    db_subnet_group_name          = optional(string)    engine                        = optional(string,\"aurora-postgresql-serverless-v2\")    enable_cdr_rds_secrets        = optional(bool,true)    publicly_accessible           = optional(bool,false)  })</pre> <pre>{  \"enable_cdr_rds_secrets\": true,  \"engine\": \"aurora-postgresql-serverless-v2\"}</pre> no db_instances n/a <pre>list(object({    name                          = string    db_subnet_discovery_tags      = optional(map(string))    db_subnet_discovery_enabled   = optional(bool)    db_subnet_ids                 = optional(list(string))    db_subnet_group_name          = optional(string)    engine                        = optional(string)    enable_cdr_rds_secrets        = optional(bool)    name_suffix                   = optional(string)    name_override                 = optional(string)    publicly_accessible           = optional(bool)    public_cidr_blocks            = optional(list(string))    rds_kms_arn                   = optional(string)    kms_deletion_window           = optional(number)    secrets_kms_key_arn           = optional(string)    master_username               = optional(string)    manage_master_user_password   = optional(bool)    dbname                        = optional(string)    dbport                        = optional(number)    default_auth_type             = optional(string)  }))</pre> <code>[]</code> no db_subnet_ids Subnet IDs where the Database will be located. If this is left null then an existing db_seubnet_group MUST be provided with db_subnet_group_name. <code>list(string)</code> <code>[]</code> no db_use_old_helm_schema Subnet IDs where the Database will be located. If this is left null then an existing db_seubnet_group MUST be provided with db_subnet_group_name. <code>bool</code> <code>true</code> no db_users n/a <pre>list(object({    db_instance_name              = string    name                          = string    cdr_modules                   = optional(list(string))    dbusername                    = optional(string)    dbname                        = optional(string)    auth_type                     = optional(string,\"password\")  }))</pre> <code>[]</code> no db_vpc_id ID of VPC to create Database in. Uses the same VPC as the EKS cluster by default <code>string</code> <code>null</code> no eks_cluster_ca_certificate Override auto-detected EKS Cluster CA Certificate. This must be a base64 encoded certificate. <code>string</code> <code>null</code> no eks_cluster_endpoint Override auto-detected EKS Cluster endpoint <code>string</code> <code>null</code> no eks_cluster_name Name of the EKS cluster to install Smile CDR <code>string</code> n/a yes eks_cluster_oidc_provider_arn Override auto-detected EKS OIDC Provider ARN e.g., arn:aws:iam:::oidc-provider/ <code>string</code> <code>null</code> no eks_cluster_oidc_provider_url Override auto-detected EKS OIDC Provider URL e.g., https://oidc.eks.&lt;region.amazonaws.com/id/ <code>string</code> <code>null</code> no eks_cluster_security_group_id Security group ID for EKS Cluster <code>string</code> <code>null</code> no eks_cluster_version Override auto-detected EKS Cluster version <code>string</code> <code>null</code> no enable_cdr_license_secret Use AWS Secrets Manager for Smile CDR License Secret <code>bool</code> <code>false</code> no enable_cdr_rds_secrets Use AWS Secrets Manager for RDS Secrets <code>bool</code> <code>true</code> no enable_cdr_regcred_secret Disable handling of secrets for registry credentials <code>bool</code> <code>true</code> no enable_irsa Override auto-detected EKS Cluster version <code>bool</code> <code>null</code> no extra_iam_policies n/a <code>map(map(any))</code> <code>{}</code> no extra_secrets n/a <code>list(any)</code> <code>[]</code> no helm_chart The name of the Smile CDR Helm Chart in the repository <code>string</code> <code>\"smilecdr\"</code> no helm_chart_mapped_files List of files &amp; data to include in the classes &amp; customerlib directories. <pre>list(object(    {      name     = string      data     = string      location = string    }  ))</pre> <code>[]</code> no helm_chart_values List of raw yaml values files to pass in to the Helm Chart. These will be merged into the final Helm Values. <code>list(string)</code> <code>[]</code> no helm_chart_values_set_overrides Individual values overrides to pass in to the Helm Chart. Each item in the provided map is equivalent to using the --set option with the helm command. <code>map(string)</code> <code>{}</code> no helm_chart_version The version of the Smile CDR Helm Chart to use <code>string</code> <code>\"1.0.0-pre.110\"</code> no helm_deploy Deploy using the Smile CDR Helm Chart <code>bool</code> <code>true</code> no helm_release_name The release name used in the Smile CDR Helm Chart <code>string</code> <code>\"smilecdr\"</code> no helm_repository The Helm Repo where the Smile CDR Helm Chart is hosted <code>string</code> <code>\"https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\"</code> no helm_service_account_suffix The suffix used for ServiceAccount in the Smile CDR Helm Chart <code>string</code> <code>\"-smilecdr\"</code> no ingress_config Configuration for DNS <pre>map(object(    {      hostname                = optional(string,\"\")      parent_domain           = optional(string,\"\")      route53_create_record   = optional(bool,true)      route53_zone_name       = optional(string)    }  ))</pre> <code>{}</code> no kms_deletion_window Deletion Window for KMS. Set from 7 to 30 <code>number</code> <code>7</code> no mock_data_aws_eks_cluster Mock unit-testing data for aws_eks_cluster data source <pre>object(    {      arn                               = optional(string)      access_config                     = optional(list(object(        {          authentication_mode           = string        }      )))      certificate_authority             = optional(list(object(        {          data                          = string        }      )))      created_at                        = optional(string)      enabled_cluster_log_types         = optional(set(string))      endpoint                          = optional(string)      id                                = optional(string)      cluster_id                        = optional(string)      identity                          = optional(list(object(        {          oidc=list(object(            {              issuer                    = string            }          ))        }      )))      kubernetes_network_config         = optional(list(object(        {          ip_family                     = string          service_ipv4_cidr             = string          service_ipv6_cidr             = string        }      )))      name                              = optional(string)      outpost_config                    = optional(list(object(        {          control_plane_instance_type   = string          control_plane_placement       = list(object(            {              group_name                = string            }          ))          outpost_arns                  = set(string)        }      )))      platform_version                  = optional(string)      role_arn                          = optional(string)      status                            = optional(string)      version                           = optional(string)      vpc_config                        = optional(list(object(        {          cluster_security_group_id     = string          endpoint_private_access       = optional(bool)          endpoint_public_access        = optional(bool)          public_access_cidrs           = optional(set(string))          security_group_ids            = optional(set(string))          subnet_ids                    = optional(set(string))          vpc_id                        = string        }      )))      tags                              = optional(object({}))    }  )</pre> <code>{}</code> no mock_data_aws_iam_openid_connect_provider Mock unit-testing data for aws_iam_openid_connect_provider data source <pre>object(    {      arn                               = optional(string)      client_id_list                    = optional(list(string))      thumbprint_list                   = optional(list(string))      id                                = optional(string)      url                               = optional(string)      tags                              = optional(map(string))    }  )</pre> <code>{}</code> no name Unique identifier for this Smile CDR instance <code>string</code> n/a yes namespace The Namespace this will be deployed in <code>string</code> <code>null</code> no prod_mode Sets some sane defaults for environments being deployed into production <code>bool</code> <code>true</code> no rds_kms_arn ARN for KMS key used for RDS. If not specified, one will be created <code>string</code> <code>null</code> no resourcenames_suffix Set suffix on generated resource names. Will generate random suffix if not set. <code>string</code> <code>null</code> no s3_read_buckets n/a <code>list(any)</code> <code>[]</code> no s3_write_buckets n/a <code>list(any)</code> <code>[]</code> no secrets_deletion_window Deletion Window for Secrets. Set to 0 for dev environments, or set from 7 to 30 <code>number</code> <code>7</code> no secrets_kms_key_arn ARN for KMS key used for Secrets. If not specified, one will be created <code>string</code> <code>null</code> no tags Tags to add to infrastructure resources <code>map(string)</code> <code>{}</code> no unit_testing Use mock_* vars instead of data sources for unit testing <code>bool</code> <code>false</code> no"},{"location":"terraform/smilecdrdeps/#outputs","title":"Outputs","text":"Name Description eks_cluster n/a helm_sa_annotation n/a helm_secret_configs n/a iam_users n/a name n/a secrets_cdr_regcred n/a secrets_kms_key n/a url n/a"},{"location":"upgrading/","title":"Upgrading the Helm Chart and Smile CDR","text":""},{"location":"upgrading/#upgrading-from-pre-release-versions","title":"Upgrading From Pre-Release Versions","text":"<p>If you have installed Smile CDR using the <code>1.0.0-pre.x</code> pre-release versions of the Helm Chart, then you will need to adjust your deployment process to use the <code>stable</code> release channel before you can use the officially supported versions.</p> <p>Please refer to the Release Channels section for more information on switching release channels.</p>"},{"location":"upgrading/#choosing-helm-chart-versions","title":"Choosing Helm Chart Versions","text":"<p>As these Helm Charts follow SemVer 2.0, the version format is <code>MAJOR.MINOR.PATCH</code> with an optional suffix depending on the release channel being used (e.g. pre release (<code>-pre.n</code>) or next major release (<code>-next.n</code>))</p> <p>Refer to the Versioning Strategy page for more information.</p>"},{"location":"upgrading/#choosing-smile-cdr-versions","title":"Choosing Smile CDR Versions","text":"<p>Although each version of the Helm Chart will default to a specific version of Smile CDR, it is often desirable to explicitly define the Smile CDR version so that you can be in more direct control of any upgrades that take place.</p> <p>Note</p> <p>Any version of Smile CDR can be specified as long as it falls within the range of supported versions for the Helm Chart version you are using. See the version compatibility section and the version matrix.</p> <p>You can override the version of Smile CDR by adjusting the <code>image.tag</code> in your Helm values file. i.e:</p> <pre><code>image:\n  tag: \"2024.08.R01\"\n</code></pre>"},{"location":"upgrading/#smile-cdr-version-compatibility","title":"Smile CDR Version Compatibility","text":"<p>In general, when installing a current version of Smile CDR, we recommend you use the latest version of the Helm Chart. Each new version of the Helm Chart will support the previous 4 major versions of Smile CDR. e.g:</p> Helm Chart Version Default Smile CDR Version Oldest Supported Smile CDR Version Notes v2.0.0 <code>2024.08.R01</code> <code>2023.08.R10</code> Not Yet Released v1.0.0 <code>2024.05.R03</code> <code>2023.05.R03</code> <p>Note</p> <p>When explicitly specifying a Smile CDR version, please refer to the full version matrix to ensure compatibility for the version combination you require.</p>"},{"location":"upgrading/#using-pre-release-versions-of-smile-cdr","title":"Using Pre Release Versions of Smile CDR","text":"<p>On occasion, you may be working directly with the Smile Digital Health support teams to deploy or test Pre-Release versions of Smile CDR.</p> <p>Due to unforeseen changes in future versions of Smile CDR, there may be compatibility issues with the current version of the Helm Chart.</p> <p>When such situations occur, you may need to switch to another release channel to use a pre release or beta version of the Helm Chart that will work with the Pre-Release version of Smile CDR that you are attempting to use.</p> <p>Typically, if the current Helm Chart version is <code>2.0.0-next-major.1</code> (Which uses Smile CDR <code>2024.08.R01</code>), then the following versions would be available in other release channels.</p> Helm Chart Version Smile CDR Pre-Release Version <code>3.0.0-beta.1</code> <code>2024.11.PRE-1</code> <code>4.0.0-beta.1</code> <code>2025.02.PRE-1</code> <p>Note</p> <p>Refer to the prerelease sections of the version matrix to see if there is a suitable Helm Chart version available.</p> <p>Refer to Release Channels for more information on selecting and switching release channels.</p>"},{"location":"upgrading/release-channels/","title":"Release Channels","text":"<p>The Smile CDR Helm Charts are available in a number of release channels.</p> Release Type Channel Name Helm Chart Repo Official <code>stable</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/stable v1.x prereleases <code>devel</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/devel v2.x and up prereleases <code>pre</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/pre Next version prereleases <code>next</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/next Beta releases <code>beta</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/beta Alpha releases <code>alpha</code> https://gitlab.com/api/v4/projects/40759898/packages/helm/alpha <p>Note</p> <p>The <code>devel</code> release channel has been deprecated and will no longer be used going forwards.</p> <p>This was only used prior to the initial release of version <code>1.0.0</code> of the Helm Chart</p> <p>We recommend using the <code>stable</code> channel so that you can deploy officially supported releases of the Smile CDR Helm Chart.</p> <p>If you wish to preview upcoming features that are not yet available in a currently supported Helm Chart version in the <code>stable</code> channel, then you may use prerelease versions from another release channel.</p> <p>Warning</p> <p>When using release channels other than <code>stable</code>, there may be unexpected breaking changes or regressions between pre release or beta versions.</p>"},{"location":"upgrading/release-channels/#configuring-release-channels","title":"Configuring Release Channels","text":"<p>The mechanism for configuring the release channel depends on which tooling you are using to deploy the Helm Chart.</p> Using the <code>helm</code> CommandUsing the <code>helm</code> Terraform Provider <p>If you are deploying using the <code>helm</code> command, you can add multiple Helm repos to your local Helm installation like so:</p> <p>Add the <code>stable</code> repo <pre><code>$ helm repo add smiledh-stable https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\n$ helm repo update\n</code></pre></p> <p>Add the <code>pre</code> repo <pre><code>$ helm repo add smiledh-pre https://gitlab.com/api/v4/projects/40759898/packages/helm/pre\n$ helm repo update\n</code></pre></p> <p>If you are using the Terraform Helm provider, the process is slightly different as there is no concept of adding the repository to your local installation as there is with the <code>helm</code> command method.</p> <p>Instead, you specify the appropriate channel in your <code>helm_release</code> resource:</p> <p>Use the <code>stable</code> repo <pre><code>resource \"helm_release\" \"example\" {\n  name       = \"my-smilecdr-release\"\n  repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\"\n  chart      = \"smilecdr\"\n  version    = \"2.0.0-next-major.1\"\n}\n</code></pre></p> <p>Use the <code>pre</code> repo <pre><code>resource \"helm_release\" \"example\" {\n  name       = \"my-smilecdr-release\"\n  repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/pre\"\n  chart      = \"smilecdr\"\n  devel      = true\n}\n</code></pre></p> <p>Warning</p> <p>By using <code>devel = true</code>, this will install the latest version from the <code>pre</code> channel unless you explicitly set <code>version</code> This could result in unexpected breaking changes being introduced.</p> <p>It is recommended to always specify a version of the Helm Chart that you wish to install.</p>"},{"location":"upgrading/release-channels/#switching-release-channels","title":"Switching Release Channels","text":"Using the <code>helm</code> CommandUsing the <code>helm</code> Terraform Provider <p>If you have previously added the repository for the <code>devel</code> channel to your local Helm installation, you should remove it and switch to use the <code>stable</code> channel instead.</p> <p>Remove the existing repository for the <code>devel</code> channel and add the repository for the <code>stable</code> channel.</p> <pre><code>$ helm repo remove smiledh # &lt;-- Or whatever you previously named the repo locally\n$ helm repo add smiledh https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\n$ helm repo update\n</code></pre> <p>You can also add additional release channels as additional local repositories. To do this, you would run the <code>helm repo add</code> command for each repository, using different aliases as shown in the examples further up this page.</p> <p>If you are using the Terraform <code>helm_provider</code> as explained above, switching from the <code>devel</code> channel to the <code>stable</code> channel is done by replacing:</p> <pre><code>    repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\"\n    devel      = true\n</code></pre> <p>with</p> <pre><code>    repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/stable\"\n    version    = \"2.0.0-next-major.1\"\n</code></pre> <p>Note</p> <p>The <code>devel</code> option has no effect on the <code>stable</code> branch and need not be used, as there are no pre release or beta versions published there.</p>"},{"location":"upgrading/release-channels/#choosing-the-helm-chart-version","title":"Choosing The Helm Chart Version","text":"<p>It is advisable to explicitly set the <code>version</code> rather than allowing Helm to automatically use the latest version. This will help reduce the chances of accidental upgrades to new versions.</p>"},{"location":"upgrading/version-matrix/","title":"Supported Versions","text":"<p>Before choosing a version of the Smile CDR Helm Chart or the core Smile CDR product, refer to the version tables below to ensure that you are using a compatible combination.</p>"},{"location":"upgrading/version-matrix/#current-stable-version","title":"Current Stable Version","text":"<ul> <li>Smile CDR Helm Chart: <code>2.0.0-next-major.1</code></li> <li>Smile CDR: <code>2024.08.R01</code></li> </ul>"},{"location":"upgrading/version-matrix/#stable-releases","title":"Stable Releases","text":"<p>These are the current stable releases, published in the <code>stable</code> release channel</p> Helm Chart Version Default Smile CDR Version Oldest Supported Smile CDR Version v2.0.0 <code>2024.08.R01</code> <code>2023.08.R10</code> v1.1.1 <code>2024.05.R04</code> <code>2023.05.R03</code> v1.1.0 <code>2024.05.R04</code> <code>2023.05.R03</code> v1.0.0 <code>2024.05.R03</code> <code>2023.05.R03</code>"},{"location":"upgrading/version-matrix/#upcoming-release-previews","title":"Upcoming Release Previews","text":"<p>These future versions will be published in one of the prerelease channels</p> Helm Chart Version Release Channel Default Smile CDR Version Oldest Supported Smile CDR Version v2.1.0-pre.* <code>pre</code> <code>2024.08.R01</code> <code>2023.08.R10</code> v3.0.0-next.* <code>next</code> <code>2024.11.PRE-*</code> <code>2023.11.R06</code> v4.0.0-beta.* <code>beta</code> <code>2025.02.PRE-*</code> <code>2024.02.R06</code> v5.0.0-alpha.* <code>alpha</code> <code>2025.05.PRE-*</code> <code>2024.05.R04</code>"},{"location":"upgrading/versioning-strategy/","title":"Versioning Strategy","text":"<p>As per the Helm Best Practices guidelines, the Smile CDR Helm Chart adheres to the Semantic Versioning 2.0 specification.</p>"},{"location":"upgrading/versioning-strategy/#version-format","title":"Version Format","text":"<ul> <li>The version number is in the format <code>MAJOR.MINOR.PATCH</code><ul> <li>Optionally includes a prerelease suffix (e.g. <code>-pre.n</code> or <code>-next.n</code>).</li> </ul> </li> </ul>"},{"location":"upgrading/versioning-strategy/#release-types","title":"Release Types","text":""},{"location":"upgrading/versioning-strategy/#major-releases","title":"Major Releases","text":"<p>Occur every 3 months, aligned with the upstream releases of Smile CDR.</p> <ul> <li>The <code>MAJOR</code> version number is incremented</li> <li>The <code>MINOR</code> and <code>PATCH</code> version numbers are set to zero</li> <li>Includes new major releases of Smile CDR.</li> <li>Includes any new breaking features or fixes that could not be included in the previous major release</li> <li>For example, version <code>1.1.1</code> uses Smile CDR <code>2024.05.R04</code>, and version <code>2.0.0</code>, when released, will use <code>2024.08.R01</code>.</li> </ul>"},{"location":"upgrading/versioning-strategy/#minor-releases","title":"Minor Releases","text":"<p>Occur whenever new non-breaking features are released.</p> <ul> <li>The <code>MAJOR</code> version number remains unchanged</li> <li>The <code>MINOR</code> version number is incremented</li> <li>The <code>PATCH</code> version number is set to zero</li> <li>Includes any new non-breaking features</li> <li>May also included bundled new non-breaking fixes</li> </ul>"},{"location":"upgrading/versioning-strategy/#patch-releases","title":"Patch Releases","text":"<p>Occur whenever new non-breaking fixes are released.</p> <ul> <li>The <code>MAJOR</code> and <code>MINOR</code> version numbers remain unchanged</li> <li>The <code>PATCH</code> version number is incremented</li> <li>Includes any new non-breaking fixes</li> <li>Includes changes in the patch level of Smile CDR.</li> <li>For example, version <code>1.1.0</code> uses Smile CDR <code>2024.05.R03</code>, and version <code>1.1.1</code> uses <code>2024.05.R04</code>.</li> </ul>"},{"location":"upgrading/versioning-strategy/#pre-releases","title":"Pre Releases","text":"<p>Occur prior to minor or patch level releases for the current stable major release</p> <ul> <li>Named in the format <code>MAJOR.MINOR.PATCH-pre.n</code>, where <code>MAJOR.MINOR.PATCH</code> represents the version being previewed, and <code>n</code> is the pre release number.</li> <li>Incremental pre-releases before a minor or patch version update (e.g. upgrading from version <code>1.2.0-pre.5</code> to version <code>1.2.0-pre.6</code>) may include breaking changes.</li> <li>Used for testing and collaboration on new features for the current major release</li> </ul>"},{"location":"upgrading/versioning-strategy/#next-major-releases","title":"Next Major Releases","text":"<p>Occur prior to future major releases</p> <ul> <li>Named in the format <code>MAJOR.MINOR.PATCH-next.n</code>, where <code>MAJOR.MINOR.PATCH</code> represents the next major version being previewed, and <code>n</code> is the pre release number for the upcoming major release.</li> <li>Incremental pre-releases before a minor or patch version update (e.g. upgrading from version <code>2.0.0-pre.1</code> to version <code>2.0.0-pre.2</code>) may include breaking changes.</li> <li>Used for testing and collaboration on new features for the next major release, <code>n+1</code></li> </ul>"},{"location":"upgrading/versioning-strategy/#beta-releases","title":"Beta Releases","text":"<p>Occur prior to future major releases</p> <ul> <li>Named in the format <code>MAJOR.MINOR.PATCH-beta.n</code>, where <code>MAJOR.MINOR.PATCH</code> represents the major version in beta, and <code>n</code> is the beta release number.</li> <li>Incremental beta releases before a major version update (e.g. upgrading from version <code>3.0.0-beta.2</code> to version <code>3.0.0-beta.3</code>) may include breaking changes.</li> <li>Used for testing and collaboration on new features for a future major release, <code>n+2</code></li> <li>After <code>2.0.0</code> is released, future <code>3.x</code> minor or patch level releases will be developed on the <code>next</code> releases, starting at <code>3.0.0-next.1</code></li> </ul>"},{"location":"upgrading/versioning-strategy/#alpha-releases","title":"Alpha Releases","text":"<p>Occur prior to future major releases</p> <ul> <li>Named in the format <code>MAJOR.MINOR.PATCH-alpha.n</code>, where <code>MAJOR.MINOR.PATCH</code> represents the major version in alpha, and <code>n</code> is the alpha release number.</li> <li>Incremental alpha releases before a major version update (e.g. upgrading from version <code>4.0.0-beta.1</code> to version <code>4.0.0-beta.2</code>) may include breaking changes.</li> <li>Used for testing and collaboration on new features for a future  major release, <code>n+3</code></li> <li>After <code>2.0.0</code> is released, future <code>4.x</code> minor or patch level releases will be developed on the <code>beta</code> releases, starting at <code>4.0.0-beta.1</code></li> </ul>"},{"location":"upgrading/versioning-strategy/#breaking-change-features","title":"Breaking Change Features","text":"<p>If a new Helm Chart feature or fix includes a breaking change, then it will cause a bump in the major version number.</p> <p>In order to maintain the 3-month rule for Major versions being aligned with Smile CDR releases, any new features with breaking changes will be released along with the next Major version of Smile CDR</p>"},{"location":"upgrading/versioning-strategy/#back-porting-features","title":"Back Porting Features","text":"<p>As new features are developed on the current and future versions of the Helm Chart, they will not normally be available on previous versions.</p> <p>If you need to use a new feature, it is strongly recommended to update to the latest major version of the Helm Chart. Back-porting of features to previous versions of the Helm Chart will be considered on an individual basis.</p>"},{"location":"upgrading/versioning-strategy/#critical-bug-fixes","title":"Critical Bug Fixes","text":"<p>Any critical bug or security fixes will be back-ported to the previous 4 major versions of the Helm Chart if it is technically feasible to do so.</p>"}]}