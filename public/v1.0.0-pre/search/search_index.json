{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is the documentation for the Helm Charts provided by Smile Digital Health.</p> <p>These Helm Charts are provided to simplify installation and configuration of Smile Digital Health products in Kubernetes environments using a number of best practices.</p> <p>Currently, the only chart available is for the core Smile CDR product.</p> <p>WARNING: These charts are still in pre-release! As such, there may still be breaking changes introduced without notice.</p> <p>Only use this version of the chart for evaluation or testing.</p>"},{"location":"#required-helm-version","title":"Required Helm Version","text":"<p>To avoid unexpected issues, we recommend using the latest version of Helm v3.</p> <p>These Helm Charts have been developed and tested using Helm <code>&gt;=3.10.1, &lt;4.0.0</code>.</p> <p>Warning: Using older versions may cause backwards compatibility issues.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To quickly see these charts in action, follow the guide in the Quickstart section.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>Full details on configuring the products using these charts are provided in the User Guide section.</p>"},{"location":"charts/smilecdr/","title":"Smile CDR Helm Chart","text":"<p>A Helm Chart to install Smile CDR on Kubernetes</p>"},{"location":"charts/smilecdr/#releases","title":"Releases","text":"<p>Latest release: 1.0.0-pre.79</p>"},{"location":"charts/smilecdr/CHANGELOG-PRE/","title":"1.0.0-pre.91 (2023-08-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features","title":"Features","text":"<ul> <li>smilecdr: add db suffix configuration (ddf3b63)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre90-2023-08-25","title":"1.0.0-pre.90 (2023-08-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>smilecdr: add error checking for mapped files (95b82d0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre89-2023-08-19","title":"1.0.0-pre.89 (2023-08-19)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>smilecdr: add config sanity checks (eea5ae6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre88-2023-08-18","title":"1.0.0-pre.88 (2023-08-18)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow hierarchical config (f664d5e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre87-2023-07-27","title":"1.0.0-pre.87 (2023-07-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow specifying db name (4d4cd84)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre86-2023-06-27","title":"1.0.0-pre.86 (2023-06-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix base_url for hybrid provider (e80367d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre85-2023-06-26","title":"1.0.0-pre.85 (2023-06-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>smilecdr: update default modules (f83ff90)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre84-2023-06-26","title":"1.0.0-pre.84 (2023-06-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_1","title":"Features","text":"<ul> <li>smilecdr: make readiness probe configurable (e83a98e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre83-2023-06-20","title":"1.0.0-pre.83 (2023-06-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_2","title":"Features","text":"<ul> <li>smilecdr: allow disabling of module ingress (ca3011b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre82-2023-06-09","title":"1.0.0-pre.82 (2023-06-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>smilecdr: update transaction module name (f4472ca)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre81-2023-06-08","title":"1.0.0-pre.81 (2023-06-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_3","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R02 (e7362b8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre80-2023-06-06","title":"1.0.0-pre.80 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_4","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R01 (99aa74d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre79-2023-06-06","title":"1.0.0-pre.79 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_5","title":"Features","text":"<ul> <li>smilecdr: Update to Smile CDR 2023.05.R01 (bf89b79)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre78-2023-06-06","title":"1.0.0-pre.78 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix k8s resource labels (6378fe0)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre77-2023-06-06","title":"1.0.0-pre.77 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix disableAutoJarCopy option (03d5163)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre76-2023-06-06","title":"1.0.0-pre.76 (2023-06-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>smilecdr: update uid for curl images (a7cb4de)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre75-2023-05-08","title":"1.0.0-pre.75 (2023-05-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>common: remove <code>chart.shortname</code> template (f7a0a8b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre74-2023-05-04","title":"1.0.0-pre.74 (2023-05-04)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>pmp: add per-component imagePullSecrets logic (880c74e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre73-2023-05-04","title":"1.0.0-pre.73 (2023-05-04)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_6","title":"Features","text":"<ul> <li>pmp: add pmp chart (d52cc0c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre72-2023-05-02","title":"1.0.0-pre.72 (2023-05-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_7","title":"Features","text":"<ul> <li>pmp: add pmp-keycloak chart (10541a4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre71-2023-05-01","title":"1.0.0-pre.71 (2023-05-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_8","title":"Features","text":"<ul> <li>pmp: add pmp-directus chart (f8c0b1e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre70-2023-05-01","title":"1.0.0-pre.70 (2023-05-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_9","title":"Features","text":"<ul> <li>common: add Smile DH common library chart (46a3e67)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre69-2023-04-05","title":"1.0.0-pre.69 (2023-04-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix s3 copy for customerlib (66ea90a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre68-2023-04-05","title":"1.0.0-pre.68 (2023-04-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix s3 copy with readonly rootfs (f7a6a12)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre67-2023-03-28","title":"1.0.0-pre.67 (2023-03-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix kafka configs with IAM auth (170a14c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre66-2023-03-27","title":"1.0.0-pre.66 (2023-03-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix value for grace period (54686f7)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre65-2023-03-23","title":"1.0.0-pre.65 (2023-03-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>smilecdr: add preStop delay (2d31c8c)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre64-2023-03-22","title":"1.0.0-pre.64 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_10","title":"Features","text":"<ul> <li>smilecdr: add database properties mode (bb0b614)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre63-2023-03-22","title":"1.0.0-pre.63 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix licence module settings (ceab69d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre62-2023-03-22","title":"1.0.0-pre.62 (2023-03-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>smilecdr: use correct labels for Kafka admin (fce2935)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre61-2023-03-15","title":"1.0.0-pre.61 (2023-03-15)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_11","title":"Features","text":"<ul> <li>smilecdr: add HL7v2 support (05d5579)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre60-2023-03-08","title":"1.0.0-pre.60 (2023-03-08)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_12","title":"Features","text":"<ul> <li>smilecdr: update to latest Smile CDR version (db1caae)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre59-2023-03-07","title":"1.0.0-pre.59 (2023-03-07)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_13","title":"Features","text":"<ul> <li>smilecdr: add Kafka admin pod (c9f0493)</li> <li>smilecdr: rework Kafka configuration (d27a00b)</li> <li>smilecdr: update consumer properties (c1978e5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: This affects the default consumer properties configured in Smile CDR.</li> <li>smilecdr: Existing Kafka/Strimzi configurations have changed. As they were previously untested, the required changes may be unpredictable. Please refer to the docs to configure Kafka.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre58-2023-02-28","title":"1.0.0-pre.58 (2023-02-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>smilecdr: update initContainer configurations (c974af5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre57-2023-02-27","title":"1.0.0-pre.57 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_14","title":"Features","text":"<ul> <li>smilecdr: improve secrets error handling (8df9476)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre56-2023-02-27","title":"1.0.0-pre.56 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_15","title":"Features","text":"<ul> <li>smilecdr: improve warnings for chart errors (b50e54e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre55-2023-02-27","title":"1.0.0-pre.55 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_16","title":"Features","text":"<ul> <li>smilecdr: refactor image pull secrets (c7d376c), closes #78</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_1","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Deprecation warning - Values files must be updated to use <code>image.imagePullSecrets</code> instead of <code>image.credentials</code>.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre54-2023-02-27","title":"1.0.0-pre.54 (2023-02-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>smilecdr: follow redirects for curl (698d045)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre53-2023-02-26","title":"1.0.0-pre.53 (2023-02-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_17","title":"Features","text":"<ul> <li>smilecdr: add init-sync for customerlib (099cb57)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre52-2023-02-22","title":"1.0.0-pre.52 (2023-02-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_18","title":"Features","text":"<ul> <li>smilecdr: update to latest Smile CDR version (d9c0240)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre51-2023-02-21","title":"1.0.0-pre.51 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_19","title":"Features","text":"<ul> <li>smilecdr: improve readiness probe definition (25273a9)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre50-2023-02-21","title":"1.0.0-pre.50 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_20","title":"Features","text":"<ul> <li>smilecdr: allow extra env vars and volumes (9d53ec8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre49-2023-02-21","title":"1.0.0-pre.49 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>smilecdr: use provided tag for initcontainer (2f68eb6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre48-2023-02-21","title":"1.0.0-pre.48 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_21","title":"Features","text":"<ul> <li>smilecdr: update k8s secrets mechanism (0733b5d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre47-2023-02-21","title":"1.0.0-pre.47 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>smilecdr: correct spelling of license (25dd99a)</li> <li>smilecdr: use camelCase for <code>useDefaultModules</code> (a0178ac)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_2","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: This change affects the default module configuration.</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre46-2023-02-21","title":"1.0.0-pre.46 (2023-02-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_23","title":"Bug Fixes","text":"<ul> <li>smilecdr: remove short-circuit dependency (9af24c2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre45-2023-02-09","title":"1.0.0-pre.45 (2023-02-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_22","title":"Features","text":"<ul> <li>smilecdr: add license support (3c429d8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre44-2023-02-09","title":"1.0.0-pre.44 (2023-02-09)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_24","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix paths for initcontainer (ce17551)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre43-2023-01-30","title":"1.0.0-pre.43 (2023-01-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_23","title":"Features","text":"<ul> <li>smilecdr: add support for 2023.02 release (7c583de)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre42-2023-01-30","title":"1.0.0-pre.42 (2023-01-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_24","title":"Features","text":"<ul> <li>smilecdr: add config locking options (351d48d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre41-2023-01-28","title":"1.0.0-pre.41 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_25","title":"Features","text":"<ul> <li>smilecdr: add support for Alpine3 base image (dc7e960)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre40-2023-01-28","title":"1.0.0-pre.40 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_25","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ingress annotation overrides (78d254e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre39-2023-01-27","title":"1.0.0-pre.39 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_26","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in env vars (81defae)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre38-2023-01-27","title":"1.0.0-pre.38 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_27","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in k8s secret (b84760a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre37-2023-01-26","title":"1.0.0-pre.37 (2023-01-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_28","title":"Bug Fixes","text":"<ul> <li>smilecdr: change objectAlias naming logic (ade3f22)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre36-2023-01-11","title":"1.0.0-pre.36 (2023-01-11)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_26","title":"Features","text":"<ul> <li>smilecdr: update ingress logic and docs (affff39)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre35-2023-01-06","title":"1.0.0-pre.35 (2023-01-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_27","title":"Features","text":"<ul> <li>smilecdr: copy files from external location (ea2710f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre34-2022-12-22","title":"1.0.0-pre.34 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_29","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ConfigMap reference in volume (5cc81e8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre33-2022-12-22","title":"1.0.0-pre.33 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_30","title":"Bug Fixes","text":"<ul> <li>smilecdr: force lower case in resource names (6d1e4aa)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre32-2022-12-20","title":"1.0.0-pre.32 (2022-12-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_28","title":"Features","text":"<ul> <li>smilecdr: change db secret config (8069b77)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_3","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Values file needs to be updated if using sscsi for DB secrets</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre31-2022-12-14","title":"1.0.0-pre.31 (2022-12-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_29","title":"Features","text":"<ul> <li>smilecdr: disable crunchypgo (a2a4e32)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre30-2022-12-10","title":"1.0.0-pre.30 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_30","title":"Features","text":"<ul> <li>smilecdr: add argocd feature (75a4874)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre29-2022-12-10","title":"1.0.0-pre.29 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_31","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix secret reference keys (2f6411b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-06","title":"1.0.0-pre.28 (2022-12-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_31","title":"Features","text":"<ul> <li>smilecdr: make image secret required (e068330)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-05","title":"1.0.0-pre.28 (2022-12-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_32","title":"Features","text":"<ul> <li>smilecdr: make image secret required (2c2389b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre27-2022-12-03","title":"1.0.0-pre.27 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_32","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix DB_PORT in default modules (2239f60)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre26-2022-12-03","title":"1.0.0-pre.26 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_33","title":"Bug Fixes","text":"<ul> <li>smilecdr: correct the field for image secrets (2ddeef2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre25-2022-12-02","title":"1.0.0-pre.25 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_33","title":"Features","text":"<ul> <li>smilecdr: support multiple databases (e28bb13)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre24-2022-12-02","title":"1.0.0-pre.24 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_34","title":"Bug Fixes","text":"<ul> <li>smilecdr: improve modules include logic (f7850d2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre23-2022-12-02","title":"1.0.0-pre.23 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_35","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow quoted config entries (f9d7e1f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre22-2022-12-01","title":"1.0.0-pre.22 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_36","title":"Bug Fixes","text":"<ul> <li>smilecdr: change crunchydata resource names (654417b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_34","title":"Features","text":"<ul> <li>smilecdr: improve CrunchyData integration (4289432)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre21-2022-12-01","title":"1.0.0-pre.21 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_35","title":"Features","text":"<ul> <li>smilecdr: update JVM tuning params (cc1f859)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre20-2022-12-01","title":"1.0.0-pre.20 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_37","title":"Bug Fixes","text":"<ul> <li>smilecdr: remove image reference (2b47fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre19-2022-12-01","title":"1.0.0-pre.19 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_36","title":"Features","text":"<ul> <li>smilecdr: add support for strimzi kafka (27b8fb4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre18-2022-12-01","title":"1.0.0-pre.18 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_37","title":"Features","text":"<ul> <li>smilecdr: add autoscaling support (6ff0f2f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre17-2022-12-01","title":"1.0.0-pre.17 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_38","title":"Features","text":"<ul> <li>smilecdr: configure rolling deployments (daa9c4b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre16-2022-12-01","title":"1.0.0-pre.16 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_39","title":"Features","text":"<ul> <li>smilecdr: add pod disruption budget (b8b46e4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre15-2022-12-01","title":"1.0.0-pre.15 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_40","title":"Features","text":"<ul> <li>smilecdr: add redeploy on config changes (90e33ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre14-2022-11-30","title":"1.0.0-pre.14 (2022-11-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_41","title":"Features","text":"<ul> <li>smilecdr: add readiness probe (5a03100)</li> <li>smilecdr: add startup probe (65014ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre13-2022-11-25","title":"1.0.0-pre.13 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_42","title":"Features","text":"<ul> <li>smilecdr: update CDR version and modules (116e187)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_4","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: - This updates the Smile CDR version</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre12-2022-11-25","title":"1.0.0-pre.12 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_43","title":"Features","text":"<ul> <li>smilecdr: add support for deploying postgres (0fc81b1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre11-2022-11-24","title":"1.0.0-pre.11 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_38","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix reference to configmap data (383fdd6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre10-2022-11-24","title":"1.0.0-pre.10 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_44","title":"Features","text":"<ul> <li>smilecdr: support injecting files (a823a7d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre9-2022-11-23","title":"1.0.0-pre.9 (2022-11-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_45","title":"Features","text":"<ul> <li>smilecdr: automate setting JVMARGS (6fcda8b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre8-2022-11-22","title":"1.0.0-pre.8 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_46","title":"Features","text":"<ul> <li>smilecdr: set default replicas to 1 (9788832)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre7-2022-11-22","title":"1.0.0-pre.7 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_47","title":"Features","text":"<ul> <li>smilecdr: add name override for CrunchyPGO (110e133)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre6-2022-11-22","title":"1.0.0-pre.6 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_48","title":"Features","text":"<ul> <li>smilecdr: add Secrets Store CSI support (f8f23ba)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre5-2022-11-21","title":"1.0.0-pre.5 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_49","title":"Features","text":"<ul> <li>smilecdr: add support for IRSA (IAM roles) (509bbe3)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre4-2022-11-21","title":"1.0.0-pre.4 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_50","title":"Features","text":"<ul> <li>smilecdr: update Ingress definition logic (2271d57)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_5","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Now uses <code>nginx-ingress</code> instead of <code>aws-lbc-nlb</code> for specifying Nginx Ingress Controller</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre3-2022-11-21","title":"1.0.0-pre.3 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_51","title":"Features","text":"<ul> <li>smilecdr: add back default tag functionality (46785e5)</li> <li>smilecdr: add common labels to all resources (618ba2e)</li> <li>smilecdr: normalize resource names (6ca25bd)</li> <li>smilecdr: remove extra labels from default values (d971934)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre2-2022-11-21","title":"1.0.0-pre.2 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_52","title":"Features","text":"<ul> <li>smilecdr: remove hard coded entries from ConfigMap (b6a2fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre1-2022-11-21","title":"1.0.0-pre.1 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_53","title":"Features","text":"<ul> <li>repo: initial Commit (5f98460), closes #68834381</li> <li>smilecdr: add external module files support (da374c1)</li> <li>smilecdr: update application version (9b203f2)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/","title":"Smile CDR","text":"<p>The Smile CDR Helm Chart provides a flexible and consistent method to deploy Smile CDR on a Kubernetes cluster.</p> <p>It is provided by Smile Digital Health to help reduce the effort and complexity of installing Smile CDR on Kubernetes. It has been well tested on Amazon EKS and has growing compatibility for Azure AKS.</p>"},{"location":"charts/smilecdr/smilecdr/#feature-matrix","title":"Feature Matrix","text":"<p>The Smile CDR Helm Chart supports a number of features to help you install Smile CDR in a secure, reliable, cost effective and scalable manner with operational efficiency in mind.</p>"},{"location":"charts/smilecdr/smilecdr/#application-features","title":"Application Features","text":"<p>This following table shows you the Smile CDR features that are currently supported by this Helm Chart \"out-of-the-box\", which platform (AWS EKS / Azure AKS) they are supported on and the required Smile CDR and Helm Chart versions:</p> Smile CDR Feature EKS AKS Notes Smile CDR Version Helm Chart Version Install Smile CDR <code>2023.08</code> Smile CDR <code>2023.05</code> is the minimum supported version.Helm Install Guide <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Minor version upgrades Upgrade by overriding image tag.Smile CDR Upgrades <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Flexible CDR Node cluster configurations Configuration for single-node or multi-node Smile CDR cluster designs. Cluster Configuration <code>2023.08.R01</code> <code>v1.0.0-pre93</code> Cluster Scaling Horizontal Pod Autoscaling may be enabled. You need sufficient licenced core allocation if using autoscaling.Smile CDR Scaling <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Configuration of CDR Modules All modules can be configured and updated with zero downtime.Module Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Postgres Database Supports multiple databases. i.e. for Clustermgr, Persistence, Audit etc.Database Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> JVM Tuning Resource Tuning using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Kafka Message Broker Message Broker Configuration using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> Add files to <code>classpath</code> or <code>customerlib</code> Including Files using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> HL7 v2.x with <code>HL7_OVER_HTTP</code> Configuring HL7 v2.x Endpoint using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> License Module Configuring License using Helm Chart <code>2023.08.R01</code> <code>v1.0.0-pre92</code> <p>The following Smile CDR features are not currently supported:</p> Smile CDR Feature Notes GitLab Issue Install Smile CDR <code>2023.02</code> and lower Core module configuration changes were made in <code>2023.05.R01</code>, so this Helm Chart does not officially support lower versions. See CDR Versions section for more info NA Zero Downtime Upgrades Support planned to be added. See Smile CDR Docs for info on Zero Downtime Upgrades here GitLab Issue Pre-Seeding Users Smile CDR Docs GitLab Issue Pre-Seeding OIDC Servers Smile CDR Docs GitLab Issue Pre-Seeding OIDC Clients Smile CDR Docs GitLab Issue Pre-Seeding OIDC Keystores Smile CDR Docs GitLab Issue Pre-Seeding Packages Smile CDR Docs GitLab Issue Pre-Seeding FHIR Resources Smile CDR Docs GitLab Issue IAM Auth for RDS Databases Support for IAM database auth to be added GitLab Issue MSSQL Databases Support for MS SQL databases to be added GitLab Issue Oracle Databases Support for Oracle databases to be added GitLab Issue MongoDB Databases Support for MongoDB databases to be added GitLab Issue ActiveMQ Message Broker Support not currently planned NA"},{"location":"charts/smilecdr/smilecdr/#infrastructure-features","title":"Infrastructure Features","text":""},{"location":"charts/smilecdr/smilecdr/#app-networking","title":"App Networking","text":"<ul> <li>Automatic configuration of Kubernetes Services and Ingresses</li> <li>Coming soon...<ul> <li>Network Policies</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#ingress","title":"Ingress","text":"<ul> <li>TLS termination at load Balancer</li> <li>Nginx Ingress Controller</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> <li>Coming soon...<ul> <li>Multiple Ingress (e.g. internal and external, for different modules)</li> <li>End-to-end TLS encryption</li> <li>Support for other ingress controllers may be implemented as required</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#dependency-provisioning","title":"Dependency Provisioning","text":"<p>You can use this chart to configure and automatically deploy the following components. If enabled, they will automatically be configured in a production-like configuration, although we do not recommend using them in production environments at this time.</p> <ul> <li>Postgres Database - Uses the CrunchyData Postgres Operator</li> <li>Kafka Cluster - Uses the Strimzi Kafka Operator</li> <li>Coming soon...<ul> <li>MongoDB</li> </ul> </li> </ul> <p>With these components installed in your Kubernetes cluster, you can provision an entire Smile CDR stack, complete with persistent backed-up database and a Kafka cluster in about 5-10 mins. May take longer if your K8s cluster needs to autoscale to create more worker nodes first.</p>"},{"location":"charts/smilecdr/smilecdr/#security-features","title":"Security Features","text":"<p>It's no good having an easy to use Helm Chart if you cannot use it in a secure manner. As such, we have included the following features when running on Amazon EKS (Other providers to follow):</p> <ul> <li>IAM roles for service accounts(AWS Only) -   Smile CDR pods run with their own IAM role, independent and isolated from other workloads on the cluster.</li> <li>Secrets Store CSI Driver - Store secrets in a secure vault, and not in your code.<ul> <li>AWS SSCSI Provider - (Uses AWS Secrets Manager)</li> </ul> </li> <li>NEW!!! Enhanced pod security<ul> <li>Pods run as non-root, non-privileged</li> <li>Privilege escalation disabled</li> <li>Read-only root filesystem</li> <li>All container security capabilities disabled</li> </ul> </li> <li>Coming soon...<ul> <li>Execution role support in other cloud providers</li> <li>Support for other SSCSI providers</li> <li>Pod Security Policies</li> <li>Security Groups For Pods</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#reliability-features","title":"Reliability Features","text":"<ul> <li>High availability when running 2 or more Pods</li> <li>Self healing - Failed pods get restarted</li> <li>Pod disruption budgets (Prevents accidental outages)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/#operational-efficiency-features","title":"Operational Efficiency Features","text":"<ul> <li>Zero-downtime configuration changes (Using rolling deployments)</li> <li>Horizontal Auto-Scaling (Within bounds of Smile CDR licence) - to ensure cost effective use of compute resources</li> <li>Coming soon...<ul> <li>Zero-downtime upgrades with controllable manual/automatic schema upgrades</li> <li>Management dashboard for consolidated logs and metrics gathering for all components in the deployment</li> </ul> </li> </ul>"},{"location":"examples/","title":"Example Configurations","text":"<p>This section contains some example configurations and instructions to help get you started.</p>"},{"location":"examples/aws-dependencies/","title":"Configuring AWS Dependencies","text":"<p>This example shows how you would install dependencies in an AWS environment.</p> <p>We will go over the creation of the following AWS resources in preparation for deploying Smile CDR using the Helm Chart.</p> <ul> <li>IAM Role using IRSA</li> <li>AWS Secrets Manager</li> </ul> <p>Note: These resources are configured in a way that is not obvious to an AWS administrator that has not dealt with IRSA before, which is why we are including them here. For other AWS resources (Such as RDS, S3, Certificates Manager etc) conventional configurations can be used, so we do not cover them in this example at this time.</p> <p>You will need both of these if you are using the recommended method of storing your container registry secrets - using Secrets Store CSI Driver</p>"},{"location":"examples/aws-dependencies/#creating-iam-role-with-irsa","title":"Creating IAM Role with IRSA","text":"<p>To set up an IAM Role to be used by the application pods, we use IRSA (IAM Roles for Service Accounts). Detailed information and instructions for IRSA are located here</p> <p>In this example, we will be creating the role to work with an deployment of Smile CDR in a fictional EKS cluster with the following properties:</p> <ul> <li>AWS Region <code>us-east-1</code></li> <li>Cluster Name <code>mycluster</code></li> <li>Namespace <code>smilecdr</code></li> <li>Helm Release Name <code>my-smile</code></li> </ul> <p>These are important as they will be referenced in the trust policy.</p>"},{"location":"examples/aws-dependencies/#iam-policy","title":"IAM Policy","text":"<p>Before starting, you need to determine which AWS services need to be accessed by this role. Typical examples would be:</p> <ul> <li>AWS Secrets Manager - Used for storing credentials for container repository and database</li> <li>RDS - Required if Smile CDR is configured to use IAM authentication for RDS</li> <li>S3 - Required if you are including extra files using the external method</li> </ul> <p>You will need to keep these resources in mind when creating your IAM Policy.</p> <p>Note: When creating IAM Policies, you should keep the principle of least privilege in mind and only allow the minimum required access for the resources needed. Avoid using wildcard entries for <code>actions</code> and <code>resources</code> where possible.</p> <p>In this example, we will create a policy that only has access to the container repository secret that we create below</p> <p>Following the AWS CLI instructions from here we would do the following:</p> <ol> <li> <p>Create an IAM policy file</p> <p>Create IAM Policy file with the following content: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:GetSecretValue\",\n            \"Resource\": \"arn:aws:secretsmanager:us-east-1:&lt;accountid&gt;:secret:demo/dockerpull-??????\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"arn:aws:kms:*:&lt;accountid&gt;:aws/secretsmanager\"\n        }\n    ]\n}\n</code></pre></p> <p>Note: The <code>??????</code> is a wildcard that matches the random suffix added to an AWS Secrets Manager Secret. See here for more info.</p> </li> <li> <p>Create the IAM Policy <pre><code>aws iam create-policy --policy-name smilecdr-dockersecret-policy --policy-document file://my-policy.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#iam-role","title":"IAM Role","text":"<p>When creating the IAM role, it needs to be associated with the Kubernetes service account via a trust policy. To do this, we need a few details in advance:</p> <p>AWS Account number <pre><code>account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\n</code></pre></p> <p>EKS cluster's OIDC provider <pre><code>oidc_provider=$(aws eks describe-cluster --name mycluster --region us-east-1 --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\")\n</code></pre> Namespace and ServiceAccount Resource Names</p> <p>In the case of this example, we are using the <code>smilecdr</code> namespace, with the <code>my-smile</code> release name as mentioned above. This will result in a Service Account with the name <code>my-smile-smilecdr</code> <pre><code>export namespace=smilecdr\nexport service_account=my-smile-smilecdr\n</code></pre></p> <ol> <li> <p>Create trust policy file</p> <pre><code>cat &gt;trust-relationship.json &lt;&lt;EOF\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n        \"StringEquals\": {\n        \"$oidc_provider:aud\": \"sts.amazonaws.com\",\n        \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\"\n        }\n    }\n    }\n]\n}\nEOF\n</code></pre> </li> <li> <p>Create the IAM Role <pre><code>aws iam create-role --role-name smilecdr-role --assume-role-policy-document file://trust-relationship.json --description \"Smile CDR Application Role\"\n</code></pre></p> </li> <li> <p>Attach the IAM Policy to the role <pre><code>aws iam attach-role-policy --role-name smilecdr-role --policy-arn=arn:aws:iam::$account_id:policy/smilecdr-dockersecret-policy\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#using-iam-role-in-helm-values","title":"Using IAM Role in Helm Values","text":"<p>Configure your values file to use this role for the Service Account like so: <pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::&lt;account id&gt;:role/smilecdr-role\n</code></pre></p> <p>Now, when you deploy Smile CDR, it will use the above IAM role whenever accessing AWS resources.</p>"},{"location":"examples/aws-dependencies/#creating-aws-secrets-manager-secrets","title":"Creating AWS Secrets Manager Secrets","text":"<p>Secrets can be a bit of a chicken-and-egg problem.</p> <p>If you want to avoid storing secrets in code, by using a secrets vault, how do you do this 'via code'?</p> <p>One mechanism is to get the vault software to generate a random secret, or rotate the secret after the initial creation. These are not always viable options, which is certainly the case for storing secrets to access external systems, such as a container registry.</p> <p>In this example, we will create the <code>docker pull</code> secret manually via the AWS CLI. You could just as easily deploy the secret using some other mechanism and then update it with the cli or with the AWS console.</p>"},{"location":"examples/aws-dependencies/#create-secret","title":"Create Secret","text":"<p>The value of Kubernetes <code>imagePullSecrets</code> needs to be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p> <p>This essentially means the secret value is a Json string representing the Docker <code>config.json</code> file. As an AWS Secrets Manager secret consists of a Json map of secrets and values, we end up with a nested Json data structure.</p> <p>The easiest way to pass this to the AWS CLI command is to temporarily store the Json in a file which can be passed in to the <code>create-secret</code> command as a parameter.</p> <ol> <li>Create the temporary password json file</li> </ol> <p>Update your user &amp; password before running the below.</p> <pre><code>cat &gt;dockerconf.json &lt;&lt;EOF\n{\n  \"auths\":{\n    \"docker.smilecdr.com\":{\n      \"auth\": \"$(echo -n \"user:password\" | base64)\"\n    }\n  }\n}\nEOF\n\ncat &gt;tempsecret.json &lt;&lt;EOF\n{\n  \"dockerconfigjson\": \"$(cat dockerconf.json)\"\n  }\n}\nEOF\n</code></pre> <ol> <li> <p>Create the Secrets Manager Secret <pre><code>aws secretsmanager create-secret \\\n    --name \"demo/dockerpull\" \\\n    --secret-string file://tempsecret.json\n</code></pre></p> <p>Note: The name <code>demo/dockerpull</code> is just an example. You may use any scheme that you like for secret names. If you have an existing standard, use that.</p> </li> <li> <p>Remove the temporary secret file <pre><code>rm tempsecret.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#update-secret","title":"Update Secret","text":"<p>Use the following to update the secret with a new value if required.</p> <pre><code>aws secretsmanager update-secret \\\n    --secret-id \"demo/dockerpull\" \\\n    --secret-string file://tempsecret.json\n</code></pre> <p>Note: You need to update rather than delete and recreate, as AWS Secrets Manager implements a grace period on secrets deletion.</p>"},{"location":"examples/aws-dependencies/#use-the-secret","title":"Use The Secret","text":"<p>If the IAM Role, IAM Policies and Helm Values were all set up correctly as per the above steps, you will now be able to launch pods using images from the authenticated container repository.</p>"},{"location":"examples/external-db-multi/","title":"Multiple External DB Configuration","text":"<p>This example demonstrates using multiple external Postgres Databases.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Separate databases for Cluster Manager, Audit logs, Transaction logs and default Persistence module</li> </ul>"},{"location":"examples/external-db-multi/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Multiple external Postgres databases provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published Json structure</li> </ul>"},{"location":"examples/external-db-multi/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\ndatabase:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\n      module: clustermgr\n    - secretName: auditSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:auditSecret\"\n      module: audit\n    - secretName: txLogsSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:txLogsSecret\"\n      module: transaction\n    - secretName: persistenceSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:persistenceSecret\"\n      module: persistence\n</code></pre>"},{"location":"examples/external-db/","title":"External DB Configuration","text":"<p>This example demonstrates using an external Postgres Database.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> </ul>"},{"location":"examples/external-db/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>External Postgres database provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published Json structure</li> </ul>"},{"location":"examples/external-db/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\ndatabase:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\n      module: clustermgr\n</code></pre>"},{"location":"examples/extra-files-external/","title":"Adding Files Configuration","text":"<p>This example demonstrates passing in extra files to the deployment from external sources.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR, using Amazon S3</li> <li>Elastic APM <code>.jar</code> file will be included in the <code>customerlib</code> directory in Smile CDR, using curl</li> </ul>"},{"location":"examples/extra-files-external/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>Amazon S3 bucket with a customized <code>logback.xml</code> file copied to a <code>classes</code> folder</li> <li>AWS IAM Role configured to access:<ul> <li>AWS Secrets Manager</li> <li>Amazon S3 bucket</li> </ul> </li> </ul>"},{"location":"examples/extra-files-external/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\ncopyFiles:\n  classes:\n    sources:\n    # Copies files recursively from S3 to the classes directory\n    - type: s3\n      bucket: s3-bucket-name\n      # The below S3 bucket prefix must contain the custom logback.xml\n      # file, as well as any other needed files.\n      path: /path-to/classes\n  customerlib:\n    sources:\n    # Downloads a single file using curl to the customerlib directory\n    # (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n    - type: curl\n      fileName: elastic-apm/elastic-apm-agent-1.13.0.jar\n      url: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre>"},{"location":"examples/extra-files-helm/","title":"Adding Files Using Helm","text":"<p>This example demonstrates passing in extra files to the deployment via the Helm Chart.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR</li> </ul>"},{"location":"examples/extra-files-helm/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Customized <code>logback.xml</code> file available in your configuration repo/folder</li> </ul>"},{"location":"examples/extra-files-helm/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmappedFiles:\n  logback.xml:\n    type: configMap\n    path: /home/smile/smilecdr/classes\n</code></pre>"},{"location":"examples/extra-files-helm/#extra-install-steps","title":"Extra Install Steps","text":"<p>To use this feature, you must update your <code>helm upgrade</code> command to include <code>--set-file mappedFiles.logback\\\\.xml.data=logback.xml</code></p>"},{"location":"examples/minimal/","title":"Minimal Configuration","text":"<p>This values file demonstrates the minimal required configurations to install Smile CDR in a secure manner (i.e. not passing repository secrets)</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/minimal/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/minimal/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre>"},{"location":"examples/modules-advanced/","title":"Advanced Modules Configuration","text":"<p>This example demonstrates defining all modules from scratch, not using any of the default modules.</p> <p>It is based on the minimal example.</p> <p>We will create a couple of examples.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Completely custom Smile CDR module configuraton<ul> <li>Disabled default module configuraton</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-advanced/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-advanced/#example-1-minimal-module-config","title":"Example 1 - Minimal module config","text":"<p>This example shows how you would configure Smile CDR to just use the following modules:</p> <ul> <li>Cluster Manager</li> <li>Persistence Module</li> <li>Local Security</li> <li>Admin Web</li> <li>Fhir Endpoint</li> </ul> <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  useDefaultModules: false\n\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: false\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre>"},{"location":"examples/modules-advanced/#example-2-minimal-module-config-with-separate-values-files","title":"Example 2 - Minimal module config with separate values files","text":"<p>As you can see from the above, the values file can start getting unwieldy.</p> <p>It is advised to split them up into manageable chunks like so.</p> <p>values-common.yaml <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  useDefaultModules: false\n</code></pre></p> <p>values-clustermgr.yaml <pre><code>modules:\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n</code></pre></p> <p>values-persistence-r4.yaml <pre><code>modules:\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: false\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n</code></pre></p> <p>values-security.yaml <pre><code>modules:\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n</code></pre></p> <p>values-admin-web.yaml <pre><code>modules:\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre></p> <p>values-fhir-endpoint.yaml <pre><code>modules:\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre></p> <p>When installing the above, you would then pass in the multiple values file like so: <pre><code>helm upgrade -i my-smile-env -f values-clustermgr.yaml -f values-persistence-r4.yaml -f values-security.yaml -f values-admin-web.yaml -f values-fhir-endpoint.yaml smiledh/smilecdr\n</code></pre></p>"},{"location":"examples/modules-simple/","title":"Basic Modules Configuration","text":"<p>This example demonstrates simple reconfiguration of Smile CDR modules.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Modified Smile CDR module configuraton</li> <li>We will only modify <code>dao_config.inline_resource_storage_below_size</code> for the persistence database</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-simple/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-simple/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  persistence:\n    config:\n      dao_config.inline_resource_storage_below_size: 4000\n</code></pre>"},{"location":"examples/previousrootversion/","title":"Using Previous (root) Version","text":"<p>This example demonstrates running an older version of Smile CDR that requires the container to be running as root.</p> <p>This only applies if using versions <code>2022.11.R04</code> or earlier.</p> <p>It is based on the Advanced Modules Configuration example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Container explicitly set to run as root</li> <li>Completely custom Smile CDR module configuraton<ul> <li>Disabled default module configuraton, including audit, transaction and license modules</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/previousrootversion/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/previousrootversion/#example-config","title":"Example Config","text":"Click to expand <pre><code>specs:\n  hostname: smilecdr.mycompany.com\n\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  tag: \"2022.11.R04\"\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n\nsecurityContext:\n  runAsNonRoot: false\n  runAsUser: 0\n\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n\nmodules:\n  # Define modules to be used. Some of these will contain service definitions.\n  # A service and an ingress rule will be created for modules that use services.\n  # Canonical endpoint URLs will be generated by _smile-module-helpers.tpl and\n  # populated in the smilecdr.services variable. These can be consumed by other\n  # modules that reference them.\n\n  useDefaultModules: false\n\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      # Valid options include H2_EMBEDDED, DERBY_EMBEDDED, MYSQL_5_7, MARIADB_10_1, POSTGRES_9_4, ORACLE_12C, MSSQL_2012\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  UPDATE\n      stats.heartbeat_persist_frequency_ms: 15000\n      stats.stats_persist_frequency_ms: 60000\n      stats.stats_cleanup_frequency_ms: 300000\n      audit_log.request_headers_to_store: Content-Type,Host\n      seed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n      transactionlog.enabled: false\n      retain_transaction_log_days: 7\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.hibernate.showsql: false\n      db.hibernate_search.directory: ./database/lucene_fhir_persistence\n      db.schema_update_mode: UPDATE\n      dao_config.expire_search_results_after_minutes: 60\n      dao_config.allow_multiple_delete.enabled: false\n      dao_config.allow_inline_match_url_references.enabled: true\n      dao_config.allow_external_references.enabled: false\n      dao_config.inline_resource_storage_below_size: 4000\n\n  admin_json:\n    name: JSON Admin Services\n    enabled: true\n    type: ADMIN_JSON\n    service:\n      enabled: true\n      svcName: admin-json\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: json-admin\n      port: 9000\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      # This is required right now as the default is not being honored.\n      # Can be removed if the default gets fixed. May be good to leave it explicit.\n      # Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\n      password_encoding_type: BCRYPT_12_ROUND\n\n  subscription:\n    name: Subscription\n    enabled: true\n    type: SUBSCRIPTION_MATCHER\n    requires:\n      PERSISTENCE_ALL: persistence\n\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  fhirweb_endpoint:\n    name: FHIRWeb Console\n    enabled: true\n    type: ENDPOINT_FHIRWEB\n    service:\n      enabled: true\n      svcName: fhirweb\n    requires:\n      SECURITY_IN_UP: local_security\n      ENDPOINT_FHIR: fhir_endpoint\n    config:\n      context_path: fhirweb\n      port: 8001\n      threadpool.min: 2\n      threadpool.max: 10\n      tls.enabled: false\n      anonymous.access.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  # Fhir Endpoint\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n      threadpool.min: 2\n      threadpool.max: 10\n      browser_highlight.enabled: true\n      cors.enable: true\n      default_encoding: JSON\n      default_pretty_print: true\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      request_validating.enabled: false\n      request_validating.fail_on_severity: ERROR\n      request_validating.tags.enabled: false\n      request_validating.response_headers.enabled: false\n      request_validating.require_explicit_profile_definition.enabled:  false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  smart_auth:\n    name: SMART Security\n    enabled: true\n    type: SECURITY_OUT_SMART\n    service:\n      enabled: true\n      svcName: smart-auth\n    requires:\n      CLUSTERMGR: clustermgr\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: smartauth\n      port: 9200\n      openid.signing.keystore_id: default-keystore\n      issuer.url: default\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n\n  package_registry:\n    name: Package Registry\n    enabled: true\n    type: ENDPOINT_PACKAGE_REGISTRY\n    service:\n      enabled: true\n      svcName: pkg-registry\n    requires:\n      PACKAGE_CACHE: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: package_registry\n      port: 8002\n      tls.enabled: false\n      anonymous.access.enabled: true\n      security.http.basic.enabled: true\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n</code></pre>"},{"location":"examples/quickstart/","title":"Quickstart Configuration","text":"<p>This is the configuration used in the Quickstart</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via values file (Don't do this!)</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/quickstart/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Credentials to an image repository with the official Smile CDR images.</li> </ul>"},{"location":"examples/quickstart/#values-file","title":"Values File","text":"<pre><code>specs:\n  hostname: smilecdr.mycompany.com\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: values\n    registry: docker.smilecdr.com\n    username: &lt;DOCKER_USERNAME&gt;\n    password: &lt;DOCKER_PASSWORD&gt;\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre>"},{"location":"guide/","title":"Chart Guides","text":"<p>The guides in this section cover all details of configuration using the Helm Charts</p>"},{"location":"guide/helm-repo/","title":"Helm Repository Setup","text":""},{"location":"guide/helm-repo/#configure-helm-repository","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>This will differ, depending on the method you will be using to deplpy the charts.</p>"},{"location":"guide/helm-repo/#native-helm","title":"Native Helm","text":"<p>The simplest way to get up and running is by using the native <code>helm</code> commands.</p> <p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"guide/helm-repo/#terraform","title":"Terraform","text":"<p>If installing the chart using Terraform, you may have a resource definition like so:</p> <pre><code>resource \"helm_release\" \"example\" {\n  name       = \"my-smilecdr-release\"\n  repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\"\n  chart      = \"smilecdr\"\n  # Use this to pin to a semantic version\n  # version    = \"~1.0.0\"\n  # Use this to use latest pre-release versions\n  devel      = \"true\"\n\n  values = [\n    \"${file(\"my-values.yaml\")}\"\n  ]\n\n  set {\n    name  = \"values.override\"\n    value = \"value\"\n  }\n}\n</code></pre> <p>See the Terraform Helm Provider for more info on this.</p>"},{"location":"guide/helm-repo/#argocd","title":"ArgoCD","text":"<p>If installing in ArgoCD using an <code>Application</code> Custom Resource, you will need to create a custom 'Umbrella Chart' for your deployment so that you can pass in your values file (And any other files).</p> <p>To do this, you would create a configuration directory with your configuration files as well as a <code>Chart.yaml</code> file that may look like this:</p> <pre><code>apiVersion: v2\nname: umbrella-smilecdr\ndescription: An Umbrella Helm chart to deploy the Smile CDR Helm Chart\n\n# This Umbrella Helm Chart can be used to deploy Smile CDR in ArgoCD while\n# passing in your values files.\n\ntype: application\nversion: 1.0.0\n\n# Remember, when passing values files in to dependency charts, the entire yaml map needs to be\n# moved to a root key that matches the `name` of the dependency.\ndependencies:\n- name: smilecdr\n  version: \"~1.0.0-pre.10\"\n  repository: \"https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\"\n</code></pre>"},{"location":"guide/helm-repo/#provide-repo-credentials","title":"Provide Repo Credentials","text":"<p>This Helm Chart supports multiple methods to securely provide access to container repositories.</p> Method Security Difficulty Notes AWS ECR with IAM role Highest Medium Recommended method if using ECR.This is only an option if you have a workflow to host the Smile CDR image on your own ECR repository.This has the highest security stance as long lived credentials are not required.No K8s <code>Secret</code> objects need to be created or referenced. Secrets Store CSI High Hardest Recommended method if not using ECR.You will need the SSCSI driver, an appropriate SSCSI provider, A secrets vault Secret and any IAM roles configured to access the secret Kubernetes Secret Medium Medium Need to manually set up Kubernetes Secret Values File Low Easiest K8s secret created by chart. Password is in your code (Bad)"},{"location":"guide/helm-repo/#aws-elastic-container-registry","title":"AWS Elastic Container Registry","text":"<p>If you are using AWS ECR as your container registry, you can avoid storing long-lived credentials in secrets by using a suitable IAM role.</p> <p>In order to use this method, the IAM role added to the Instance Profile of your Kubernetes worker nodes must include a policy that allows read access to your container registry. This mechanism does not use IRSA, so can only use the worker node's Instance Profile/IAM role.</p>"},{"location":"guide/helm-repo/#private-registry","title":"Private Registry","text":"<p>If you are pulling the Smile CDR docker image directly from <code>docker.smilecdr.com</code> or if you have custom Docker images that you build and publish on a private container registry that requires credentials, then you will need to provide them in a secure manner so that Kubernetes can pull the image.</p> <p>In Kubernetes, this is done using a list of secret names in <code>pod.spec.imagePullSecrets</code>. These secrets reference the name of pre-existing <code>Secret</code> objects in the same namespace.</p> <p>Note For more information on using private container registries with Kubernetes, see the official documentation here</p> <p>More details can be found in the Secrets Handling section of these docs.</p>"},{"location":"guide/helm-repo/#configuring-to-use-ecr-repository","title":"Configuring to use ECR Repository","text":"<p>In order for Kubernetes worker nodes to pull images from ECR, they must have an Instance Profile/IAM Role that has an IAM policy with the following actions allowed for your ECR Repository</p> <p>TODO: Add details of required IAM policy actions</p> <pre><code>iamsnippet:\n  here: yes yes\n</code></pre> <p>There are no extra steps after this, you do not need to specify any image pull secrets.</p> <p>If you do define image pull secrets for other containe registries, this will not be affected. You do not need to remove them.</p>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-secrets-store-csi-driver","title":"Configuring Repo Credentials using Secrets Store CSI Driver","text":"<p>Before using this configuration in your values file, ensure that you have followed the appropriate section in the Secrets Handling guide to set up Secrets Store CSI, the AWS Provider, your AWS Secret, your IAM Role and configured the <code>ServiceAccount</code>.</p> <p>Once you have done that, you need to add an item to the <code>image.imagePullSecrets</code> list like so like so:</p> <pre><code>image:\n  imagePullSecrets:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n</code></pre>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-kubernetes-secret","title":"Configuring Repo Credentials using Kubernetes Secret","text":"<p>Before using this configuration, you need to create a Kubernetes <code>Secret</code> object of type <code>kubernetes.io/dockerconfigjson</code> using some external/manual mechanism. For more info on this, refer to the Kubernetes section in Secrets Handling.</p> <p>Once this <code>Secret</code> object is created, you can add it in the same way that you would with a regular <code>imagePullSecrets</code> entry in the K8s PodSpec:</p> <p>It would look like this in your custom values file: <pre><code>image:\n  imagePullSecrets:\n  - name: myK8sSecretName\n    # Optional if you wish to make it explicitly clear which credential type you are using in your code\n    # type: k8sSecret\n</code></pre></p>"},{"location":"guide/helm-repo/#configuring-multiple-repo-credentials","title":"Configuring Multiple Repo Credentials","text":"<p>If you need to connect to multiple container repositories, you can mix and match the above types in the list of <code>imagePullSecrets</code></p> <pre><code>image:\n  imagePullSecrets:\n  # Regular pre-existing `Secret` object\n  - name: myK8sSecretName\n    type: k8sSecret\n  # Helm Chart will automatically create `Secret` objects for these:\n  - type: sscsi\n    provider: aws\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\n  - type: sscsi\n    provider: aws\n    # The secretArn must be uniqe\n    secretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname2\"\n# Service Account is still required here as we are using Secrets Store CSI\nserviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n</code></pre>"},{"location":"guide/secrets/","title":"Secrets Handling","text":"<p>Secrets management can be a hard subject to get right. Unfortunately, the easy way quite often lacks basic security considerations and can lead to unexpected data compromises.</p> <p>At Smile Digital Health, we take security very seriously, so we have designed these Helm Charts in a way that follows best practices, to reduce the likelihood of such compromises.</p>"},{"location":"guide/secrets/#secrets-best-practices","title":"Secrets Best Practices","text":""},{"location":"guide/secrets/#use-temporary-credentials","title":"Use Temporary Credentials","text":"<p>Where possible, use of long-lived credentials should be avoided. This is a general best-practice for cloud based environments that typically relies on underlying cloud provider technologies.</p> <p>For example, on AWS the best practice is to use temporary credentials using IAM Instance Profiles and IAM Roles For Service Accounts (IRSA).</p>"},{"location":"guide/secrets/#dont-store-secrets-in-your-configuration-code","title":"Don't Store Secrets in your Configuration Code","text":"<p>There are some scenarios where long-lived secret values still need to be used in this Helm Chart. These secret values may be required at the Kubernetes level, such as when pulling container images from private repositories, or at the application level, such as when connecting to databases or other external systems that require authentication.</p> <p>While it is easy to create secrets (such as passwords or API keys) in code, to simplify provisioning and automation, it is generally considered bad practice to do so because it can compromise the security of your system.</p> <p>If the code containing the secrets is somehow leaked, protected resources may be compromised. Additionally, if the code is shared among multiple team members, it can be difficult to control who has access to the secrets and when they were last rotated.</p>"},{"location":"guide/secrets/#use-secrets-management-tools","title":"Use Secrets Management Tools","text":"<p>It is recommended to use a secrets management tool to store and manage secrets separately from code. This way, secrets can easily be rotated and access can be tightly controlled.</p> <p>Various secrets management tools are available that allow you to store secrets in a secure, centralized location and control access to them through granular permissions. This helps ensure that only authorized personnel have access to sensitive information and helps prevent accidental disclosure of secrets.</p>"},{"location":"guide/secrets/#supported-secret-mechanisms","title":"Supported Secret Mechanisms","text":"<p>These Helm Charts support the following three methods to reference secrets.</p> Method Security Difficulty Notes Secrets Store CSI High Hardest Recommended method. You will need the SSCSI driver, an appropriate SSCSI provider, A secrets vault Secret and any IAM roles configured to access the secret Kubernetes Secret Medium Medium Need to manually set up Kubernetes Secret Values File Low Easiest K8s secret created by chart. Password is in your code (Bad)"},{"location":"guide/secrets/#secrets-store-csi-driver","title":"Secrets Store CSI Driver","text":"<p>Using the Secrets Store CSI Driver(SSCSI) is the preferred method to configure secrets in these Helm Charts.</p> <p>This mechanism is recommended by AWS, Azure and Google to retrieve secrets from their respective secret management services. It also has support for other Secret Vault providers such as HashiCorp Vault.</p> <p>Currently, the Smile CDR Helm Chart only supports the Secrets Store CSI Driver with the AWS Secrets Manager provider.</p> <p>Before you can use this method in your configuration, you will need to set up some pre-requisites.</p> <ul> <li>Create your secret in AWS Secrets Manager.<ul> <li>The secret data should be in a suitabe structured Json format, as described here</li> <li>Your secret should be encrypted using an AWS CMK (Customer Managed Key).</li> </ul> </li> <li>Create an IAM role and trust policy.</li> <li>Enable the <code>ServiceAccount</code> and reference the IAM role in the annotations.</li> <li>See the Service Account Configuration section for more details on setting this up.</li> </ul> <p>The way the secret is configured in your <code>values</code> file differs depending on the section of configuration. Please refer to the individual sections below for details:</p> <ul> <li>Image Repository Credentials</li> <li>Database Credentials</li> <li>Smile CDR License</li> </ul>"},{"location":"guide/secrets/#kubernetes-secret","title":"Kubernetes Secret","text":"<p>Alternatively, you can create the Kubernetes <code>Secret</code> object through some other method. Although it avoids the secret data being included in your code, it does not provide a centralized location to store, manage and controll access to secrets.</p> <p>Be wary of including custom Kubernetes <code>Secret</code> manifests alongside your Helm values files. Although this is a convenient way to provision them, it just re-introduces the problem of secrets residing in your code, which should be avoided.</p>"},{"location":"guide/secrets/#values-file","title":"Values File","text":"<p>Finally, we do support providing credentials in the values file itself. This is not a recommended solution and really only intended as a quickstart method to allow for quick experimentation with the charts.</p> <p>WARNING: This is not a recommended approach as it is insecure. This functionality may be removed in future versions of the charts as it can lead to insecure habits/practices forming.</p>"},{"location":"guide/serviceaccount/","title":"Service Accounts","text":"<p>Certain features of the application, when installed with these Helm Charts, require authorized access to external systems so that they can function correctly.</p> <p>Examples of this include:</p> <ul> <li>Retrieving credentials from secrets management systems (e.g. AWS Secrets Manager)</li> <li>Accessing AWS managed services such as:<ul> <li>RDS authentication using IAM roles</li> <li>AWS HealthLake</li> <li>Accessing Amazon MSK (Managed Kafka)</li> <li>Accessing S3 buckets</li> </ul> </li> <li>Waiting for Kubernetes jobs to complete (i.e. during product upgrades and migration tasks)</li> </ul> <p>As explained in the Secrets Handling section above, passing in secrets (such as AWS Access Keys &amp; Tokens etc) directly to your configuration is a dangerous practice. Instead we can use the mechanisms provided by various infrastructure providers to use secure methods to gain access to these external systems.</p>"},{"location":"guide/serviceaccount/#iam-roles-for-service-accounts-irsa","title":"IAM Roles For Service Accounts (IRSA)","text":"<p>To give the application access to AWS resources, we use IAM Roles For Service Accounts, also known as IRSA. This attaches AWS IAM roles to a Kubernetes <code>ServiceAccount</code> which then gets attached to the application Pods.</p> <p>As a result of this, the application can access AWS services without needing to directly pass in AWS IAM User credentials.</p> <p>Note Currently, the Smile CDR Helm Chart only supports this integration in AWS, but support for other cloud providers will be added.</p>"},{"location":"guide/serviceaccount/#service-account-configuration","title":"Service Account Configuration","text":"<p>To use this feature, you will need to enable the Service Account and reference the IAM role that it should be connected to. Note that the IAM role being used needs to have the appropriate Trust Policy set up so that it can be used by your Cluster. More info and instructions are available here</p> <ul> <li>Create an IAM role for your deployment.<ul> <li>This role will be used for any Role-based access that the application pods need,   so name it accordingly to avoid confusion, e.g. <code>smilecdr-role</code></li> <li>If being used for Secrets Store CSI, ensure that it has read access to the secrets it will need to provide, and any KMS key used to encrypt them.</li> </ul> </li> <li>Create a trust policy for the IAM role so that it can be used with IRSA. Instructions   here<ul> <li>These instructions use the <code>eksctl</code> command which abstracts away some details into CloudFormation templates. Using something like Terraform would require different steps.</li> </ul> </li> </ul> <p>Once the IAM role is set up correctly, enable the <code>ServiceAccount</code> and reference the IAM role in the annotations in your values file like so: <pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/example-role-name\n</code></pre></p>"},{"location":"guide/serviceaccount/#examples","title":"Examples","text":"<p>Examples are available that show how you would correctly set up the IAM Policy, IAM Role, Trust Policy and Secret Manager dependencies. See here</p>"},{"location":"guide/values-files-management/","title":"Managing Values Files","text":"<p>You should configure your instance of Smile CDR using separate Helm <code>values</code> files for different configurations.</p>"},{"location":"guide/values-files-management/#create-new-values-files","title":"Create New Values Files","text":"<p>It is generally recommended to create a new, empty values file rather than copying the default values.yaml file from the Helm chart. The default values file can be lengthy and may contain values that are not relevant or suitable for your specific deployment.</p> <p>By starting with a fresh values file, you can customize your configuration to only include the values that you need to override. This can help to make your values file more concise and easier to manage. Additionally, starting from a blank file allows you to ensure that your deployment is not impacted by future updates to the default values file, which could potentially cause issues if you are using an older version.</p> <p>Creating your own values file from scratch gives you greater control and flexibility over your Helm chart deployment. It can help to ensure that your deployment is secure and stable, as you have the ability to carefully consider and set the values that are most relevant to your specific needs.</p>"},{"location":"guide/values-files-management/#organizing-values-files","title":"Organizing Values Files","text":"<p>It is a common practice to put all Helm Chart configurations into a single values file and provide that to the <code>helm upgrade</code> command.</p> <p>Using multiple values files can be a more efficient way to manage configurations, particularly in complex environments. This approach can help to avoid having a single, large values file that may be difficult to read and maintain. By dividing the configuration into smaller, more focused files, it can be easier to manage and update the settings as needed.</p> <p>To use multiple values files, you would simply provide multiple <code>-f valuesfle.yaml</code> options on the <code>helm upgrade</code> command.</p>"},{"location":"guide/values-files-management/#multiple-environments","title":"Multiple Environments","text":"<p>When deploying an application, it is often necessary to consider multiple environments, such as dev, uat, and prod. While it is possible to create a separate configuration for each environment, this approach can lead to repetition and duplication of settings.</p> <p>This can be problematic, as it can result in configuration drift between environments. If there are changes that need to be made to the configuration, it can be challenging to ensure that the updates are applied consistently across all environments.</p> <p>TODO Insert Diagram explaining Drift</p> <p>A more effective approach may be to use a base configuration, with per-environment overlays. This allows you to define a set of common configuration settings that apply to all environments, while also allowing you to specify any environment-specific settings as needed. This can help to minimize repetition and ensure that the configuration is consistent across all environments.</p> <p>This can be easily achieved using multiple directories for the different environments like so:</p> <p>TODO Insert Diagram showing multiple configurations in a per-directory model</p>"},{"location":"guide/values-files-management/#modular-configurations","title":"Modular Configurations","text":"<p>Using multiple values files can also be a useful way to create modular units of configuration that can be easily included or excluded in your environment. This can help to make your configuration more flexible and adaptable to changing needs.</p> <p>For example, you might create a set of values files that represent different configurations or modules that have been fully tested and approved for use in your organization. These might include a base configuration file that defines the minimum requirements for running Smile CDR, as well as additional files for specific features or components, such as an R4 persistence module, a MongoDB persistence module, or an AWS Healthlake module.</p> <p>TODO Insert Diagram showing module files</p> <p>This approach allows you to build your configuration in a modular way, which can be more manageable and easier to maintain. It also gives you the flexibility to selectively include or exclude certain modules as needed, depending on the specific requirements of your environment.</p>"},{"location":"guide/values-files-management/#flexible-solution","title":"Flexible Solution","text":"<p>When it comes to managing values files, there isn't a single \"right\" way to do it - the approach that works best will depend on specific needs and organizational standards.</p> <p>Although the above techniques can be helpful for keeping things organized and efficient, they may not be right for you or your organization. You should use a technique that works for your team and organization. If this means using a single large values file per environment or some other technique, then that is fine.</p> <p>The aim here is to find a solution that helps you maintain a stable, well-organized configuration.</p>"},{"location":"guide/smilecdr/","title":"Smile CDR Helm Chart User Guide","text":"<p>This guide covers configuration details for the Smile CDR Helm Chart</p>"},{"location":"guide/smilecdr/#smile-cdr-configuration-options","title":"Smile CDR Configuration Options","text":"<p>Please note that for details on the Smile CDR configuration options, you should consult the official product documentation on the Smile CDR website</p>"},{"location":"guide/smilecdr/cdr-license/","title":"Configuring Smile CDR License","text":"<p>Some components of Smile CDR require an additional license in order for them to function. In order to enable your license in a secure fashion, please follow this guide.</p>"},{"location":"guide/smilecdr/cdr-license/#prerequisites","title":"Prerequisites","text":"<p>The recommended way to configure your Smile CDR license is by importing it from a secure secrets vault. Currently this chart only supports AWS Secrets Manager using the Secrets Store CSI Driver. For more information on these pre-requisites and how secrets are handled in this chart, please refer to the Secrets Handling section of this guide.</p>"},{"location":"guide/smilecdr/cdr-license/#configure-using-secrets-store-csi","title":"Configure using Secrets Store CSI","text":""},{"location":"guide/smilecdr/cdr-license/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>The Smile CDR License is a regular JWT token. When storing it in an AWS Secrets Manager Secret, store it under the <code>jwt</code> key in the JSON object.</p>"},{"location":"guide/smilecdr/cdr-license/#values-file","title":"Values File","text":"<p>Add a snippet in your values file like so</p> <pre><code>license:\n  type: sscsi\n  provider: aws\n  secretArn: arn:aws:secretsmanager:us-east-1:111111111111:secret:my-smile-license\n</code></pre>"},{"location":"guide/smilecdr/cdr-license/#alternative-methods","title":"Alternative methods","text":"<p>If you do not wish to use the above method, you can also include your license file using the existing method for including files as described in the Including Extra Files section of this guide.</p> <p>If you use this method, you will also need to update your module configuration so that <code>license.config.jwt_file</code> points to the correct file.</p> <p>This chart does not currently support providing the licence directly via Kubernetes <code>Secret</code> objects.</p> <p>WARNING: Be aware that your license should be considered sensitive material. If you do use this method, your license may show up in your infrastructure logs that are used to provision this Helm Chart.</p>"},{"location":"guide/smilecdr/cdrnode/","title":"Smile CDR Cluster Configuration","text":"<p>Smile CDR is designed to be installed with flexible cluster configurations, as per the documentation here.</p> <p>Using this Helm Chart, it is possible to configure your Smile CDR cluster with a flexible architecture design:</p> <ul> <li> <p>Single Node All modules are contained in a single Node configuration.</p> </li> <li> <p>Multi Node Modules can be split amongst an arbitrary number of Node configurations.</p> </li> </ul> <p>When using a multi node architecture, each Node will be deployed using separate Kubernetes resources. All configurations, such as the number of replicas, resource requests &amp; limits, probes, autoscaling configurations, mapped files and CDR module configurations, can be configured separately for each Node.</p>"},{"location":"guide/smilecdr/cdrnode/#single-node-design","title":"Single Node Design","text":"<p>By default, Smile CDR will be installed with a single-node configuration. No special actions are required to run Smile CDR with this configuration.</p>"},{"location":"guide/smilecdr/cdrnode/#overriding-default-nodeid","title":"Overriding Default NodeID","text":"<p>When using the default configuration, a single Smile CDR Node will be created with a <code>NodeID</code> of <code>Masterdev</code>. If required this can be changed using one of two methods.</p> <p>Method 1 - Override name in default configuration</p> <p>This is the default configuration which results in <code>node.id</code> being set to <code>Masterdev</code> <pre><code>cdrNodes:\n  masterdev:\n    name: Masterdev\n</code></pre></p> <p>To override this name, simply alter the <code>name:</code> field in your <code>values.yaml</code> file.</p> <p>The following configuration results in <code>node.id</code> being set to <code>MyNodeID</code> <pre><code>cdrNodes:\n  masterdev:\n    name: MyNodeID\n</code></pre></p> <p>Method 2 - Disable default configuration</p> <p>The above mentioned settings use the <code>cdrNode.masterdev</code> YAML dictionary that is predefined in the default <code>values.yaml</code> file. This dictionary contains a number of default node settings as follows: <pre><code>cdrNodes:\n  masterdev:\n    name: Masterdev\n    enabled: true\n    config:\n      locked: true\n      troubleshooting: false\n    security:\n      strict: false\n</code></pre></p> <p>If you wish to start with a fresh node configuration, you can disable the default <code>cdrNodes.masterdev</code> and configure your own item that does not inherit any default values.</p> <p>Add the following snippet to your <code>values.yaml</code> file to disable the default configuration and define your own node using the <code>mynode</code> key, with a <code>node.id</code> of <code>MyNode</code> <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  mynode:\n    enabled: true\n    name: MyNode\n</code></pre></p>"},{"location":"guide/smilecdr/cdrnode/#multi-node-design","title":"Multi Node Design","text":"<p>WARNING: Multi-Node configuration using this Helm Chart is still a new and developing feature. It's not recommended at this time to use it in a production environment and should only be used for evaluation purposes.</p> <p>Building on the section above, it's simple to create a multi-node configuration by adding multiple node specifications under the <code>cdrNodes</code> map entry.</p>"},{"location":"guide/smilecdr/cdrnode/#sample-architecture","title":"Sample Architecture","text":"<p>For this example, we will replicate the Sample Architecture described in the Smile CDR documentation here</p> <p></p>"},{"location":"guide/smilecdr/cdrnode/#basic-configuration","title":"Basic Configuration","text":"<p>The below example configuration will install a Smile CDR cluster with two nodes, as per the above diagram.</p> <p>Note: Do NOT use this configuration, it is merely used to demonstrate the mechanisms available for defining Smile CDR Nodes. Use it only as a guideline for configuring your own multi-node cluster.</p> <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  admin:\n    name: AdminNode\n    enabled: true\n    modules:\n      clustermgr:\n        # AdminNode overrides for Cluster Manager Module...\n      admin_json:\n        # JSON Admin API Module Spec...\n      admin_web:\n        # Web Admin Console Module Spec...\n      # Disabling unused default modules.\n      fhir_endpoint:\n        enabled: false\n      fhirweb_endpoint:\n        enabled: false\n      persistence:\n        enabled: false\n      subscription:\n        enabled: false\n      smart_auth:\n        enabled: false\n      package_registry:\n        enabled: false\n      audit:\n        enabled: false\n      license:\n        enabled: false\n      transaction:\n        enabled: false\n  fhir:\n    name: FhirNode\n    enabled: true\n    resources:\n      requests:\n        cpu: \"4\"\n      limits:\n        memory: 8Gi\n    modules:\n      useDefaultModules: false\n      clustermgr:\n        # FhirNode overrides for Cluster Manager Module...\n      persistence:\n        # Persistence Module Spec...\n      fhir_endpoint:\n        # Fhir Endpoint Module Spec...\n\n# Any configurations in the root context will be used as defaults by all enabled\n# nodes in `cdrNodes`. Configurations in `cdrNodes` will have priority, so this\n# is a useful mechanism for defining global defaults to reduce config duplication.\nmodules:\n  clustermgr:\n    # Global Cluster Manager Module Spec...\n  local_security:\n    # Global Security Module Spec...\n\nresources:\n  requests:\n    cpu: \"1\"\n  limits:\n    memory: 4Gi\n</code></pre> <p>Various concepts from the above will be covered in the below sections.</p>"},{"location":"guide/smilecdr/cdrnode/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>When defining a Node as per above, the Helm Chart will merge configurations from multiple locations to determing the correct value for any given setting. This applies to any of the Helm Chart settings that can be set in your <code>values.yaml</code> file.</p> <p>Configuration values are effectively determined by using the first entry found when looking in the following locations in order:</p> <ol> <li>Local Node spec in <code>cdrNodes</code> entry</li> <li>Values file root context</li> <li>Default values file</li> </ol> <p>The result of this is that any settings in the root context of the values file can be used as a mechanism for defining global defaults.</p>"},{"location":"guide/smilecdr/cdrnode/#example-explanation","title":"Example Explanation","text":"<p>In the example configuration above, we can see that this concept was utilized for specifying resource usage quotas.</p> <ul> <li>The root context defined <code>resources.requests.cpu: 1</code> and <code>resources.limits.memory: 4Gi</code>. These values become the default for all nodes</li> <li>The FhirNode overrides these values to <code>4</code> and <code>8Gi</code> respectively, thus overriding the global defaults.</li> <li><code>replicaCount</code> has neither been defined in the root context, nor in the FhirNode Spec. As such, the value for <code>replicaCount</code> will be determined from the default values file.</li> <li>There are modules defined both in the root context, as well as the Node specs under <code>cdrNodes</code>. This will be discussed in the section below.</li> </ul>"},{"location":"guide/smilecdr/cdrnode/#per-node-module-definitions","title":"Per-node Module Definitions","text":"<p>When using a multi-node configuration such as above there are multiple ways that you can configure the modules for each node.</p> <ul> <li>Use default modules - Although this may work, do not do this, as all nodes would have the same set of modules, negating the purpose of having a multi-node cluster in the first place ;)</li> <li>Use default modules, but disable modules that are not required on a given node. This is a reasonable option if you wish to use the default module definitions. It could get confusing if you have a lot of module definitions as it may become unclear which modules are defined where.</li> <li>Disable default modules and define all required modules yourself. This is probably the most complicated solution, but offers improved manageability as all your modules will be defined in one location.</li> </ul> <p>In either of the options above, the same inheritance process will be used to determine the final module congfiguration.</p> <p>Module Inheritance with Default Modules Enabled</p> <p>If you are using the default modules, module configurations will be determined in the following order:</p> <ol> <li>Modules section of Node spec in <code>cdrNodes</code> entry</li> <li>Modules section of root context</li> <li>Default Modules file</li> </ol> <p>Module Inheritance with Default Modules Disabled</p> <p>If you have disabled the default modules, module configurations will be determined in the following order:</p> <ol> <li>Modules section of Node spec in <code>cdrNodes</code> entry</li> <li>Modules section of root context</li> </ol>"},{"location":"guide/smilecdr/cdrnode/#example-explanation_1","title":"Example Explanation","text":"<p>In the example configuration above, we can see that both of these methods were demonstrated.</p> <p>AdminNode uses method 1, leaving default modules enabled.</p> <ul> <li>AdminNode leaves default modules enabled, but explicitly disables any modules that are not required in the node</li> <li>AdminNode uses global overrides for the <code>clustermgr</code> and <code>local_security</code> modules.</li> <li>AdminNode overrides values for the <code>clustermgr</code>, <code>admin_json</code> and <code>admin_web</code> modules locally.</li> </ul> <p>AdminNode uses method 2, disabling the default modules.</p> <ul> <li>FhirNode disables default modules.</li> <li>FhirNode uses global defaults for the <code>clustermgr</code> and <code>local_security</code> modules.</li> <li>FhirNode overrides values for the <code>clustermgr</code>, <code>persistence</code> and <code>fhir_endpoint</code> modules locally.</li> </ul> <p>In reality, you would likely choose one option or the other for consistency. Both were used here just for demonstration purposes.</p>"},{"location":"guide/smilecdr/cdrnode/#multi-node-considerations","title":"Multi Node Considerations","text":"<p>When running Smile CDR in a multi-node configuration, there are some things to consider. Please study the documentation for designing a cluster</p>"},{"location":"guide/smilecdr/cdrnode/#clustermgr-module-configuration","title":"ClusterMgr Module Configuration","text":"<p>It is important that the configuration for the cluster manager module is mostly the same amongst the different nodes.</p> <p>Note: Despite the clustermgr configuration being identical, the Database configuration may differ slightly. See the Database Configuration section below.</p>"},{"location":"guide/smilecdr/cdrnode/#batch-job-visibility","title":"Batch Job Visibility","text":"<p>As per the documentation here, the Sample Architecture above will not have the ability to display Batch jobs in the Web Admin Console.</p> <p>A solution to this is to also configure any <code>persistence</code> modules in your AdminNode (Or any such node that has the <code>admin_web</code> module configured). If doing this, take special note of the considerations mentioned in the linked docs.</p>"},{"location":"guide/smilecdr/cdrnode/#database-configuration","title":"Database Configuration","text":"<p>If multiple modules are configured to point to the same database, consider the following:</p> <ul> <li>Only one node should have the <code>schema_update_mode</code> set to <code>UPDATE</code>. All others should have it set to <code>NONE</code></li> <li>When configuring multiple nodes/modules to point to the same persistence database (e.g. for viewing Batch Jobs as mentioned above) then:<ul> <li><code>suppress_scheduled_maintenance_jobs</code> should be set to true on all but one node</li> <li><code>read_only_mode.enabled</code> should be set to true on the AdminNode</li> <li>The <code>maxidle</code> and <code>maxtotal</code> db connections can be reduced on the AdminNode</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/cdrnode/#readiness-probes","title":"Readiness Probes","text":"<p>With a default single node configuraton of Smile CDR, the Kubernetes readiness probe is set up to use the healthcheck of the FHIR Endpoint module. Although this is a reasonable compromise when it comes to the question of \"What do I monitor\", it falls short if you need to ensure that the Web Admin Console is always available.</p> <p>With a multi-node configuration this issue is solved as you will now have a separate readiness probe configuration for each node. This means your AdminNode could use the Web Admin Console healthcheck and your FhirNode could use the healthcheck of the FHIR Endpoint.</p> <p>With such a configuration, both the Web Admin Console and the FHIR Endpoint will gain resilience and self-healing benefits from the Kubernetes control plane.</p>"},{"location":"guide/smilecdr/cdrnode/#multi-node-example-configuration","title":"Multi Node Example Configuration","text":"<p>Below is a realistic example configuration for a multi-node cluster. It's based on the Sample Architecture further up in this page, but includes the following:</p> <ul> <li>Realistic comprehensive module configurations using global defaults</li> <li>Includes <code>audit</code>, <code>transaction</code> &amp; <code>license</code> modules</li> <li>Includes ability to view Batch jobs from the Web Admin Console</li> </ul>"},{"location":"guide/smilecdr/cdrnode/#my-multi-node-valuesyaml","title":"<code>my-multi-node-values.yaml</code>","text":"Click to expand <pre><code>cdrNodes:\n  masterdev:\n    enabled: false\n  admin:\n    name: AdminNode\n    enabled: true\n    modules:\n      clustermgr:\n        config:\n          db.schema_update_mode: UPDATE\n      audit:\n        config:\n          db.schema_update_mode: UPDATE\n      transaction:\n        config:\n          db.schema_update_mode: UPDATE\n          # transactionlog.show_request_body.enabled: true\n      persistence:\n        config:\n          suppress_scheduled_maintenance_jobs: true\n          read_only_mode.enabled: true\n          db.connectionpool.maxidle: 2\n          db.connectionpool.maxtotal: 4\n      admin_web:\n        name: Web Admin\n        enabled: true\n        type: ADMIN_WEB\n        enableReadinessProbe: true\n        service:\n          enabled: true\n          svcName: admin-web\n          hostName: default\n        requires:\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: \"\"\n          port: 9100\n          # tls.enabled: false\n          # https_forwarding_assumed: true\n          # respect_forward_headers: true\n      admin_json:\n        name: JSON Admin Services\n        enabled: false\n        type: ADMIN_JSON\n        service:\n          enabled: true\n          svcName: admin-json\n        requires:\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: json-admin\n          port: 9000\n          tls.enabled: false\n          anonymous.access.enabled: true\n          security.http.basic.enabled: true\n          https_forwarding_assumed: true\n          respect_forward_headers: true\n  fhir:\n    name: FHIRNode\n    enabled: true\n    modules:\n      persistence:\n        config:\n          db.schema_update_mode: UPDATE\n      fhir_endpoint:\n        name: FHIR Service\n        enabled: true\n        type: ENDPOINT_FHIR_REST\n        enableReadinessProbe: true\n        service:\n          enabled: true\n          svcName: fhir\n          hostName: default\n        requires:\n          PERSISTENCE_ALL: persistence\n          SECURITY_IN_UP: local_security\n        config:\n          context_path: fhir_request\n          port: 8000\n\nmodules:\n  useDefaultModules: false\n  clustermgr:\n    name: Shared Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  NONE\n      audit_log.db.always_write_to_clustermgr: false\n      audit_log.request_headers_to_store: Content-Type,Host\n      transactionlog.enabled: false\n      retain_transaction_log_days: 7\n\n  local_security:\n    name: Shared Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n\n  audit:\n    name: Shared Audit DB Config\n    enabled: true\n    type: AUDIT_LOG_PERSISTENCE\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode:  NONE\n\n  license:\n    name: Shared License Module Config\n    type: LICENSE\n    enabled: true\n\n  transaction:\n    name: Shared Transaction Log DB\n    enabled: true\n    type: TRANSACTION_LOG_PERSISTENCE\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.username: \"#{env['DB_USER']}\"\n      db.password: \"#{env['DB_PASS']}\"\n      db.schema_update_mode: NONE\n\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n      db.schema_update_mode: NONE\n</code></pre>"},{"location":"guide/smilecdr/cdrversions/","title":"Supported Smile CDR Versions","text":"<p>By default, this Helm Chart supports the latest published version of the Smile CDR docker image.</p>"},{"location":"guide/smilecdr/cdrversions/#important-set-your-image-tag","title":"Important! Set Your Image Tag","text":"<p>If you need to pin to a specific version of Smile CDR, be sure to specify the appropriate value for <code>image.tag</code> in your values file. If you fail to do this, your deployment may get unexpectedly upgraded when using a newer version of the Helm Chart.</p> <p>Set <code>image.tag</code> to your required version</p>"},{"location":"guide/smilecdr/cdrversions/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>image:\n  tag: \"2023.05.R02\"\n</code></pre> <p>Warning Pre-release versions of this Helm Chart may default to pre-release versions of Smile CDR. Always update the image tag when using this Helm Chart to update an existing installation that is running a previous version of Smile CDR.If you do not perform this step, Smile CDR may automatically upgrade your database to the latest version, which may be an irreversible step!</p>"},{"location":"guide/smilecdr/cdrversions/#current-smile-cdr-version","title":"Current Smile CDR Version","text":""},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202308","title":"Smile CDR <code>2023.08</code>","text":"<p>Versions <code>v1.0.0-pre.92</code> and newer of the chart support the latest production release of Smile CDR - <code>2023.08.R01</code> and above.</p> <p>This version does not contain any changes that affect the Helm Chart.</p> <p>Please refer to the Smile CDR changelog for more information on feature changes.</p>"},{"location":"guide/smilecdr/cdrversions/#version-support-table","title":"Version Support Table","text":"<p>For each Smile CDR version, this table shows the Min and Max Helm Chart version that officially support it.</p> <p>Older Smile CDR versions will not work beyond the chart version in the Extra column - see the notes.</p> Smile CDR Min Max Extra Notes <code>2023.11.*</code> tbd tbd tbd <code>2023.08.R01</code> <code>v1.0.0-pre.92</code> tbd tbd <code>2023.05.R02</code> <code>v1.0.0-pre.80</code> <code>v1.0.0-pre.91</code> tbd Note 1 <code>2023.02.R03</code> <code>v1.0.0-pre.52</code> <code>v1.0.0-pre.78</code> tbd Note 2"},{"location":"guide/smilecdr/cdrversions/#notes","title":"Notes","text":"<p>Note 1 If using older version of Smile CDR with newer version of the Helm Chart, ensure that you have the correct <code>image</code> value provided.</p> <p>Note 2 Unsupported beyond the 'Max' version. If using older version of Smile CDR with newer version of the Helm Chart, please see the section below for any compatibility considerations.</p>"},{"location":"guide/smilecdr/cdrversions/#upgrading","title":"Upgrading","text":"<p>When upgrading from older versions of Smile CDR, there may be some additional required steps.</p> <p>Changes across multiple versions may be cumulative, so you should perform any upgrade steps one major version (Of Smile CDR) at a time.</p>"},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202305-helm-chart-version-v100-pre91","title":"Smile CDR <code>2023.05</code> (Helm Chart version &lt; <code>v1.0.0-pre.91</code>)","text":"<ul> <li>There are currently no known required changes.</li> </ul>"},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202302-helm-chart-version-v100-pre78","title":"Smile CDR <code>2023.02</code> (Helm Chart version &lt; <code>v1.0.0-pre.78</code>)","text":"<ul> <li>Provision a database for the <code>transaction</code> module, or disable the new module in your values file.</li> <li>Configure the transaction module credentials in your values file.</li> <li>Update your FHIR Endpoint module type to <code>ENDPOINT_FHIR_REST</code>, as the <code>ENDPOINT_FHIR_REST_R4</code> module has been deprecated</li> <li>Update the dependency type from <code>PERSISTENCE_R4</code> to <code>PERSISTENCE_ALL</code></li> </ul>"},{"location":"guide/smilecdr/cdrversions/#previous-smile-cdr-versions","title":"Previous Smile CDR Versions","text":""},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202305","title":"Smile CDR <code>2023.05</code>","text":"<p>Versions <code>v1.0.0-pre.80</code> and newer of the chart support the latest production release of Smile CDR - <code>2023.05.R01</code> and above.</p> <p>This version includes some significant changes from previous versions that may cause some incompatibility. The following changes have been included in this version of the Helm Chart.</p> <ul> <li>New Transaction logging module introduced that utilizes separate database</li> <li>Fhir Endpoint module now uses <code>ENDPOINT_FHIR_REST</code> instead of <code>ENDPOINT_FHIR_REST_R4</code>. This is a backwards-compatible change.</li> </ul> <p>If you wish to continue to use Smile CDR <code>2023.05.*</code> with Versions <code>v1.0.0-pre.92</code> or greater, then you will need to do the following before using the newer chart: * Include the appropriate image tag in <code>values.image.tag</code> (e.g. <code>2023.05.R02</code>)</p>"},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202302","title":"Smile CDR <code>2023.02</code>","text":"<p>If you wish to continue to use Smile CDR <code>2023.02.*</code> with Versions <code>v1.0.0-pre.80</code> or greater, then you will need to do the following before using the newer chart:</p> <ul> <li>Disable the <code>transaction</code> module</li> <li>Revert the <code>fir_endpoint</code> module <code>type</code> to <code>ENDPOINT_FHIR_REST_R4</code></li> <li>Include the appropriate image tag in <code>values.image.tag</code> (e.g. <code>2023.02.R03</code>)</li> </ul> <p>Versions <code>v1.0.0-pre.52</code> to <code>v1.0.0-pre.78</code> of the chart natively support the previous production release of Smile CDR - <code>2023.02.R03</code>.</p> <p>This version included some major changes from previous versions that cause some incompatibility.</p> <ul> <li>Pod is now configured with an enhanced security posture<ul> <li>Containers now run as non-root user</li> <li>Root filesystem is mounted read-only</li> <li>Extra ephemeral volumes are used for certain directories that need write access   e.g. logs, tmp</li> </ul> </li> <li>New Audit logging mechanism introduced that optionally utilizes separate database</li> <li>Licencing module introduced</li> </ul>"},{"location":"guide/smilecdr/cdrversions/#smile-cdr-202211","title":"Smile CDR <code>2022.11</code>","text":"<p>It is not recommended to run Smile CDR versions <code>2022.11</code> and earlier with this Helm Chart due to a number of security changes. To do so, you will need to make some configuration changes in your Helm values file to disable some of the security features.</p> <ul> <li>Disable running as non-root user</li> <li>Disable Audit and License modules</li> <li>Set the image tag</li> </ul> <p>As with any version of Smile CDR that you use with this Helm Chart, part of the deployment process is developing a set of Helm Values that works for your particular use case. This does not change that, but these settings may need to be added/changed if you already have a versions file that you have developed.</p>"},{"location":"guide/smilecdr/cdrversions/#disable-running-as-non-root-user","title":"Disable Running as non-root User","text":"<p>In order to do this, we must override the <code>podSecurityContext</code> that is defined in the default <code>values.yaml</code> file in the Helm Chart.</p>"},{"location":"guide/smilecdr/cdrversions/#default-valuesyaml","title":"Default <code>values.yaml</code>","text":"<pre><code>securityContext:\n  capabilities:\n    drop:\n    - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  privileged: false\n  allowPrivilegeEscalation: false\n</code></pre>"},{"location":"guide/smilecdr/cdrversions/#required-additions-to-my-valuesyaml-to-re-enable-running-as-root","title":"Required additions to <code>my-values.yaml</code> to re-enable running as root","text":"<pre><code>securityContext:\n  runAsNonRoot: false\n  runAsUser: 0\n</code></pre> <p>Note - You do not need to disable the capabilities dropping or the read only root file-system as previous versions of Smile CDR still function with these security enhancements in place.</p>"},{"location":"guide/smilecdr/cdrversions/#disable-unsupported-modules","title":"Disable Unsupported modules","text":"<p>In order to disable all default modules, you need to set <code>useDefaultModules</code> to false. See here for more info.</p> <p>Then you need to explicityly define ALL modules that you need to configure.</p>"},{"location":"guide/smilecdr/cdrversions/#my-valuesyaml_1","title":"<code>my-values.yaml</code>","text":"<pre><code>modules:\n  useDefaultModules: false\n  clustermgr:\n    ...\n</code></pre> <p>Note See examples section for a complete configuration showing this.</p>"},{"location":"guide/smilecdr/database/","title":"Database Configuration","text":"<p>To use this chart, you must configure a database. There are two ways to do this:</p> <ul> <li>Use or provision an external database (or databases) using existing techniques/processes in your   organisation. Any external database can be referenced in this chart and Smile CDR will be configured   to use it.</li> <li>As a quick-start convenience, support has been included to provision a PostgreSQL cluster locally in   the Kubernetes cluster using the CrunchyData PostreSQL Operator. When enabling this option, the   database(s) will be automatically created and Smile CDR will be configured to connect to it.</li> </ul> <p>If you do not specify one or the other, the chart will fail to render any output and will return a descriptive error instead</p> <p>WARNING - Do not use built-in H2 database: Due to the ephemeral and stateless nature of Kubernetes Pods, there is no use case where it makes sense to provision Smile CDR using the internal H2 database. You are free to configure your persistence module to do so, but every time the Pod restarts, it will start with an empty database and will perform a fresh install of Smile CDR. In addition to this, if you were to configure multiple replicas, each Pod would appear as its own distinct Smile CDR install. As such, you should not configure Smile CDR in this fashion and you must instead provision some external database.</p>"},{"location":"guide/smilecdr/database/#referencing-externally-provisioned-databases","title":"Referencing Externally Provisioned Databases","text":"<p>To reference a database that is external to the cluster, you will need:</p> <ul> <li>Network connectivity from the K8s cluster to your database.</li> <li>A secret containing the connection credentials in a structured Json format.</li> <li>It is common practice to include all connection credentials in DB secrets, this way it becomes simple   to manage the database without having to reconfigure Smile CDR. e.g. when 'restoring' an RDS instance, the   DB cluster name will typically change. By keeping these details inside the secret then any such change will be automatically applied without reconfiguring. See   here   for info on the schema used by AWS for this purpose. Note that an app restart will be required to pick up the new secret value.</li> <li>The secret can be a Kubernetes <code>Secret</code> object that you provision through some external mechanism, or it can be a secret in a   secure secrets vault. The latter is the preferred option for increased security and the ability to easily   rotate credentials. At this time, the only supported secrets vault is AWS Secrets Manager, using the Secrets Store CSI Driver.   See the Secrets Handling section for more info on this.</li> </ul> <p>If using AWS Secrets Manager, set the <code>credentials.type</code> to <code>sscsi</code> and <code>credentials.provider</code> to <code>aws</code>. If you have created a <code>Secret</code> object in Kubernetes through some othert mechanism, set <code>credentials.type</code> to <code>k8sSecret</code>.</p>"},{"location":"guide/smilecdr/database/#example-secret-configurations","title":"Example Secret Configurations","text":""},{"location":"guide/smilecdr/database/#using-aws-secret-json-structure","title":"Using AWS Secret Json structure","text":"<p>If you are using the above mentioned Json structure (i.e. <code>engine</code>, <code>host</code>, <code>username</code>, <code>password</code>, <code>dbname</code> and <code>port</code>) in your secret, then you should simply configure your secret as per the following yaml fragment. Those default keys will be used to extract the credentials.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n      module: clustermgr\n</code></pre> <p>Note: <code>clustermgrSecret</code> can be any friendly name, it's not important. The Kubernetes <code>Secret</code> resource will be named using this value.</p>"},{"location":"guide/smilecdr/database/#using-custom-secret-json-structure","title":"Using Custom Secret Json structure","text":"<p>If the Json keys in your secret are different than above, they can be overridden by specifying them with the <code>*Key</code> attributes to override the defaults.</p> <p>The below are just examples, to show how the Json keys can be overridden. You need to ensure that this matches the configuration of your secret and the keys it contains.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_1","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: clustermgrSecret\n      secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\n      module: clustermgr\n      urlKey: url-key-name\n      portKey: port-key-name\n      dbnameKey: dbname-key-name\n      userKey: user-key-name\n      passKey: password-key-name\n</code></pre> <p>If a required field is not included in the secret, you can specify it in your values file like so.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_2","title":"<code>my-values.yaml</code>","text":"<pre><code>- secretName: smilecdr\n  module: clustermgr\n  url: db-url # this is the actual url/hostname\n  port: 5432\n  dbname: dbname\n  user: username\n  passKey: password\n</code></pre> <p>NOTE: You cannot override the passKey value. The password will always come from the referenced secret.</p>"},{"location":"guide/smilecdr/database/#using-crunchydata-pgo-databases","title":"Using CrunchyData PGO Databases","text":"<p>This chart supports automatic creation of an in-cluster Postgres database using the CrunchyData Postgres Operator (PGO).</p> <p>In order to use this feature, you will need to ensure that your K8s cluster already has the operator installed (Operator installation instructions here).</p> <p>After the PGO operator is installed and configured in your Kubernetes cluster, you can enable this feature using the following yaml fragment for your database configuration:</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_3","title":"<code>my-values.yaml</code>","text":"<p><pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre> This will create a 2 instance HA PostgreSQL cluster, each with 1cpu, 2GiB memory and 10GiB storage. These defaults can be configured using <code>database.crunchypgo.config</code> keys.</p> <p>Backups are enabled by default as it's a feature of the Operator.</p>"},{"location":"guide/smilecdr/database/#configuring-multiple-databases","title":"Configuring Multiple Databases","text":"<p>This chart has support to use multiple databases. It is recommended (and in some cases, required) to configure Smile CDR this way, with a separate DB for the Cluster Manager, Audit logs, Transaction logs and for any Persistence Modules.</p> <p>NoteIf there is only one database configured then it will be used for all modules.</p>"},{"location":"guide/smilecdr/database/#module-autoconfiguration-of-databases","title":"Module Autoconfiguration of Databases","text":"<p>This Helm Chart will automatically configure any Smile CDR modules that use a database.</p> <p>If you configure multiple databases, the <code>module</code> key specified for each one is used to determine which Smile CDR module is using it. This key is important as it tells the Helm Chart which module uses this database.</p>"},{"location":"guide/smilecdr/database/#environment-variables","title":"Environment Variables","text":"<p>In the examples below, the <code>clustermgr</code>, <code>audit</code>, <code>transaction</code> and <code>persistence</code> modules will automatically have their own set of environment variables configured for DB connections as follows: <code>CLUSTERMGR_DB_*</code>, <code>AUDIT_DB_*</code>, <code>TRANSACTION_DB_*</code> and <code>PERSISTENCE_DB_*</code></p>"},{"location":"guide/smilecdr/database/#module-configuration","title":"Module Configuration","text":"<p>As the modules are configured automatically, you must NOT manually update your module configurations to point to these environment variable references.</p> <p>When a given module is configured, any <code>DB_*</code> placeholders in the Helm Values files are automatically replaced with the appropriate <code>&lt;modulename&gt;_DB_*</code> values.</p> <p>For example the default Cluster Manager values file has DB connection settings that look like this:</p> <p><pre><code>db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\n</code></pre> The Helm Chart will generate a Smile CDR properties file with automatically updated values to match the environment variabls, like so:</p> <pre><code>module.clustermgr.config.db.url      = jdbc:postgresql://#{env['CLUSTERMGR_DB_URL']}:#{env['CLUSTERMGR_DB_PORT']}/#{env['CLUSTERMGR_DB_DATABASE']}?sslmode=require\nmodule.clustermgr.config.db.password = #{env['CLUSTERMGR_DB_PASS']}\nmodule.clustermgr.config.db.username = #{env['CLUSTERMGR_DB_USER']}\n</code></pre> <p>This will happen automatically for any module that references <code>DB_*</code> environment variables.</p> <p>With multiple databases, the examples given above may look like this:</p>"},{"location":"guide/smilecdr/database/#external-multiple-database-example","title":"External Multiple Database Example","text":""},{"location":"guide/smilecdr/database/#my-valuesyaml_4","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  external:\n    enabled: true\n    credentials:\n      type: sscsi\n      provider: aws\n    databases:\n    - secretName: smilecdr\n      module: clustermgr\n    - secretName: smilecdr-audit\n      module: audit\n    - secretName: smilecdr-txlogs\n      module: transaction\n    - secretName: smilecdr-pers\n      module: persistence\n</code></pre>"},{"location":"guide/smilecdr/database/#crunchydata-pgo-multiple-database-example","title":"CrunchyData PGO Multiple Database Example","text":"<p>NoteThe CrunchyData PGO is a little different from the above as it uses the concept of 'users' in the configuration to configure multiple databases. That is why we are specifying multiple users here:.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_5","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    users:\n    - name: smilecdr\n      module: clustermgr\n    - name: smilecdr-audit\n      module: audit\n    - name: smilecdr-txlogs\n      module: transaction\n    - name: smilecdr-pers\n      module: persistence\n</code></pre>"},{"location":"guide/smilecdr/database/#crunchydata-database-name-suffixes","title":"CrunchyData Database Name Suffixes","text":"<p>When using CrunchyData PGO for experimenting with different Smile CDR configurations, it is often convenient to experiment with a fresh (empty) database, or flip back and forth between multiple database configurations.</p> <p>There are multiple ways this could be done:</p> <ul> <li>Deprovision/Reprovision the Db cluster - Destructive and time consuming</li> <li>Manually drop and recreate databases - Destructive and time consuming. Also requires DB tooling and connectivity.</li> <li>Reconfigure the database definitions in the <code>crunchypgo</code> section of the values file</li> </ul> <p>These methods have shortcomings that can slow down progress of testing initiatives.</p> <ul> <li>All of these options can be time consuming and error prone</li> <li>Any dropped and recreated databases will naturally lose any data</li> <li>Manually reconfiguring, although non-destructive, can get tedious and error-prone when dealing with configurations that have many databases, especially if you wish to 'flip' back and forth between different database configurations.</li> </ul> <p>As a convenience function, it is possible to quickly alter the database names, either individually or as an entire group. This can be done by using <code>dbName</code> or <code>dbSuffix</code>. The default suffix is <code>-db</code> and <code>-</code> will be prefixed on any provided suffix.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_6","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    users:\n    # Unaltered DB name will be `clustermgr-db`\n    - name: smilecdr\n      module: clustermgr\n\n    # Overridden DB name with disabled suffix will be `my-persistence-db-name`\n    - name: smilecdr-pers\n      module: persistence\n      dbName: my-persistence-db-name\n      dbSuffix: \"\"\n\n    # Default DB name with overridden suffix will be `audit-mydbsuffix`\n    - name: smilecdr-audit\n      module: audit\n      dbSuffix: \"-mydbsuffix\"\n</code></pre> <p>To reduce needing to specify the db suffix for multiple databases, the default suffix can be changed by setting <code>defaultDbSuffix</code> at the root level of the <code>crunchypgo</code> configuration as follows:</p> <p>NoteYou can still override the suffix for a single DB, as can be seen below with the <code>audit</code> and <code>persistence</code> modules below.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_7","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\n  crunchypgo:\n    enabled: true\n    internal: true\n    defaultDbSuffix: test2\n    users:\n    # DB name will be `clustermgr-test2`\n    - name: smilecdr\n      module: clustermgr\n\n    # DB name will be `audit-test1`\n    - name: smilecdr-audit\n      module: audit\n      dbSuffix: test1\n\n    # DB name will be `transaction-test2`\n    - name: smilecdr-txlogs\n      module: transaction\n\n    # DB name will be `persistence`\n    - name: smilecdr-pers\n      module: persistence\n      dbSuffix: \"\"\n</code></pre>"},{"location":"guide/smilecdr/envvars/","title":"Custom Environment Configuration","text":"<p>Sometimes you may have custom components in your Smile CDR deployment that need to have configurations provided to them.</p> <p>In Smile CDR, this can be done one in of two ways.</p> <ul> <li>Java System Property Substitution</li> <li>System Environment Variable Substitution</li> </ul> <p>You can read more about these methods in the official Smile CDR documentation here</p> <p>Currently, this Helm Chart only supports using the second mechanism - System Environment Variable Substitution</p>"},{"location":"guide/smilecdr/envvars/#passing-extra-environment-variables","title":"Passing Extra Environment Variables","text":"<p>In order to configure extra environment variables into the pod, use the <code>ExtraEnvVars</code> entry in your values file as follows:</p> <pre><code>extraEnvVars:\n- name: MYENVVARNAME\n  value: my-env-var-value\n- name: MYOTHERENVVARNAME\n  value: my-other-env-var-value\n</code></pre> <p>This is a list of objects that follow the same <code>env</code> schema as the Kubernetes <code>podSpec.containers</code> See here</p> <p>Note: Although it is possible to use this to add secrets using <code>env.valueFrom.secretKeyRef</code>, it's recommended to use one of the existing mechanisms if you wish to pass in secret data to your pod. See the Secrets section for more info on this.</p>"},{"location":"guide/smilecdr/envvars/#multi-node-configuration","title":"Multi-node configuration","text":"<p>If deploying Smile CDR in a multi-node configuration, you may wish to have different environment variables for the different CDR Nodes.</p> <p>Note: For more details on how to deploy Smile CDR with multi-node configurations, please refer to the CDR Nodes section.</p> <pre><code>cdrNodes:\n  node1:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node1-env-var-value\n  node2:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node2-env-var-value\n  node3:\n    extraEnvVars:\n    - name: MYENVVARNAME\n      value: my-node3-env-var-value\n    - name: GLOBALENVVARNAME\n      value: node3-overriden-global-value\n\nextraEnvVars:\n- name: GLOBALENVVARNAME\n  value: my-global-env-var-value\n</code></pre> <p>In the above configuration example:</p> <ul> <li>Each node gets its own set of extra environment variables.</li> <li>Each node gets the <code>GLOBALENVVARNAME</code> variable set to <code>my-global-env-var-value</code> except...</li> <li>Node3 has overriden the <code>GLOBALENVVARNAME</code> variable to <code>node3-overriden-global-value</code></li> </ul> <p>This allows for flexible configuration of extra environment variables in any Smile CDR configuration.</p>"},{"location":"guide/smilecdr/files/","title":"Including Extra Files","text":"<p>It is often required to include extra files into your Smile CDR instance. This could be to provide updated configuration changes (e.g. a modified <code>logback.xml</code>), provide <code>.js</code> scripts, <code>.jar</code> files and other libraries to extend the functionality of Smile CDR.</p> <p>Rather than having to build a custom Smile CDR container image to include these files, it is possible to include them using this Helm Chart.</p>"},{"location":"guide/smilecdr/files/#available-methods","title":"Available Methods","text":"<p>There are two mechanisms available to load files.</p> <ul> <li>Including files in the Helm deployment</li> <li>Pulling files from an external location</li> </ul> <p>Each of these mechanisms has its own advantages.</p>"},{"location":"guide/smilecdr/files/#choosing-which-method-to-use","title":"Choosing Which Method To Use","text":""},{"location":"guide/smilecdr/files/#helm-chart-method","title":"Helm Chart Method","text":"<p>Using the Helm Chart method is ideal when:</p> <ul> <li>The files are text based and under 1MiB in size<ul> <li>Config files and small scripts are good examples</li> <li>Not ideal for binary files, even if small</li> </ul> </li> <li>You do not have many files to add<ul> <li>Although there is no limit, your configuration will get very hard to manage if you use too many</li> <li>Between 5 &amp; 10 would be a good limit, but this is just a suggestion</li> </ul> </li> <li>You don't have a mechanism in place to stage the files somewhere (i.e. Amazon S3)<ul> <li>This method provides a simple deployment solution as it has no external dependencies</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/files/#external-pull-method","title":"External Pull Method","text":"<p>Using the External Pull method is ideal when:</p> <ul> <li>You have binary files or large files<ul> <li>Any file over 1MiB requires you use this method</li> </ul> </li> <li>You have many files<ul> <li>This mechanism will copy files recursively without clogging up your configuration</li> </ul> </li> <li>You are able to stage your files and file updates on Amazon S3<ul> <li>Currently only S3 is supported, but other external file sources will be added as required</li> </ul> </li> <li>You wish to pull files that are publically hosted (e.g. public <code>.jar</code> files)</li> </ul>"},{"location":"guide/smilecdr/files/#using-both-methods","title":"Using Both Methods:","text":"<p>Using both methods is possible too:</p> <ul> <li>If you had a set of <code>.jar</code> files and scripts being staged on S3, you could still add files using the Helm chart method if it makes for a simpler workflow</li> <li>Be wary that having it split up like this could make your configuration more confusing (i.e. \"Where was that file copied from again?\")</li> <li>Files copied using the Helm Chart method will take precedence over any files copied from an external source.</li> </ul>"},{"location":"guide/smilecdr/files/#using-the-helm-chart-method","title":"Using the Helm Chart Method","text":"<p>To pass in files using the Helm Chart, there are two things you need to do: 1. Use a Helm commandline option to load the file into the deployment 2. Reference and configure the file in your values file.</p>"},{"location":"guide/smilecdr/files/#include-file-in-helm-deployment","title":"Include File in Helm Deployment","text":"<p>To include a file in the deployment, use the following commandline option: <pre><code>helm upgrade -i my-smile-env --devel -f my-values.yaml --set-file mappedFiles.logback\\\\.xml.data=logback.xml smiledh/smilecdr\n</code></pre></p> <p>WARNING: Pay special attention to the escaping required to include the period in the filename. You need to use <code>\\\\.</code> when running this from a shell. This is just the way this works.</p> <p>This will encode the file and load it into the provided values under the <code>mappedFiles.logback.xml.data</code> key.</p>"},{"location":"guide/smilecdr/files/#include-file-in-values-file","title":"Include File in Values File","text":"<p>The included file also needs to be referenced from your values file so that the chart knows where to mount the file in the application's Pod: <pre><code>mappedFiles:\n  logback.xml:\n    path: /home/smile/smilecdr/classes\n</code></pre> As the result of the above, a <code>ConfigMap</code> will be created and mapped into the pod at <code>/home/smile/smilecdr/classes/logback.xml</code> using <code>Volume</code> and <code>VolumeMount</code> resources. If the content of the file is changed, then it will be automatically picked up on the next deployment. (See Automatic Deployment of Config Changes for more info on this)</p>"},{"location":"guide/smilecdr/files/#using-the-external-pull-method","title":"Using the External Pull Method","text":"<p>The external pull method can be used to pull files from Amazon S3 or from public websites that publish resources (e.g. Maven).</p>"},{"location":"guide/smilecdr/files/#how-it-works","title":"How It Works","text":""},{"location":"guide/smilecdr/files/#shared-volumes","title":"Shared Volumes","text":"<p>Pod-local shared volumes are used for the <code>classes</code> and <code>customerlib</code> directories so that the files can be copied there before the main Smile CDR container starts up.</p> <p>These volumes are only accessible to containers running inside the pods and are deleted when the pod is terminated so they are not accessible outside the pod's lifecycle. If the underlying Kubernetes node volume uses encrypted storage, then these volumes will also be encrypted.</p>"},{"location":"guide/smilecdr/files/#init-containers","title":"Init Containers","text":"<p>Kubernetes init containers are then used to pull files from S3, or some other location.</p> <p>It uses multiple Kubernetes init containers to synchronize and pull files to these shared volumes during pod startup.</p> <p>This feature has been implemented to support Amazon S3 and curl. Other mechanisms may be introduced in a future version of this chart.</p> <p>The init containers are auto-configured based on the provided <code>copyFiles</code> settings. They function as follows:</p> <p><code>init-sync-classes</code></p> <ul> <li>This container copies the default files from the classes directory from the Smile CDR base image to a <code>classes</code> shared volume that is local to the pod.</li> <li>The <code>init-pull-classes</code> container will overwrite any of these files with the same names.</li> <li>This is a required step if you wish to retain the default files. As such, it's enabled by default</li> <li>If you require a 'clean' <code>classes</code> directory, this step can be disabled using <code>copyFiles.classes.disableSyncDefaults: true</code>.<ul> <li>If disabled, you will need to provide all <code>classes</code> files that are required for Smile CDR to start up (With the exception of the config properties file which is generated by this Helm Chart).</li> </ul> </li> </ul> <p><code>init-sync-customerlib</code></p> <ul> <li>This container copies the default files from the customerlib directory from the Smile CDR base image to a <code>customerlib</code> shared volume that is local to the pod.</li> <li>When using the default Smile CDR image, no files will be copied as the directory is empty.</li> <li>If using a customised image with files preloaded into the <code>customerlib</code> directory, this step is necessary to prevent those files being clobbered. As such, it's enabled by default.</li> <li>If you require a 'clean' <code>customerlib</code> directory, this step can be disabled using <code>copyFiles.classes.disableSyncDefaults: true</code>.</li> </ul> <p><code>init-pull-classes-*</code></p> <ul> <li>These containers copy files from the specified location to the classes shared volume</li> <li>Currently they support pulling files from Amazon S3 or downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> </ul> <p><code>init-pull-customerlib-*</code></p> <ul> <li>This container copies files from the specified location to the customerlib shared volume</li> <li>Currently they support pulling files from Amazon S3 or downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> </ul>"},{"location":"guide/smilecdr/files/#s3-prerequisites","title":"S3 Prerequisites","text":"<p>To pass in files from an Amazon S3 bucket, you need the following prerequisites in place:</p> <ul> <li>An S3 bucket with:<ul> <li>A folder containing your <code>classes</code> files</li> <li>A folder containing your <code>customerlib</code> files</li> <li>Ideally, these should be in a higher level folder to control versioning<ul> <li>e.g. <code>v1</code>, <code>v2</code> or a <code>UID</code></li> </ul> </li> <li>Bucket should not be publically accessible<ul> <li>It will work with public buckets too, but this is a bad security practice</li> </ul> </li> <li>Bucket should use encryption<ul> <li>Again, it will work without, but it's good security practice to encrypt everything by default</li> </ul> </li> <li>The mechanism to copy the files into this bucket is out of the scope of this Helm Chart</li> </ul> </li> <li>Service Account must be enabled and configured to use IRSA. See here for more info on this</li> <li>The IAM Role used for the Service Account must have read access to the S3 bucket</li> </ul> Required IAM policy actions for S3 copyFile configurations <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjectsInBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::bucket-name\"]\n        },\n        {\n            \"Sid\": \"GetObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:GetObject\"], ## or s3:GetObjectVersion\n            \"Resource\": [\"arn:aws:s3:::bucket-name/*\"]\n        }\n    ]\n}\n</code></pre>"},{"location":"guide/smilecdr/files/#a-note-on-file-versioning","title":"A Note On File Versioning","text":"<p>Though not required, it is reccommended to include some versioning structure in your S3 bucket.</p> <p>While already running pods cannot be affected by this (As they have already copied their files), any new pods that start up (e.g in scaling or reconciliation events) may be adversely affected if files have been unexpectedly changed or deleted.</p> <p>By including a new version whenever a given set of files is updated, previous deployments of the application will remain unaffected. This is also beneficial during rollbacks as the previous set of files will remain.</p> <p>This does introduce challenges of file duplication and managing multiple old versions. As the number of files included is typically low, this should not be of huge concern.</p>"},{"location":"guide/smilecdr/files/#configure-helm-values-file","title":"Configure Helm Values File","text":"<p>To enable this feature, add the following snipped to your values file. Replace the bucket name and path to match your environment.</p> <pre><code>copyFiles:\n  classes:\n    sources:\n    # Copies files recursively from S3 to the classes directory\n    - type: s3\n      # disableSyncDefaults: true &lt;- Optional. Use with caution! (See above)\n      bucket: s3-bucket-name\n      path: /path-to/classes\n      # Example versioned locations.\n      # path: /v1/classes\n      # path: /v1.1/classes\n      # path: /v2/classes\n      # path: /&lt;sha256-of-file-content&gt;/classes &lt;- You could generate a sha256 hash of the entire file contents.\n      # path: /&lt;UID&gt;/classes &lt;- You could generate a unique UID for each new version\n  customerlib:\n    sources:\n    # Copies files recursively from S3 to the customerlib directory\n    - type: s3\n      bucket: s3-bucket-name\n      path: /path-to/customerlib-src\n    # Downloads a single file using curl to the customerlib directory (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n    - type: curl\n      fileName: elastic-apm/elastic-apm-agent-1.13.0.jar\n      url: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre> <p>Note: The Service Account configurations have been left out for clarity. Please refer to the Service Account guide for instructions on enabling IRSA and IAM roles.</p>"},{"location":"guide/smilecdr/hl7v2/","title":"HL7 v2.x Listening Endpoint Module","text":"<p>To use the HL7 v2.x Listening Endpoint module with this Helm Chart, special configuration is required. For info this module, please refer to the official Smile CDR documentation here</p> <p>This module supports 2 transport mechanisms:</p> <ul> <li><code>HL7_OVER_HTTP</code></li> <li><code>MLLP_OVER_TCP</code></li> </ul> <p>Currently, this chart only supports using the <code>HL7_OVER_HTTP</code> transport protocol. <code>MLLP_OVER_TCP</code> support may be added in the future.</p>"},{"location":"guide/smilecdr/hl7v2/#prerequisites","title":"Prerequisites","text":"<p>To use this module, you need to configure an additional DNS entry. This is because this module will only function using the root context path (i.e. <code>https://hl7endpoint.mydomain.com/</code>) which prevents it from running on the same hostname as the other Smile CDR endpoints.</p> <p>The DNS entry created for this module should point to the same load balancer that is used for the other Smile CDR endpoints.</p>"},{"location":"guide/smilecdr/hl7v2/#configuring-module","title":"Configuring Module","text":"<p>To configure the HL7 v2.x Listening Endpoint module to use the above domain, you need to add a <code>hostName</code> entry to the <code>service</code> section of your module definition.</p> <p>Use the following module definition to enable this module and ingress route.</p> <pre><code>modules:\n  hlendpoint:\n    name: hl7v2\n    enabled: true\n    type: ENDPOINT_HL7V2_IN\n    service:\n      enabled: true\n      svcName: hl7v2\n      hostName: hl7endpoint.mydomain.com\n    requires:\n      PERSISTENCE_ALL: persistence\n    config:\n      port: 8008\n      store_original_message: true\n      transport: HL7_OVER_HTTP\n</code></pre>"},{"location":"guide/smilecdr/ingress/","title":"Configuring Ingress","text":"<p>This chart enables flexible Kubernetes Ingress resource configuration, supporting multiple Ingress and IngressClass resources for versatile application traffic routing.</p> <p>Currently supported controllers include Nginx Ingress, AWS Load Balancer, and Azure Application Gateway.</p>"},{"location":"guide/smilecdr/ingress/#ingress-type","title":"Ingress Type","text":"<p>This chart uses a concept of ingress <code>Type</code> to determine what kind of Ingress Controller is being used. It should not be confused with the <code>IngressClass</code>.</p> <p>The following ingress types are currently supported:</p> <ul> <li><code>nginx-ingress</code></li> <li><code>aws-lbc-alb</code></li> <li><code>azure-appgw</code></li> </ul> <p>This setting is used to help automatically configure the <code>Service</code> and <code>Ingress</code> resources so that the configured Ingress Controller can configure infrastructure resources appropriately.</p> <p>The ingress type for any given ingress can be set with <code>ingresses.default.type: ingress-type</code>.</p> <p>NOTE: If migrating from Helm Chart versions older than <code>v1.0.0-pre.104</code>, you will need to adjust from the old config schema to the new one. This can be done by moving any configurations that were previously under <code>ingress.*</code> to <code>ingresses.default.*</code>. e.g. <code>ingress.type: aws-lbc-alb</code> would become <code>ingresses.default.type: aws-lbc-alb</code></p>"},{"location":"guide/smilecdr/ingress/#nginx-ingress","title":"Nginx Ingress","text":"<p>By default, this chart is configured to use the Nginx Ingress Controller. <code>ingresses.default.type</code> is already set to <code>nginx-ingress</code> so you do not need to do anything to use this Ingress Controller.</p> <p>The behavior of the Nginx Ingress controller differs based on the cloud provider being used. When used in conjunction with the AWS Load Balancer Controller, the Nginx Ingress will be backed by an AWS NLB (Network Load Balancer).</p> <p>The <code>Service</code> objects will be set as <code>ClusterIP</code> rather than <code>NodePort</code>. This increases the security stance of the deployment as Kubernetes does not expose these services externally to the cluster. All traffic comes from the Nginx Ingress Controller pod, directly to the application pods.</p>"},{"location":"guide/smilecdr/ingress/#dedicated-nginx-ingress","title":"Dedicated Nginx Ingress","text":"<p>By default, this option uses the <code>nginx</code> ingress class. If multiple ingresses all use the same default IngressClass, then they will share the same underlying NLB.</p> <p>If you need to use a dedicated (or multiple) NLBs for this deployment, you can do so by first creating any required Nginx Ingress Controllers with a different IngressClass name. You can then specify this ingress class with <code>ingresses.default.ingressClassNameOverride</code>.</p>"},{"location":"guide/smilecdr/ingress/#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>To directly use the AWS Load Balancer Controller set <code>ingresses.default.type</code> to <code>aws-lbc-alb</code>. By default, this option uses the <code>alb</code> ingress class.</p> <p>This automatically adds appropriate default <code>Ingress</code> annotations for the AWS Load Balancer Controller. The controller will then create an AWS ALB (Application Load Balancer).</p> <p>You will still need to add some extra annotations, such as <code>alb.ingress.kubernetes.io/certificate-arn</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#known-problems","title":"Known Problems","text":"<p>There is currently a problem with the AWS Load Balancer Controller configuration where the health checks do not function as expected. This is somewhat mitigated by the fact that the <code>Service</code> objects are using <code>NodePort</code>. This will be addressed in a future release of this chart.</p>"},{"location":"guide/smilecdr/ingress/#azure-application-gateway-controller","title":"Azure Application Gateway Controller","text":"<p>If you wish to use the Azure Application Gateway Controller, set <code>ingresses.default.type</code> to <code>azure-appgw</code>. By default, this option uses the <code>azure/application-gateway</code> ingress class.</p> <p>When using this method, the chart will automatically add <code>Ingress</code> annotations for the Azure Application Gateway Controller. The controller will then create an Azure Application Gateway to be used as ingress.</p> <p>You will still need to add some extra annotations, such as <code>appgw.ingress.kubernetes.io/appgw-ssl-certificate</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#multiple-ingress-resources","title":"Multiple Ingress Resources","text":"<p>This chart allows for configurations using an arbitrary number of ingress resources and ingress classes.</p> <p>This enables the implementation of architectures that require multiple routes for accessing for your environment. For example, you may require some services available publically while others, such as the Admin Web Console, may only permit access from a private network.</p>"},{"location":"guide/smilecdr/ingress/#default-ingress","title":"Default Ingress","text":"<p>In order to simplify deployment of certain architectures, this chart supports the concept of a 'default' Ingress resource.</p> <p>The default Ingress resource will be used for any Smile CDR modules that have not explicitly defined <code>service.ingresses</code> in their configuration.</p>"},{"location":"guide/smilecdr/ingress/#the-pre-defined-default-ingress-configuration","title":"The pre-defined Default Ingress Configuration","text":"<p>If no changes are made to the <code>ingresses</code> section of your Helm Values, a pre-defined Ingress Configuration is created, that effectively looks like this: <pre><code>ingresses:\n  default:\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n    ingressClassName: nginx\n</code></pre> With the above pre-defined ingress, any module with an endpoint-enabled service will have rules injected into the <code>Ingress</code> resource created by this configuration.</p>"},{"location":"guide/smilecdr/ingress/#the-defaultingress-setting","title":"The <code>defaultIngress</code> setting","text":"<p>This setting tells an Ingress resource to serve as the active default ingress. This setting can only be enabled for a single Ingress Configuration at a time.</p> <p>If you are defining a custom default Ingress Configuration and do not wish to use the pre-defined one, you need to disable it like so:</p> <pre><code>ingresses:\n  default:\n    enabled: false\n  myCustomDefaultIngress\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n    ...\n</code></pre> <p>NOTE: If there are no Ingress Configurations with this setting enabled, then any Smile CDR modules that HAVE NOT explicitly defined <code>service.ingresses</code> in their configuration will not be exposed externally to the K8s cluster.</p>"},{"location":"guide/smilecdr/ingress/#configuring-the-pre-defined-default-ingress-configuration","title":"Configuring the pre-defined Default Ingress Configuration","text":"<p>The pre-defined Ingress Configuration can be reconfigured as follows: <pre><code>ingresses:\n  default:\n    type: azure-appgw\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#disabling-the-pre-defined-default-ingress-configuration","title":"Disabling the pre-defined Default Ingress Configuration","text":"<p>If you are creating multiple custom Ingress Configurations, you may wish to disable the pre-defined default ingress. This can be done as follows: <pre><code>ingresses:\n  default:\n    enabled: false\n</code></pre></p> <p>NOTE: If you disable the default ingress, then you must define at least one alternative Ingress Configuration if you wish to allow access from outside the Kubernetes cluster.</p>"},{"location":"guide/smilecdr/ingress/#defining-custom-ingress-configurations","title":"Defining Custom Ingress Configurations","text":"<p>To allow for complex ingress architectures, you may define an arbitrary number of Ingress Configurations like so <pre><code>ingresses:\n  default:\n    enabled: false\n  myPrivateNginx:\n    enabled: true\n    type: nginx-ingress\n    defaultIngress: true\n  myPublicNginx:\n    enabled: true\n    type: nginx-ingress\n    ingressClassNameOverride: nginx-public\n\n# These two are just to demonstrate using the\n#  AWS Load Balancer Controller for Ingress\n\n  myPrivateALB:\n    enabled: true\n    type: aws-lbc-alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internal\n  myPublicALB:\n    enabled: true\n    type: aws-lbc-alb\n    annotations:\n      alb.ingress.kubernetes.io/scheme: internet-facing\n</code></pre> In the above example we are:</p> <ul> <li>Disabling the default Ingress Configuration (We do this, because we will be defining a new default Ingress Configuration)</li> <li>Creating a <code>myPrivateNginx</code> defult Ingress Configuration.</li> <li>Using <code>defaultIngress</code> creates the default Ingress resource. This will be used by any Smile CDR modules that do not explicitly define <code>service.ingresses</code>.</li> <li>The Ingress resource will use the Nginx Ingress Controller via the <code>ingressClass</code> resource named <code>nginx</code></li> <li>Creating a <code>myPublicNginx</code> Ingress Configuration.</li> <li>This ingress will not be used unless a module specifies that it should use this ingress.</li> <li>The Ingress resource will use the Nginx Ingress Controller via the <code>ingressClass</code> resource named <code>nginx-public</code>. This additional Nginx Ingress Controller will need to be deployed before trying to use this configuration.</li> </ul> <p>The following two are just to demonstrate how you would use the AWS Load Balancer Controller as an alternative mechanism.</p> <ul> <li>Creating a <code>myPrivateALB</code> Ingress Configuration.</li> <li>This will use the AWS Load Balancer Controller with an IngressClassName of <code>alb</code>.</li> <li>It uses AWS Load Balancer Controller annotations to set this ingress to use an internal only AWS Application Load Balancer.</li> <li>Creating a <code>myPublicALB</code> Ingress Configuration.</li> <li>This will use the Nginx Ingress controller with an IngressClassName of <code>alb</code>.</li> <li>It uses AWS Load Balancer Controller annotations to set this ingress to use an internet facing AWS Application Load Balancer.</li> </ul> <p>NOTE: There are no restrictions on mixing the <code>type</code> for Ingress Configurations. For example, you may have some Ingress Configurations use <code>nginx-ingress</code> and others use <code>aws-lbc-alb</code>.</p>"},{"location":"guide/smilecdr/ingress/#configure-ingress-for-modules","title":"Configure Ingress for Modules","text":"<p>Smile CDR modules will automatically be configured to use whichever ingress has the <code>defaultIngress</code> setting enabled.</p> <p>If you do not want a module to use the default Ingress resource, or if there is no default Ingress Configuration defined, then you need to explicitly configure the <code>service.ingresses</code> for any Smile CDR modules that need to be accessed from outside the Kubernetes cluster.</p> <p>To configure a Smile CDR module to use a specific ingress, specify it in the <code>moduleSpec.service.ingresses</code> map as follows: <pre><code>modules:\n  fhirweb_endpoint:\n    service:\n      hostName: myFhirwebHost.example.com\n      ingresses:\n        myPublicNginx:\n          enabled: true\n</code></pre> In the above example we are telling the module named <code>fhirweb_endpoint</code> to use the <code>myPublicNginx</code> ingress specified previously mentioned. With the sample 'Multiple Ingress' configurations provided in on this page, all Smile CDR modules will only be accessible via the <code>myPrivateNginx</code> ingress, except for the <code>fhirweb_endpoint</code> module which will be available via the public access route.</p> <p>NOTE: Although you can configure a module to use multiple Ingress resources, be careful doing this unless you are using a split-dns configuration that preserves the host name for all ingresses.</p> <p>Having an endpoint be accessible via multiple hostnames can cause issues with incorrect links to resources generated by the application. It's advisable to only have a given module be accessible via a single ingress/hostname.</p>"},{"location":"guide/smilecdr/ingress/#extra-annotations","title":"Extra Annotations","text":"<p>Depending on the ingress type you select, the chart will automatically add a set of default annotations that are appropriate for the ingress type being used.</p> <p>However, it is not possible for the chart to automatically include all annotations as some need to be specified in your configuration.</p> <p>To add any extra annotations, or override existing ones, include them in your values file like so:</p> <pre><code>ingresses:\n  default:\n    annotations:\n      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm::01234567890:certificate/abcdef\n      alb.ingress.kubernetes.io/inbound-cidrs: ['0.0.0.0/0']\n</code></pre> <p>or <pre><code>ingresses:\n  default:\n    annotations:\n      appgw.ingress.kubernetes.io/appgw-ssl-certificate: mysslcert\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#ingress-class-name","title":"Ingress Class Name","text":"<p>This chart uses the following default class names for the different ingress types</p> Selected <code>ingresses.default.type</code> Default <code>ingressClassName</code> <code>nginx-ingress</code> <code>nginx</code> <code>aws-lbc-alb</code> <code>alb</code> <code>azure-appgw</code> <code>azure/application-gateway</code> <p>If you have configured your Ingress Controller with a different <code>ingressClass</code> name, you can override it using <code>ingresses.default.ingressClassNameOverride</code>.</p> <p>For example, if you had a dedicated Nginx Ingress Controller with the <code>IngressClass</code> of <code>nginx-dedicated</code>, you would include it in your values file like so:</p> <pre><code>ingresses:\n  default:\n    ingressClassNameOverride: nginx-dedicated\n</code></pre>"},{"location":"guide/smilecdr/ingress/#disabling-ingress","title":"Disabling Ingress","text":"<p>In some scenarios, you may wish to disable external ingress for certain modules. For example, if you have a FHIR Rest Endpoint module that is behind a FHIR Gateway module, you may not want to expose the FHIR Rest endpoint externally to the cluster.</p> <p>In this case, you can disable the ingress for a given service like so: <pre><code>modules:\n  fhir_endpoint:\n    service:\n      ingresses:\n        default:\n          enabled: false\n</code></pre></p> <p>Configuring your module like this will prevent any rules from being added to the default Ingress resource that is generated.</p> <p>If your module is a FHIR Rest Endpoint module, the <code>base_url.fixed</code> setting will be automatically configured appropriately and there is no need for you set this in your <code>moduleSpec.config</code>.</p>"},{"location":"guide/smilecdr/ingress/#service-type","title":"Service Type","text":"<p>The appropriate type for the <code>Service</code> resources depend on which Ingress type is being used. The default <code>Service</code> created by this chart is <code>ClusterIP</code>. This is the preferred option as it does not expose the Services externally to the cluster.</p> <p>When using the AWS Load Balancer Controller, or Azure Application Gateway Controller, the service objects are instead set to <code>NodePort</code>.</p> <p>This can be overriden using <code>service.type</code> in your values file, but it is not recommended and may cause unpredictable behaviour.</p>"},{"location":"guide/smilecdr/install/","title":"Installing Smile CDR","text":""},{"location":"guide/smilecdr/install/#prepare-values-file","title":"Prepare Values File","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p> <p>Refer to the section on Values Files Management for more info on how to organise your values files. You can start out with one of the values files in the Examples section, or create your own from scratch using techniques from the configuration section.</p> <p>For the remainder of this section, we will assume the same values file that was used in the QuickStart guide.</p>"},{"location":"guide/smilecdr/install/#install-the-helm-chart","title":"Install the Helm Chart","text":"<p>With your custom values file(s) you install as follows: <pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre></p> <p>Smile, we're up and running! :)</p> <p>If your cluster has spare capacity available, all pods should be in the <code>Running</code> state after about 2-3 minutes. If your cluster needs to auto-scale to provision enough resources, it may take longer while the K8s worker nodes get created. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL.</p> <p>You can now continue to reconfigure it using this guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"guide/smilecdr/logging/","title":"Smile CDR Application Logging","text":"<p>Smile CDR uses the Logback logging framework to collect application logs and can be configured based on individual requirements.</p> <p>Full documentation about the logging system is available here.</p>"},{"location":"guide/smilecdr/logging/#custom-log-configuration","title":"Custom Log Configuration","text":"<p>As per the Smile CDR docs, custom log configurations can be used by providing a custom <code>logback-smile-custom.xml</code> file.</p> <p>While you could create this file manually and include it in the Helm Chart deployment by using one of the file copy methods, editing this file and configuring the copying can be troublesome and error prone.</p>"},{"location":"guide/smilecdr/logging/#automatic-log-configuration","title":"Automatic Log Configuration","text":"<p>As an alternative to the above, you can specify common configurations for the custom logging directly in your Helm values file. This eliminates the need to to perform any extra steps. This can also be helpful for automating log configuration changes.</p> <p>All of the below mentioned techniques may be combined in a single configuration.</p>"},{"location":"guide/smilecdr/logging/#enable-troubleshooting-loggers","title":"Enable Troubleshooting Loggers","text":"<p>As per the Smile CDR documentation, you can enable Troubleshooting Logs of various types.</p> <p>The same configurations can be enabled in your Helm Values file like so:</p> <p>This snippet shows how you would enable DEBUG logs for the HTTP Troubleshooting Log <pre><code>logging:\n  troubleshootingLoggers:\n    http_troubleshooting:\n      enabled: true\n      level: debug\n</code></pre></p> <p>Any of the troubleshooting loggers listed in the official docs can be enabled in the same way.</p>"},{"location":"guide/smilecdr/logging/#set-arbitrary-loggers","title":"Set Arbitrary Loggers","text":"<p>You can enable logging at any log level for any of the classes implemented within Smile CDR.</p> <p>In this example, we demonstrate how to quiesce some noisy log messages that may be flooding the logs.</p> <pre><code>logging:\n  setLoggers:\n    thymeleaf:\n      description: Mute Thymeleaf errors that were flooding the logs with errors.\n      level: \"OFF\"\n      paths:\n      - org.thymeleaf.standard.processor.AbstractStandardFragmentInsertionTagProcessor\n      - org.thymeleaf.standard.processor.StandardIncludeTagProcessor\n</code></pre> <p>Again, any Java class path can be enabled at any log level.</p>"},{"location":"guide/smilecdr/logging/#create-custom-loggers","title":"Create Custom Loggers","text":"<p>If you have the requirement to store logs in an arbitrary log format, you can create a custom logger.</p> <p>The following defines a custom logger for a fictional Smile CDR component, 'x', and saves the logs to <code>myApp.log</code></p> <pre><code>logging:\n  customLoggers:\n    myCustomLogger:\n      path: cdr.component.x\n      enabled: true\n      level: DEBUG\n      pattern: \"%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level M:%X{moduleId} R:%X{requestId} %logger{36} [%file:%line] %msg%n\"\n      target: myApp.log\n</code></pre> <p>You can also set <code>logging.customLoggers.&lt;loggerName&gt;.target: STDOUT</code> to log to <code>stdout</code> instead of writing to a log file.</p>"},{"location":"guide/smilecdr/logging/#provide-raw-custom-log-configuration","title":"Provide Raw Custom Log Configuration","text":"<p>Finally, if you need to provide custom Logback configurations instead of using the above techniques to generate it automatically, you can do so directly in your Helm values file like so:</p> <pre><code>logging:\n  rawLogConfig: |-\n    &lt;included&gt;\n      &lt;!--\n      Custom logging config:\n      This will override and replace any auto-generated logger configurations defined above.\n      --&gt;\n    &lt;/included&gt;\n</code></pre>"},{"location":"guide/smilecdr/logging/#log-collection-and-aggregation","title":"Log Collection and Aggregation","text":"<p>As with any application running in Kubernetes, you should stream logs from the Pods and use an aggregation solution to persist them in a single location.</p> <p>There are numerous solutions available to perform this task. There is no single 'correct' solution as different organisations may have differing requirements for the persisting of application logs.</p> <p>At a high level, here are some foundational concepts that should be reviewed so that a suitable logging solution may be devised.</p> <p>Kubernetes Logging Architecture</p>"},{"location":"guide/smilecdr/logging/#current-helm-chart-support","title":"Current Helm Chart Support","text":"<p>Currently this Helm Chart does not provide any functionality or guidance on how to perform these tasks.</p> <p>It is up to the architect/implementer to devise and implement a suitable solution.</p> <p>Some recommended solutions that are known to work are:</p> <ul> <li>OpenTelemetry + Loki + Grafana</li> <li>EKS Container Insights</li> <li>DataDog</li> </ul>"},{"location":"guide/smilecdr/logging/#future-helm-chart-support","title":"Future Helm Chart Support","text":"<p>There is a soon-to-be released feature of this Helm Chart that will automate the provisioning of a complete Observability Suite alongside your Smile CDR deployment.</p> <p>This feature will enable and configure all of the followinf just by enabling a few simple options inside your Helm values file.</p> <ul> <li>Full OpenTelemetry instrumentation (Metrics, Traces and Logs)</li> <li>OpenTelemetry Collector to aggregate telemetry data and send to backends</li> <li>Prometheus back-end for Metrics telemetry</li> <li>Loki back-end for Log file aggregation</li> <li>Tempo back-end for Trace telemetry</li> <li>Grafana with default dashboards connecting all of the above and providing overall view of Smile CDR cluster.</li> </ul>"},{"location":"guide/smilecdr/messagebroker/","title":"Message Broker Configuration","text":"<p>This Helm Chart support configuring Smile CDR to work with a Kafka or ActiveMQ message broker as described in the Smile CDR documentation</p> <p>While you can configure any of the message broker settings available in the <code>clustermgr</code> module, this process can be complicated and error prone. This Helm Chart simplifies this process by automatically configuring Smile CDR message broker and Kafka settings.</p> <p>You can either configure Smile CDR to use a message broker that has already been provisioned, or if available, you can make use of the Strimzi Operator to provision Kafka inside the K8s cluster.</p>"},{"location":"guide/smilecdr/messagebroker/#configuring-external-message-broker","title":"Configuring external message broker","text":"<p>Use the <code>messageBroker.external</code> section to configure an external message broker like so (in this example we use Kafka):</p> <pre><code>messageBroker:\n  external:\n    enabled: true\n    type: kafka\n    config:\n      ...\n</code></pre> <p>The remaining configuration differs for Kafka and ActiveMQ as described in the sections below.</p>"},{"location":"guide/smilecdr/messagebroker/#configuring-kafka","title":"Configuring Kafka","text":"<p>To enable a default Kafka configuration, you will need to provide connection and authentication details. For more information on the settings being used inside the Smile CDR configuration, please refer to the documentation here</p>"},{"location":"guide/smilecdr/messagebroker/#tls-connectivity","title":"TLS Connectivity","text":"<p>When connecting to Kafka clusters, it is advised to use TLS connections. In a default configuration, Smile CDR will be configured to use TLS.</p> <p>If you wish to run without enabling encryption (i.e. for testing purposes), you can also specify <code>plaintext</code> like so.</p> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: plaintext\n</code></pre> <p>Note: You cannot use mTLS or IAM authentication methods if you disable TLS</p>"},{"location":"guide/smilecdr/messagebroker/#using-custom-certificate-authority","title":"Using Custom Certificate Authority","text":"<p>If your external Kafka cluster is configured with a TLS certificate that is signed with a public Certificate Authority (CA) then no further steps are required as the truststore that is included in the Java distribution will be used.</p> <p>However, if you need to provide a custom CA certificate, you can do so by providing a <code>caCert</code> configuration in the connection settings.</p> <p>This certificate can be provided using either the <code>k8sSecret</code> or <code>sscsi</code> secret mechanisms. See the secrets section for more info.</p> <p>Using <code>k8sSecret</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: k8sSecret\n          secretName: my-kafka-ca-cert\n</code></pre></p> <p>Using <code>sscsi</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkacacert\n</code></pre> CA Cert Secret Format</p> <p>The certificate passed in to the chart must have 2 values, with the appropriate keys.</p> Key name Key value ca.p12 Trust store containing the CA certificate. Must be provided in the PKCS12 (<code>.p12</code>) format ca.password Password for verifying the contents of the trust store."},{"location":"guide/smilecdr/messagebroker/#authentication","title":"Authentication","text":"<p>This chart currently supports either Mutual TLS (mTLS) or IAM authentication, depending on the type of Kafka cluster that is being used.</p> Cluster Type mTLS IAM External (Generic) Y N Amazon MSK Y (With Private CA) Y In-cluster (Strimzi) Y (Default) N <p>Note: Other authentication mechanisms may be added at a later date.</p>"},{"location":"guide/smilecdr/messagebroker/#mtls-authentication","title":"mTLS Authentication","text":"<p>To configure mTLS authentication you need to do the following</p> <ul> <li>Configure Kafka cluster for mTLS.</li> <li>Have access to the client certificate for the configured user.</li> <li>Configure the connection type to use TLS (See above).</li> </ul> <p>Now you can provide the client certificate using <code>k8sSecret</code> or <code>sscsi</code> as follows:</p> <p>Using <code>k8sSecret</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: k8sSecret\n          secretName: my-kafka-ca-cert\n      authentication:\n        type: tls\n        userCert:\n          type: k8sSecret\n          secretName: my-kafka-client-cert\n</code></pre></p> <p>Using <code>sscsi</code> <pre><code>messageBroker:\n  external:\n    config:\n      connection:\n        type: tls\n        caCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkacacert\n      authentication:\n        type: tls\n        userCert:\n          type: sscsi\n          provider: aws\n          secretArn: arn:aws:secretsmanager:us-east-1:012345678901:secret:kafkaclientcert\n</code></pre></p> <p>User Cert Secret Format</p> <p>The user certificate passed in to the chart must have 2 values, with the appropriate keys.</p> Key name Key value user.p12 Trust store containing the user certificate and private key. Must be provided in the PKCS12 (<code>.p12</code>) format user.password Password for decrypting the private key."},{"location":"guide/smilecdr/messagebroker/#iam-authentication-amazon-msk-only","title":"IAM Authentication (Amazon MSK only)","text":"<p>If you are using Amazon MSK as your message broker, IAM is the preferred method of authentication.</p> <p>Before configuring Smile CDR to use this authentication method, you need to ensure that the following pre-requisites are in place:</p> <ul> <li>Configure IRSA for the Smile CDR application. See the Service Accounts section for more info on this.</li> <li>Ensure that your Smile CDR IAM role has a suitable MSK authorization policy attached. See the AWS Documentation for more information on how to create a suitable IAM authorization policy for MSK.</li> </ul> <p>Client Configuration The AWS documentation details the steps to configure clients to use IAM. This required configuration is automatically applied when enabling IAM in this Helm Chart and there is nothing further to do.</p> <p>To enable IAM authentication: <pre><code>messageBroker:\n  external:\n    enabled: true\n    config:\n      connection:\n        type: tls\n        bootstrapAddress: my-msk-bootstrap-address1.amazon.com:9098\n      authentication:\n        type: iam\n</code></pre></p> <p>Note: You do not need to provide a trust certificate as Amazon MSK uses endpoints with publically signed TLS certificates</p>"},{"location":"guide/smilecdr/messagebroker/#consumer-producer-properties","title":"Consumer &amp; Producer properties","text":"<p>Custom consumer properties and producer properties can be configured using the <code>messageBroker.clientConfiguration</code> section as follows:</p> <pre><code>messageBroker:\n  clientConfiguration:\n    consumerProperties:\n      max.poll.records: 20\n    producerProperties: {}\n</code></pre> <p>Note: By default, the consumer properties are configured with the Kafka documented defaults prior to Kafka 3.0</p>"},{"location":"guide/smilecdr/messagebroker/#topic-auto-creation","title":"Topic Auto Creation","text":"<p>By default, the <code>auto.create.topics.enable</code> Kafka broker setting is often set to <code>true</code>. With this setting enabled, Kafka will automatically create topics when consumers or producers try to use a topic.</p> <p>As a best practice, this setting should be changed to <code>false</code> in production environments. When doing this, any required topics should be created through some other process.</p> <p>If your Kafka cluster has topic auto creation disabled, you should set the <code>kafka.validate_topics_exist_before_use</code> option in Smile CDR. See here for more information on these Kafka settings in Smile CDR.</p> <p>As a convenience, this option is auto-configured based on various settings. By default, if using an external Kafka cluster, this option is disabled and creating Subscriptions will work without any manual intervention.</p>"},{"location":"guide/smilecdr/messagebroker/#amazon-msk-serverless","title":"Amazon MSK Serverless","text":"<p>Although Amazon MSK allows <code>auto.create.topics.enable</code> to be set to <code>true</code> (You need to use a [Custom Configuration[(https://docs.aws.amazon.com/msk/latest/developerguide/msk-configuration-properties.html)]), this is not the case for Amazon MSK Serverless. In this case, you must create topics using another mechanism. It is advisable in this case to also set the above mentioned topic validation option in Smile CDR.</p>"},{"location":"guide/smilecdr/messagebroker/#strimzi","title":"Strimzi","text":"<p>If using the Strimzi Operator and configuring this Helm Chart to deploy the Kafka cluster, it is possible to also define the required topics in the values file. When doing this, the <code>auto.create.topics.enable</code> will be automatically set to <code>false</code> to prevent collisions between topics created by the broker and topics created by the Strimzi Operator. The <code>kafka.validate_topics_exist_before_use</code> Smile CDR configuration will also be set to <code>true</code>.</p>"},{"location":"guide/smilecdr/messagebroker/#topic-pre-provisioning","title":"Topic Pre-Provisioning","text":"<p>If you are using a Kafka cluster with <code>auto.create.topics.enable</code> to be set to <code>false</code> as mentioned above, then you will need to create the topics using some other process. If you do not already have a process in place for creating Kafka topics, then there are a couple of options.</p> <ul> <li>Follow the official Kafka documentation for creating topics.This usually involves using a workstation or server with the Kafka binaries and configuration to talk to your Kafka cluster. This can be a tricky process, depending on network security requirements and connectivity/authentication configuration.</li> <li>Use a declarative approach to Kafka topic management. Your topics will be defined in code and can be applied to the Kafka cluster in a highly repeatable fashion.</li> </ul> <p>If using either of the above techniques, the default configuration of Smile CDR deployed by this Helm Chart will reqire the following two topics to be created:</p> <ul> <li><code>batch2.work.notification.Masterdev.persistence</code></li> <li><code>subscription.matching.Masterdev.persistence</code></li> </ul> <p>If you change the <code>nodeId</code> (in <code>cdrNodes</code>) or alter the default module configuration, the above names may change and would need to be determined for your configuration.</p> <p>As a convenience, this Helm Chart provides methods to help with this.</p>"},{"location":"guide/smilecdr/messagebroker/#admin-pod","title":"Admin Pod","text":"<p>This experimental feature will let you create a <code>Kafka Admin</code> pod in the same namespace as your Smile CDR instance. It can be enabled as follows:</p> <pre><code>messageBroker:\n  adminPod:\n    enabled: true\n</code></pre> <p>When enabled, there will be a new deployment created that creates a single ephemeral pod. This pod is automatically configured to use the same connectivity (Including required certificates) as the main Smile CDR pods.</p> <p>You can use this pod to inspect the Kafka cluster, or perform tasks such as creating/deleting topics. This is a convenience feature that should only be used during the development phase and not be used in production environments.</p> <ul> <li> <p>Connect to the Kafka Admin pod <pre><code>kubectl exec -ti &lt;admin-pod-name&gt; -- sh\n</code></pre></p> </li> <li> <p>Check available topics <pre><code>./bin/kafka-topics.sh --list\n</code></pre></p> </li> <li> <p>Connect to the Kafka Admin pod <pre><code>./bin/kafka-consumer-groups.sh --describe --group smilecdr\n</code></pre></p> </li> </ul> <p>Note: You do not need to provide a config file or bootstrap address on the command-line as it is auto configured.</p>"},{"location":"guide/smilecdr/messagebroker/#strimzi_1","title":"Strimzi","text":"<p>If using the Strimzi Operator, you can define topics in a declarative fashion using the <code>messageBroker.topics</code> section.</p> <pre><code>messageBroker:\n  topics:\n    batch2:\n      name: \"batch2.work.notification.Masterdev.persistence\"\n      partitions: 10\n    subscription:\n      name: \"subscription.matching.Masterdev.persistence\"\n      partitions: 10\n</code></pre> <p>When using this method, the Helm Chart will create a <code>KafkaTopic</code> resource for each of the provided topics. The topics will then be created automatically by the Strimzi Topic Operator. The Kafka brokers will be configured with <code>auto.create.topics.enable</code> set to <code>false</code> as per best practice.</p> <p>Using this method allows you to define the configuration of your topics in code for increased repeatability and reliability.</p> <p>You can disable topic auto creation by Strimzi by setting <code>messageBroker.manageTopics</code> to <code>false</code>.</p> <p>Note: Please be aware that if you create topics directly in Kafka (either using the Admin Pod or if auto topic creation is enabled) then the Strimzi Topic Operator will create <code>KafkaTopic</code> resources to match the topics in the Strimzi-managed Kafka cluster.</p>"},{"location":"guide/smilecdr/messagebroker/#provisioning-kafka-with-strimzi","title":"Provisioning Kafka with Strimzi","text":"<p>If you have the Strimzi Operator installed in your cluster, you can use the following values file section to automate provisioning of a Kafka cluster. Your Smile CDR instance will then be automatically configured to use this Kafka cluster. <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n</code></pre></p>"},{"location":"guide/smilecdr/messagebroker/#configuring-kafka-via-strimzi","title":"Configuring Kafka via Strimzi","text":"<p>With the configuration provided above, you will have a production-like Kafka cluster with the followinf configuration:</p> <ul> <li>3 ZooKeeper nodes with the following specifications<ul> <li>0.5cpu</li> <li>512MiB memory</li> <li>10GiB storage</li> </ul> </li> <li>3 Kafka Broker nodes with the following specifications<ul> <li>0.5cpu</li> <li>1GiB memory</li> <li>10GiB storage</li> </ul> </li> <li>TLS connection enabled by default</li> <li>mTLS authentication enabled by default</li> </ul> <p>All of the Kafka configurations can be configured using <code>messageBroker.strimzi.config</code> like so:</p> <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n    kafka:\n      connection:\n        type: tls\n      authentication:\n        type: tls\n      version: \"3.3.1\"\n      protocolVersion: \"3.3\"\n\n      replicas: 4\n      volumeSize: 20Gi\n      resources:\n        requests:\n          cpu: 0.5\n          memory: 4Gi\n        limits:\n          memory: 4Gi\n    zookeeper:\n      replicas: 2\n      volumeSize: 10Gi\n      resources:\n        requests:\n          cpu: 0.5\n          memory: 512Mi\n        limits:\n          memory: 512Mi\n</code></pre>"},{"location":"guide/smilecdr/messagebroker/#deprecated-strimzi-schema","title":"Deprecated Strimzi Schema","text":"<p>If you are updating from version v1.0.0-pre.106 of the Helm Chart or earlier, you will need to alter your Strimzi spec from the old schema below, which has been deprecated.</p> <pre><code>messageBroker:\n  strimzi:\n    enabled: true\n    config:\n      connection:\n        type: tls\n      authentication:\n        type: tls\n      version: \"3.3.1\"\n      protocolVersion: \"3.3\"\n      kafka:\n        replicas: 4\n      zookeeper:\n        replicas: 2\n</code></pre> <ul> <li>Anything under <code>messageBroker.strimzi.config.kafka</code> should be moved to <code>messageBroker.strimzi.kafka</code></li> <li>Anything under <code>messageBroker.strimzi.config.zookeeper</code> should be moved to <code>messageBroker.strimzi.zookeeper</code></li> <li>Anything remaining under <code>messageBroker.strimzi.config</code> should be moved to <code>messageBroker.strimzi</code></li> </ul> <p>You will recieve a deprecation warning so that you have time to update your configurations before support for the old schema is removed.</p> <p>For more details on how to configure Kafka using Strimzi, please consult the Strimzi Operator documentation here</p>"},{"location":"guide/smilecdr/modules/","title":"Module Configuration","text":"<p>Configuring modules is fairly straight forward, but somewhat different than the existing methods used for configuring Smile CDR which uses the <code>cdr-config-Master.properties</code> file.</p> <p>This file is still used behind the scenes, but is automatically generate by the Helm Chart and cannot be modified directly.</p> <p>NOTE: When using Helm Charts, they become the 'single source of truth' for your configuration. This means that repeatable, consistent deployments become a breeze. It also means you should not edit your configuration options in the Smile CDR web admin console.</p> <p>You can define your modules in your main values file, or you can define them in separate files and include them using the helm <code>-f</code> command. This is possible because Helm accepts multiple values files</p> <p>We recommend defining them in one or more separate files, as this allows you to manage common settings as well as per-environment overlays. We will discuss this further down in the Advanced Configuration section below.</p>"},{"location":"guide/smilecdr/modules/#mapping-traditional-smile-cdr-configuration-to-helm","title":"Mapping traditional Smile CDR configuration to Helm","text":"<p>Mapping existing configurations to values files is relatively straight forwards:</p>"},{"location":"guide/smilecdr/modules/#identify-the-module-configuration-parameter","title":"Identify the module configuration parameter.","text":"<p>e.g. Concurrent Bundle Validation Config.properties format: <code>module.persistence.config.dao_config.concurrent_bundle_validation = false</code></p>"},{"location":"guide/smilecdr/modules/#specify-them-in-the-values-yaml-file-format","title":"Specify them in the values yaml file format:","text":"<p><pre><code>modules:\n  persistence:\n    config:\n      dao_config.concurrent_bundle_validation: \"false\"\n</code></pre> The same effective mapping can be used for any module configurations supported by Smile CDR.</p>"},{"location":"guide/smilecdr/modules/#included-pre-defined-module-definitions","title":"Included pre-defined Module Definitions","text":"<p>This chart includes a set of pre-defined module configurations that closely matches the default <code>cdr-config-Master.properties</code> configuration that is included with Smile CDR.</p> <p>Some of these module configurations are slightly different in order to better accommodate deploying in Kubernetes. If you wish to review this default configuration, or use it as a baseline for your own custom set of module configurations, you can get it here</p> <p>WARNING: If you use a copy of the pre-defined modules, then be aware that future versions of this Helm Chart may introduce breaking changes that will require you to review changes in the default modules, so that you can update your custom module definitions.</p>"},{"location":"guide/smilecdr/modules/#configuring-endpoint-modules","title":"Configuring Endpoint Modules","text":"<p>Many Smile CDR modules provide services that can be consumed via some API. These modules implement listeners that expose them to other parts of your infrastructure. In order for these <code>endpoint</code> modules to be exposed, additional Kubernetes resources need to be created in the cluster.</p>"},{"location":"guide/smilecdr/modules/#configure-service","title":"Configure Service","text":"<p>This Helm Chart will configure these resources based on the configurations provided in the module's <code>service</code> configuration.</p> <p>e.g. This snippet will create a <code>Service</code> resource and a <code>rule</code> in the default <code>Ingress</code> resource for the Web Admin Console: <pre><code>modules:\n  admin_web:\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    config:\n      port: 9100\n</code></pre></p> <ul> <li><code>moduleSpec.service.enabled</code> must be set to <code>true</code> to enable the service</li> <li><code>moduleSpec.service.svcName</code> represents the name of the created resources and must be unique for each module</li> <li><code>moduleSpec.service.hostName</code> can be overridden with an explicit host name. If the value <code>default</code> is used, it will derive the hostName from <code>specs.hostname</code></li> <li><code>moduleSpec.service.config.port</code> must be set to a unique value for each module that defines an ingress.</li> </ul>"},{"location":"guide/smilecdr/modules/#configure-ingress","title":"Configure Ingress","text":"<p>As this Helm Chart supports multiple Ingress resources, you may need to specify which ingress is used by any modules that need to be exposed externally to the Kubernetes cluster.</p> <p>See the Ingress section for more information on Ingress Configurations.</p>"},{"location":"guide/smilecdr/modules/#default-ingress-configuration","title":"Default Ingress Configuration","text":"<p>In a default Helm Chart installation, only a single Ingress resource is created and any modules with enabled <code>service</code> configurations will use this ingress by default. No extra module configuration is required in this scenario.</p>"},{"location":"guide/smilecdr/modules/#custom-ingress-configuration","title":"Custom Ingress Configuration","text":"<p>If you wish to use a non-default Ingress resource, then this needs to be specified on a per-module basis.</p> <p>Before specifying any ingresses here, a custom Ingress Configuration needs to be created.</p> <p>In the following snippet, we will configure the Web Admin Console to use a custom internal Ingress resource and the FHIR endpoint to use a custom public Ingress resource. All other modules will use the default Ingress Resource.</p>"},{"location":"guide/smilecdr/modules/#my-module-valuesyaml","title":"<code>my-module-values.yaml</code>","text":"<pre><code>modules:\n  admin_web:\n    service:\n      ingresses:\n        myPrivateNginx:\n          enabled: true\n  fhir_endpoint:\n    service:\n      ingresses:\n        myPublicNginx:\n          enabled: true\n</code></pre> <p>NOTE: When defining a custom ingress here, you do not need to explicitly disable the default ingress as it's disabled automatically.</p>"},{"location":"guide/smilecdr/modules/#module-definition-considerations","title":"Module definition considerations","text":"<p>Here are some additional fields/considerations that need to be included in your module definitions files:</p> <ul> <li>Though not strictly required by the <code>yaml</code> spec, all values should be quoted.   You may run into trouble with some values if you do not quote them.   Specifically, values starting with <code>*</code> or <code>#</code> will fail if not quoted.</li> <li>The <code>module id</code> is taken from the yaml key name.</li> <li>Modules can be defined, but disabled. They need to be enabled with the <code>enabled: true</code> entry. Disabled modules will not be included in the generated <code>cdr-config-*.properties</code> file</li> <li>Modules other than the cluster manager need to define <code>type</code>. A list of module types is available here</li> <li>Modules which expose an endpoint need to de defined with a <code>service</code> entry, which defines infrastructure resources that are required to access the module.</li> <li>DB credentials/details can be referenced from your module configurations via <code>DB_XXX</code> environment variables.</li> </ul> <p>Any configurations you specify will merge with the defaults, priority going to the values file.</p>"},{"location":"guide/smilecdr/modules/#disabling-included-pre-defined-module-definitios","title":"Disabling included pre-defined module definitios","text":"<p>If you wish to disable any of the pre-defined default modules, you can do so individually, or you can disable all default modules and define your own from scratch.</p> <p>If you do the latter, it may be easier to determine the exact modules you have defined just by looking at your values files.</p> <p>NOTE: If doing this, you may need to review upstream changes when moving to a newer version of the Helm Chart.</p> <p>You can disable all default modules using: <pre><code>modules:\n  useDefaultModules: false\n</code></pre> You can reference the <code>default-modules.yaml</code> file as a reference by untarring the Helm Chart or viewing it directly from the repository.</p> <p>Here is an example of what your module definition may look like when configuring Smile CDR with the <code>clustermgr</code>, <code>persistence</code>, <code>local_security</code>, <code>fhir_endpoint</code> and <code>admin_web</code> modules.</p>"},{"location":"guide/smilecdr/modules/#my-module-valuesyaml_1","title":"<code>my-module-values.yaml</code>","text":"Click to expand <pre><code>modules:\n  useDefaultModules: false\n  clustermgr:\n    name: Cluster Manager Configuration\n    enabled: true\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n  persistence:\n    name: Database Configuration\n    enabled: true\n    type: PERSISTENCE_R4\n    config:\n      db.driver: POSTGRES_9_4\n      db.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\n      db.password: \"#{env['DB_PASS']}\"\n      db.username: \"#{env['DB_USER']}\"\n  local_security:\n    name: Local Storage Inbound Security\n    enabled: true\n    type: SECURITY_IN_LOCAL\n    config:\n      seed.users.file: classpath:/config_seeding/users.json\n      password_encoding_type: BCRYPT_12_ROUND\n  admin_web:\n    name: Web Admin\n    enabled: true\n    type: ADMIN_WEB\n    service:\n      enabled: true\n      svcName: admin-web\n      hostName: default\n    requires:\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: \"\"\n      port: 9100\n      tls.enabled: false\n      https_forwarding_assumed: true\n      respect_forward_headers: true\n  fhir_endpoint:\n    name: FHIR Service\n    enabled: true\n    type: ENDPOINT_FHIR_REST_R4\n    service:\n      enabled: true\n      svcName: fhir\n      hostName: default\n    requires:\n      PERSISTENCE_R4: persistence\n      SECURITY_IN_UP: local_security\n    config:\n      context_path: fhir_request\n      port: 8000\n      base_url.fixed: default\n</code></pre>"},{"location":"guide/smilecdr/modules/#define-readiness-probe","title":"Define Readiness Probe","text":"<p>As Kubernetes only supports a single readiness probe per container, you need to define which endpoint module Kubernetes should use to consider the 'readiness' of your installation.</p> <p>The default modules included with this chart are configured so that the <code>fhir_endpoint</code> module is used for the readiness probe. This is done by setting the <code>enableReadinessProbe</code> key to <code>true</code> in the module definition.</p> <p>If you wish to use a different module for the readiness probe, you must disable it for the <code>fhir_endpoint</code> module and enable it for the module of your choice. e.g.</p>"},{"location":"guide/smilecdr/modules/#my-module-valuesyaml_2","title":"<code>my-module-values.yaml</code>","text":"<p><pre><code>modules:\n  fhir_endpoint:\n    enableReadinessProbe: false\n  my_fhir_endpoint:\n    enableReadinessProbe: true\n    enabled: true\n    ...\n</code></pre> Alternatively, you may disable the included default modules as described above, and then enable the probe on one of your custom defined modules.</p> <p>Note: You must enable the readiness probe for exactly one endpoint module. If you specify none, or more than one, the Helm Chart will return an error.</p>"},{"location":"guide/smilecdr/modules/#install-smile-cdr-with-extra-modules-definition-files","title":"Install Smile CDR with extra modules definition files","text":"<p>When splitting your configuration into multiple <code>values</code> files, pass them in to your <code>helm upgrade</code> commandline like so: <pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml -f my-module-values.yaml smiledh/smilecdr\n</code></pre></p>"},{"location":"guide/smilecdr/modules/#experimentalunsupported-features","title":"Experimental/Unsupported Features","text":"<p>There are scenarios where you may wish to update Smile CDR module configurations directly in the Web console.</p> <ul> <li>You need to do some realtime troubleshooting that requires live updates of module configuration</li> <li>You are working in a development environment and you do not have a suitable code pipeline in place to enable fast iteration of changes</li> </ul> <p>In these cases, there are two settings that you can use to update configuration live in the Smile CDR Web Admin console. Using these settings will alter the values of <code>node.config.locked</code> and <code>node.propertysource</code> in the resulting Smile CDR configuration. Re fer to the Smile CDR Docs for more info on Module Property Sources.</p>"},{"location":"guide/smilecdr/modules/#troubleshooting-mode","title":"Troubleshooting Mode","text":"<p>You may enable troubleshooting mode for a given Smile CDR Node as follows:</p> <p>Note: If you have defined a different <code>cdrNodes</code> configuration, please alter the code below accordingly.</p> <p><pre><code>cdrNodes:\n  masterdev:\n    config:\n      troubleshooting: true\n</code></pre> Enabling this option lets you update module configurations in the console for troubleshooting/testing/experimenting.</p> <ul> <li>Your changes will be lost if the pod restarts or if another pod joins the cluster.</li> <li>It sets <code>node.config.locked</code> to <code>false</code> and sets <code>node.propertysource</code> to <code>PROPERTIES_UNLOCKED</code></li> </ul>"},{"location":"guide/smilecdr/modules/#database-mode","title":"Database Mode","text":"<p>The troubleshooting mode may not meet your requirements in some scenarios:</p> <ul> <li>You need these changes to persist for a longer time period and your underlying compute resources could be interrupted. For example:<ul> <li>You are using ephemeral compute resources such as AWS EC2 Spot instances which can go away with short notice.</li> <li>Your infrastructure needs to be scaled down when not actively working on it.</li> </ul> </li> <li>You wish to restart the Smile CDR pods while maintaining your manual configuration changes.</li> <li>New Kubernetes Pods may come online (If you are testing HPA or HA, for example.)</li> </ul> <p>In these scenarios, it would be a more robust solution to regularly mirror your configuration changes to your Helm <code>values</code> file and reconcile.</p> <p>In the event that this is not possible, and your manually entered configurations must persist in the above scenarios, you may use the <code>database</code> mode as follows:</p> <p>Note: If you have defined a different <code>cdrNodes</code> configuration, please alter the code below accordingly.</p> <pre><code>cdrNodes:\n  masterdev:\n    config:\n      database: true\n</code></pre> <p>When enabling this mode, consider the following:</p> <ul> <li>This is an experimental feature and is unsupported. Use at your own risk.</li> <li>It sets <code>node.config.locked</code> to <code>false</code></li> <li>It sets <code>node.propertysource</code> to <code>DATABASE</code></li> <li>The Helm Chart will still create surrounding Kubernetes resources (Ingress, Service, Extra files, Mapped secrets etc) based on the contents of the Helm <code>values</code> file.</li> <li>If you add a new module from the Smile CDR Web console and that module has an endpoint configuration, you will NOT be able to access it. No Ingress or Service objects will be created unless you also create the module using the Helm Chart.</li> <li>Any changes made in the Smile CDR Web console that do not match the Helm <code>values</code> settings will lead to configuration drift that may cause unpredictable behaviour the next time you deploy the Helm Chart.</li> <li>In the event of such drift occurring, reverting this mode to <code>disabled</code> may then lead to unpredictable behaviour that could result in modules being incorrectly configured, resulting to critical system or data integrity faults. <p>!!!DO NOT USE THIS EXPERIMENTAL UNSUPPORTED FEATURE IN NON-DEVELOPMENT ENVIRONMENTS!!!</p> </li> </ul>"},{"location":"guide/smilecdr/requirements/","title":"Requirements and Dependencies","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Helm Chart. Due to the complicated nature of configuring the product, and enforcing strong security practices, there is no quickstart option without ensuring some, or all, of these pre-requisites have been met.</p>"},{"location":"guide/smilecdr/requirements/#minimum-requirements","title":"Minimum Requirements","text":"<p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes add-ons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>One of the following supported database options:<ul> <li>Externally provisioned database in the official Smile CDR supported databases list here</li> <li>CrunchyData Postgres Operator installed in cluster. See Extra Requirements below if you follow this option.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#recommended-requirements","title":"Recommended Requirements","text":"<p>These dependencies are recommended in order to follow security best practices. These are in addition to those listed above.</p> <ul> <li>Kubernetes/EKS/AKS cluster should be configured with best practices in mind.<ul> <li>Kubernetes best practices</li> <li>Amazon EKS best practices</li> <li>Azure AKS best practices</li> </ul> </li> <li>Kubernetes cluster should, at the very least, have the following configurations<ul> <li>Secret Encryption (EKS Secret Encryption)</li> <li>Storage Class with encryption enabled if using persistent storage (PostgreSQL or Kafka)</li> <li>Enforce all pods should set resource requests</li> </ul> </li> <li>AWS IAM Role for the Smile CDR application.<ul> <li>Should follow the principle of least privilege and only have access to required AWS services</li> </ul> </li> <li>Secrets Store CSI Driver + Provider<ul> <li>Only the AWS Provider is supported at this time</li> <li>AWS IAM Role needs access to read &amp; decrypt the secrets in AWS Secrets Manager</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#extra-requirements","title":"Extra Requirements","text":"<ul> <li>Strimzi Kafka Operator<ul> <li>Allows you to install a production ready Kafka cluster as a part of the Smile CDR deployment.</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/resources/","title":"Configuring Compute Resources","text":""},{"location":"guide/smilecdr/resources/#kubernetes-memory-requests-vs-limits","title":"Kubernetes Memory Requests vs Limits","text":"<p><code>requests.memory</code> will be set to the same value as <code>limits.memory</code> unless you override it.</p> <p>The values you should use for CPU resources will depend on the number of cores you are licenced for with Smile CDR. Your total cores can be calculated by <code>replicas * requests.limits.cpu</code>, or <code>autoscaling.maxReplicas * requests.limits.cpu</code> if you are using Horizontal Pod Autoscaling.</p>"},{"location":"guide/smilecdr/resources/#jvm-sizing","title":"JVM Sizing","text":"<p>As Smile CDR is a high performance Java based application, special consideration needs to be given to the resource settings and JVM tuning parameters.</p> <p>Typical cloud best practices suggest starting small and increasing resources as workload increases. We have tested Smile CDR in its default module configuration and determined that the max heap size should be no smaller than 2GB. When smaller than this, there are excessive GC events which is not ideal.</p> <p>NOTE: If you reconfigure Smile CDR to have more modules, it may require more memory/cpu. If you split up the cluster into multiple nodes, then each node may be able to run with less memory/cpu, though total cluster may end up higher depending on your architecture.</p>"},{"location":"guide/smilecdr/resources/#jvm-heap-auto-sizing","title":"JVM Heap Auto Sizing","text":"<p>When running Java applications in Kubernetes, the <code>requests.memory</code> should be set much higher than the max heap size. Typically the Java heap size should be set to 50-75% of the total available memory.</p> <p>This Helm Chart will take the specified <code>limits.memory</code> and use <code>jvm.memoryFactor</code> to calculate the value for the Java heap size. By default, this value is <code>0.5</code>. With the default <code>limits.memory</code> of 4Gib, the chart sets Java <code>-Xmx</code> to <code>2048m</code>.</p> <p>Setting this number higher will make more efficient use of memory resources in your K8s cluster, but may increase the likelihood of <code>OOM Killed</code> errors.</p>"},{"location":"guide/smilecdr/resources/#setting-heap-minimum-size","title":"Setting Heap Minimum Size","text":"<p>If you were to set <code>jvm.memoryFactor</code> to <code>1</code> your pod is almost guaranteed to be killed with such an error, but it will happen at an unpredictable time, once the currently allocated heap grows to a certain point. This can increase difficulty of troubleshooting due to the unpredictable timing. It may fail in a few minutes, or a few hours/days/weeks/never depending on the workload characteristics.</p> <p>To reduce the likelihood of such unpredictable <code>OOM Killed</code> errors, we recommend setting <code>-Xms</code> to be the same as <code>-Xmx</code>. This can be done by setting <code>jvm.xms</code> to <code>true</code></p> <p>NOTE: You can pass in extra JVM commandline options by adding them to the list <code>jvm.args</code></p>"},{"location":"guide/smilecdr/updating/","title":"Deploying changes","text":"<p>When you make changes to the Helm Chart configuration, you need to apply them using the <code>helm upgrade</code> command. This chart has been designed in such a way that there should not be any outages during updates.</p> <p>There are multiple components that a configuration can affect. Broadly, it can affect the configuration of the Smile CDR app itself, or it can affect the surrounding infrastructure.</p> <p>In the event that a configuration change affects the Smile CDR application, then this chart will update any configuration files and create new application pods with zero-outage.</p>"},{"location":"guide/smilecdr/updating/#rolling-deployments","title":"Rolling Deployments","text":"<p>We achieve zero-outage by making use of Rolling Deployments in Kubernetes. The rolling deployment has been configured to create one new Pod with the new configuration at a time. Once each new Pod has successfully started up and is able to accept traffic, Kubernetes will start routing requests to it and then terminate one of the pods with the older configuration.</p> <p>The result of this is that the changes will be rolled out over the entire cluster in a controlled fashion over a few minutes, without any downtime or outage.</p> <p>This is a conservative rolling deployment model, but it means that if pods with the new configuration fail to come up without error, then the existing deployment will remain unaffected.</p>"},{"location":"guide/smilecdr/updating/#making-a-config-change-with-rolling-deployments","title":"Making a config change with Rolling Deployments","text":"<p>There is nothing you need to do to make use of this rolling deployment mechanism. If your chart configuration changes include something that will update the Smile CDR configuration, and if you have a sufficient number of replicas, then this will happen automatically.</p> <p>All changes other than those listed here will cause a rolling deployment of the application</p> <ul> <li><code>replicaCount</code> or <code>autoScaling</code> changes</li> <li><code>ingress</code> configuration - i.e. switching to a different ingress provider.</li> <li>CrunchyPGO database infrastructure configuration<ul> <li>Updating <code>users</code> config WILL cause a rolling deployment</li> </ul> </li> <li>Strimzi Kafka infrastructure resource configuration<ul> <li>Updating protocol/connection config will cause a rolling deployment</li> </ul> </li> </ul> <p>The method used to apply your updates will depend on how you have deployed the Helm Chart. If you have used a code reconciliation system or some other automation, you should not need to do anything.</p> <p>If using native Helm commands, you would use the same command you used to install the chart, like so:</p> <p><code>helm upgrade -i my-smile-env -f my-values.yaml smiledh/smilecdr</code></p>"},{"location":"guide/smilecdr/updating/#automatic-deployment-of-config-changes","title":"Automatic Deployment of Config Changes","text":"<p>Normally, changes that do not directly affect the Pod definition of a Deployment in Kubernetes will not trigger a deployment. Typically, this means that manual recycling of Pods may be required to force updates.</p> <p>To ensure that all changes are automatically deployed, the Smile CDR Helm Chart uses a unique <code>sha256</code> hash to identify any <code>ConfigMap</code> objects. This means that any configuration changes will be detected and automatically deployed without interruption using the Rolling Deployment strategy.</p> <p>NOTE: An extra benefit of this technique is that if a new configuration has an error and the pods fail to come up, then the existing Pods will still use their original configuration, even if they need to be restarted.</p> <p>This feature can be disabled if required by setting <code>autoDeploy</code> to <code>false</code></p>"},{"location":"guide/smilecdr/updating/#argocd-considerations","title":"ArgoCD Considerations","text":"<p>If you ArgoCD to deploy your charts, then this mechanism would cause previous versions of the <code>ConfigMap</code> to be deleted after you perform configuration changes. This interferes with the ability for the existing <code>ReplicaSet</code> to scale or self-heal. To avoid this issue, you should set <code>argocd.enabled</code> to true to prevent this issue. By doing this, it will add annotations to any <code>ConfigMap</code> resources that are identified by their hash, so that ArgoCD does not prune the resources.</p>"},{"location":"guide/smilecdr/updating/#long-running-processes","title":"Long Running Processes","text":"<p>Although these techniques will avoid any disruption to the application availability, any long running processes may be interrupted. Remember to design any workflows to be able to handle unexpected disruption, using retry mechanisms for any tasks that do not complete correctly due to transient infrastructure interruption.</p>"},{"location":"quickstart/","title":"Deployment Quickstart","text":"<p>This section of the documentation will get you up and running quickly to show how the chart works. For any real deployments, please look through the advanced deplopyments in the User Guide and Examples sections to design a solution that works for your environment.</p>"},{"location":"quickstart/#preparation","title":"Preparation","text":"<p>To deploy Smile CDR using these Helm Charts, you will need to do the following:</p> <ul> <li>Ensure all requirements and dependencies are in place</li> <li>Prepare a set of configurations to suit your planned installation architecture</li> <li>Perform the deployment</li> </ul> <p>The following pages will guide you through the above steps to so that you can gain familiarity with how these Helm Charts function.</p>"},{"location":"quickstart/#advanced-deployment","title":"Advanced Deployment","text":"<p>The Quickstart shows you a basic install that does not follow security best practices. To install with best practices in mind, refer to the advanced configurations in the User Guide section which goes into detail on all available configuration options.</p>"},{"location":"quickstart/helm-repo/","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>In this Quickstart, we will use the native <code>helm</code> command, but you may wish to deploy using alternative tooling in your environment. Please check the User Guide for more info on this.</p>"},{"location":"quickstart/helm-repo/#add-repository","title":"Add repository","text":"<p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"quickstart/install-smilecdr/","title":"Install Smile CDR","text":""},{"location":"quickstart/install-smilecdr/#install-the-helm-chart","title":"Install the Helm Chart","text":"<pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre> <p>Smile, we're up and running! :)</p> <p>After about 2-3 minutes, all pods should be in the <code>Running</code> state with <code>1/1</code> containers in the <code>Ready</code> state. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL. You can try re-configuring it using the instructions in the User Guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"quickstart/requirements/","title":"Quickstart Requirements","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Quickstart guide.</p> <p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes add-ons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment</li> <li>This is used for the Quickstart as it is the easiest way to get up and running without   having to provision an external database and configure credentials and connectivity</li> </ul> </li> <li>Persistent Volume provider that can be used to create <code>PersistentVolume</code> resources for the database</li> </ul>"},{"location":"quickstart/values-file/","title":"Create a Helm values file for your environment","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p>"},{"location":"quickstart/values-file/#a-note-on-creating-values-files","title":"A note on creating values files","text":"<p>Do not copy the default <code>values.yaml</code> file from the Helm Chart, start from a fresh empty file instead.</p> <p>See the section on Values Files Management for more info on this.</p>"},{"location":"quickstart/values-file/#example-values-file","title":"Example Values File","text":"<p>The following example will work in any Kubernetes environment that has the following components installed.</p> <ul> <li>Nginx Ingress</li> <li>CrunchyData PGO</li> <li>A suitable Persistent Volume storage provider (For the database).</li> </ul> <p>You will need to update the values specific to your environment and include credentials for a container repository that contains the Smile CDR Docker images.</p> <p>WARNING: The following method of providing Docker credentials in the values file is insecure and only shown in this quick-start demonstration to show the chart in action. You should instead use an alternative such as an external secret vault. See the secrets section for more info.</p> <p><code>my-values.yaml</code> file <pre><code>specs:\n  hostname: smilecdr.mycompany.com\nimage:\n  repository: docker.smilecdr.com/smilecdr\n  imagePullSecrets:\n  - type: values\n    registry: docker.smilecdr.com\n    username: &lt;DOCKER_USERNAME&gt;\n    password: &lt;DOCKER_PASSWORD&gt;\ndatabase:\n  crunchypgo:\n    enabled: true\n    internal: true\n</code></pre></p>"}]}