{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is the documentation for the Helm Charts provided by Smile Digital Health.</p> <p>These Helm Charts are provided to simplify installation and configuration of Smile Digital Health products in Kubernetes environments using a number of best practices.</p> <p>Currently, the only chart available is for the core Smile CDR product.</p> <p>WARNING: These charts are still in pre-release! As such, there may still be breaking changes introduced without notice.</p> <p>Only use this version of the chart for evaluation or testing.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To quickly see these charts in action, follow the guide in the Quickstart section.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>Full details on configuring the products using these charts are provided in the User Guide section.</p>"},{"location":"charts/smilecdr/","title":"Smile CDR Helm Chart","text":"<p>A Helm Chart to install Smile CDR</p>"},{"location":"charts/smilecdr/#releases","title":"Releases","text":"<p>Latest release: 1.0.0-pre33</p>"},{"location":"charts/smilecdr/CHANGELOG-PRE/","title":"1.0.0-pre.42 (2023-01-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features","title":"Features","text":"<ul> <li>smilecdr: add config locking options (351d48d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre41-2023-01-28","title":"1.0.0-pre.41 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_1","title":"Features","text":"<ul> <li>smilecdr: add support for Alpine3 base image (dc7e960)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre40-2023-01-28","title":"1.0.0-pre.40 (2023-01-28)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ingress annotation overrides (78d254e)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre39-2023-01-27","title":"1.0.0-pre.39 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in env vars (81defae)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre38-2023-01-27","title":"1.0.0-pre.38 (2023-01-27)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix key names in k8s secret (b84760a)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre37-2023-01-26","title":"1.0.0-pre.37 (2023-01-26)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>smilecdr: change objectAlias naming logic (ade3f22)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre36-2023-01-11","title":"1.0.0-pre.36 (2023-01-11)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_2","title":"Features","text":"<ul> <li>smilecdr: update ingress logic and docs (affff39)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre35-2023-01-06","title":"1.0.0-pre.35 (2023-01-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_3","title":"Features","text":"<ul> <li>smilecdr: copy files from external location (ea2710f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre34-2022-12-22","title":"1.0.0-pre.34 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix ConfigMap reference in volume (5cc81e8)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre33-2022-12-22","title":"1.0.0-pre.33 (2022-12-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>smilecdr: force lower case in resource names (6d1e4aa)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre32-2022-12-20","title":"1.0.0-pre.32 (2022-12-20)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_4","title":"Features","text":"<ul> <li>smilecdr: change db secret config (8069b77)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Values file needs to be updated if using sscsi for DB secrets</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre31-2022-12-14","title":"1.0.0-pre.31 (2022-12-14)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_5","title":"Features","text":"<ul> <li>smilecdr: disable crunchypgo (a2a4e32)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre30-2022-12-10","title":"1.0.0-pre.30 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_6","title":"Features","text":"<ul> <li>smilecdr: add argocd feature (75a4874)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre29-2022-12-10","title":"1.0.0-pre.29 (2022-12-10)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix secret reference keys (2f6411b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-06","title":"1.0.0-pre.28 (2022-12-06)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_7","title":"Features","text":"<ul> <li>smilecdr: make image secret required (e068330)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre28-2022-12-05","title":"1.0.0-pre.28 (2022-12-05)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_8","title":"Features","text":"<ul> <li>smilecdr: make image secret required (2c2389b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre27-2022-12-03","title":"1.0.0-pre.27 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix DB_PORT in default modules (2239f60)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre26-2022-12-03","title":"1.0.0-pre.26 (2022-12-03)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>smilecdr: correct the field for image secrets (2ddeef2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre25-2022-12-02","title":"1.0.0-pre.25 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_9","title":"Features","text":"<ul> <li>smilecdr: support multiple databases (e28bb13)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre24-2022-12-02","title":"1.0.0-pre.24 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>smilecdr: improve modules include logic (f7850d2)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre23-2022-12-02","title":"1.0.0-pre.23 (2022-12-02)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>smilecdr: allow quoted config entries (f9d7e1f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre22-2022-12-01","title":"1.0.0-pre.22 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>smilecdr: change crunchydata resource names (654417b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_10","title":"Features","text":"<ul> <li>smilecdr: improve CrunchyData integration (4289432)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre21-2022-12-01","title":"1.0.0-pre.21 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_11","title":"Features","text":"<ul> <li>smilecdr: update JVM tuning params (cc1f859)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre20-2022-12-01","title":"1.0.0-pre.20 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>smilecdr: remove image reference (2b47fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre19-2022-12-01","title":"1.0.0-pre.19 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_12","title":"Features","text":"<ul> <li>smilecdr: add support for strimzi kafka (27b8fb4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre18-2022-12-01","title":"1.0.0-pre.18 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_13","title":"Features","text":"<ul> <li>smilecdr: add autoscaling support (6ff0f2f)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre17-2022-12-01","title":"1.0.0-pre.17 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_14","title":"Features","text":"<ul> <li>smilecdr: configure rolling deployments (daa9c4b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre16-2022-12-01","title":"1.0.0-pre.16 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_15","title":"Features","text":"<ul> <li>smilecdr: add pod disruption budget (b8b46e4)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre15-2022-12-01","title":"1.0.0-pre.15 (2022-12-01)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_16","title":"Features","text":"<ul> <li>smilecdr: add redeploy on config changes (90e33ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre14-2022-11-30","title":"1.0.0-pre.14 (2022-11-30)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_17","title":"Features","text":"<ul> <li>smilecdr: add readiness probe (5a03100)</li> <li>smilecdr: add startup probe (65014ad)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre13-2022-11-25","title":"1.0.0-pre.13 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_18","title":"Features","text":"<ul> <li>smilecdr: update CDR version and modules (116e187)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_1","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: - This updates the Smile CDR version</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre12-2022-11-25","title":"1.0.0-pre.12 (2022-11-25)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_19","title":"Features","text":"<ul> <li>smilecdr: add support for deploying postgres (0fc81b1)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre11-2022-11-24","title":"1.0.0-pre.11 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>smilecdr: fix reference to configmap data (383fdd6)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre10-2022-11-24","title":"1.0.0-pre.10 (2022-11-24)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_20","title":"Features","text":"<ul> <li>smilecdr: support injecting files (a823a7d)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre9-2022-11-23","title":"1.0.0-pre.9 (2022-11-23)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_21","title":"Features","text":"<ul> <li>smilecdr: automate setting JVMARGS (6fcda8b)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre8-2022-11-22","title":"1.0.0-pre.8 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_22","title":"Features","text":"<ul> <li>smilecdr: set default replicas to 1 (9788832)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre7-2022-11-22","title":"1.0.0-pre.7 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_23","title":"Features","text":"<ul> <li>smilecdr: add name override for CrunchyPGO (110e133)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre6-2022-11-22","title":"1.0.0-pre.6 (2022-11-22)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_24","title":"Features","text":"<ul> <li>smilecdr: add Secrets Store CSI support (f8f23ba)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre5-2022-11-21","title":"1.0.0-pre.5 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_25","title":"Features","text":"<ul> <li>smilecdr: add support for IRSA (IAM roles) (509bbe3)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre4-2022-11-21","title":"1.0.0-pre.4 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_26","title":"Features","text":"<ul> <li>smilecdr: update Ingress definition logic (2271d57)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#breaking-changes_2","title":"BREAKING CHANGES","text":"<ul> <li>smilecdr: Now uses <code>nginx-ingress</code> instead of <code>aws-lbc-nlb</code> for specifying Nginx Ingress Controller</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre3-2022-11-21","title":"1.0.0-pre.3 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_27","title":"Features","text":"<ul> <li>smilecdr: add back default tag functionality (46785e5)</li> <li>smilecdr: add common labels to all resources (618ba2e)</li> <li>smilecdr: normalize resource names (6ca25bd)</li> <li>smilecdr: remove extra labels from default values (d971934)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre2-2022-11-21","title":"1.0.0-pre.2 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_28","title":"Features","text":"<ul> <li>smilecdr: remove hard coded entries from ConfigMap (b6a2fb5)</li> </ul>"},{"location":"charts/smilecdr/CHANGELOG-PRE/#100-pre1-2022-11-21","title":"1.0.0-pre.1 (2022-11-21)","text":""},{"location":"charts/smilecdr/CHANGELOG-PRE/#features_29","title":"Features","text":"<ul> <li>repo: initial Commit (5f98460), closes #68834381</li> <li>smilecdr: add external module files support (da374c1)</li> <li>smilecdr: update application version (9b203f2)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/","title":"Smile CDR","text":"<p>The Smile CDR Helm Chart provides a flexible and consistent method to deploy Smile CDR on a Kubernetes cluster.</p> <p>It is provided by Smile Digital Health to help reduce the effort and complexity of installing Smile CDR on Kubernetes. It has been well tested on Amazon EKS and has growing compatibility for Azure AKS.</p>"},{"location":"charts/smilecdr/smilecdr/#features","title":"Features","text":"<p>This chart supports a number of features to help you install Smile CDR in a secure, reliable, cost effective and scalable manner with operational efficiency in mind.</p>"},{"location":"charts/smilecdr/smilecdr/#application-features","title":"Application Features","text":"<p>This chart supports the following Smile CDR features \"out-of-the-box\":</p> <ul> <li>Supports official Smile CDR Docker images<ul> <li>Support for upcoming 2023.02 release which enhances container security posture</li> <li>Limited support for 2022.11 and older. See CDR Versions section for more info</li> </ul> </li> <li>*Configuration-as-code* management of all module definitions &amp; settings</li> <li>Support for multiple databases (i.e. Separate DB for cluster manager, audit log and one or more persistence stores)</li> <li>Flexible JVM tuning with sane defaults</li> <li>Adding files to the Smile CDR pods. Eliminates need to build custom images or directly access the Pod to copy them in<ul> <li>Small non-binary files up to 1Mb each via <code>ConfigMap</code> resources (i.e. config files, scripts etc)</li> <li>*NEW* Large &amp; binary file loading support - This allows you to include resources such as <code>.jar</code> files and longer scripts etc into the Smile CDR pod. Currently supports AWS S3</li> </ul> </li> <li>Kafka configuration</li> <li>Coming soon...<ul> <li>Flexible CDR Node configurations (i.e. Smile CDR Cluster Design Sample Architecture)</li> <li>AWS IAM authentication for RDS databases</li> <li>File loading from sources other than S3</li> <li>MongoDB support</li> <li>User seeding</li> <li>OIDC seeding</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#infrastructure-features","title":"Infrastructure Features","text":""},{"location":"charts/smilecdr/smilecdr/#app-networking","title":"App Networking","text":"<ul> <li>Automatic configuration of Kubernetes Services and Ingresses</li> <li>Coming soon...<ul> <li>Network Policies</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#ingress","title":"Ingress","text":"<ul> <li>TLS termination at load Balancer</li> <li>Nginx Ingress Controller</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> <li>Coming soon...<ul> <li>Multiple Ingress (e.g. internal and external, for different modules)</li> <li>End-to-end TLS encryption</li> <li>Support for other ingress controllers may be implemented as required</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#dependency-provisioning","title":"Dependency Provisioning","text":"<p>You can use this chart to configure and automatically deploy the following components. If enabled, they will automatically be configured in a production-like configuration, although we do not recommend using them in production environments at this time.</p> <ul> <li>Postgres Database - Uses the CrunchyData Postgres Operator</li> <li>Kafka Cluster - Uses the Strimzi Kafka Operator</li> <li>Coming soon...<ul> <li>MongoDB</li> </ul> </li> </ul> <p>With these components installed in your Kubernetes cluster, you can provision an entire Smile CDR stack, complete with persistent backed-up database and a Kafka cluster in about 5-10 mins. May take longer if your K8s cluster needs to autoscale to create more worker nodes first.</p>"},{"location":"charts/smilecdr/smilecdr/#security-features","title":"Security Features","text":"<p>It's no good having an easy to use Helm Chart if you cannot use it in a secure manner. As such, we have included the following features when running on Amazon EKS (Other providers to follow):</p> <ul> <li>IAM roles for service accounts(AWS Only) -   Smile CDR pods run with their own IAM role, independent and isolated from other workloads on the cluster.</li> <li>Secrets Store CSI Driver - Store secrets in a secure vault, and not in your code.<ul> <li>AWS SSCSI Provider - (Uses AWS Secrets Manager)</li> </ul> </li> <li>NEW!!! Enhanced pod security<ul> <li>Pods run as non-root, non-privileged</li> <li>Privilege escalation disabled</li> <li>Read-only root filesystem</li> <li>All container security capabilities disabled</li> </ul> </li> <li>Coming soon...<ul> <li>Execution role support in other cloud providers</li> <li>Support for other SSCSI providers</li> <li>Pod Security Policies</li> <li>Security Groups For Pods</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#reliability-features","title":"Reliability Features","text":"<ul> <li>High availability when running 2 or more Pods</li> <li>Self healing - Failed pods get restarted</li> <li>Pod disruption budgets (Prevents accidental outages)</li> </ul>"},{"location":"charts/smilecdr/smilecdr/#operational-efficiency-features","title":"Operational Efficiency Features","text":"<ul> <li>Zero-downtime configuration changes (Using rolling deployments)</li> <li>Horizontal Auto-Scaling (Within bounds of Smile CDR licence) - to ensure cost effective use of compute resources</li> <li>Coming soon...<ul> <li>Zero-downtime upgrades with controllable manual/automatic schema upgrades</li> <li>Management dashboard for consolidated logs and metrics gathering for all components in the deployment</li> </ul> </li> </ul>"},{"location":"charts/smilecdr/smilecdr/#changelog","title":"Changelog","text":""},{"location":"examples/","title":"Example Configurations","text":"<p>This section contains some example configurations and instructions to help get you started.</p>"},{"location":"examples/aws-dependencies/","title":"Configuring AWS Dependencies","text":"<p>This example shows how you would install dependencies in an AWS environment.</p> <p>We will go over the creation of the following AWS resources in preparation for deploying Smile CDR using the Helm Chart.</p> <ul> <li>IAM Role using IRSA</li> <li>AWS Secrets Manager</li> </ul> <p>Note: These resources are configured in a way that is not obvious to an AWS administrator that has not dealt with IRSA before, which is why we are including them here. For other AWS resources (Such as RDS, S3, Certificates Manager etc) conventional configurations can be used, so we do not cover them in this example at this time.</p> <p>You will need both of these if you are using the recommended method of storing your container registry secrets - using Secrets Store CSI Driver</p>"},{"location":"examples/aws-dependencies/#creating-iam-role-with-irsa","title":"Creating IAM Role with IRSA","text":"<p>To set up an IAM Role to be used by the application pods, we use IRSA (IAM Roles for Service Accounts). Detailed information and instructions for IRSA are located here</p> <p>In this example, we will be creating the role to work with an deployment of Smile CDR in a fictional EKS cluster with the following properties:</p> <ul> <li>AWS Region <code>us-east-1</code></li> <li>Cluster Name <code>mycluster</code></li> <li>Namespace <code>smilecdr</code></li> <li>Helm Release Name <code>my-smile</code></li> </ul> <p>These are important as they will be referenced in the trust policy.</p>"},{"location":"examples/aws-dependencies/#iam-policy","title":"IAM Policy","text":"<p>Before starting, you need to determine which AWS services need to be accessed by this role. Typical examples would be:</p> <ul> <li>AWS Secrets Manager - Used for storing credentials for container repository and database</li> <li>RDS - Required if Smile CDR is configured to use IAM authentication for RDS</li> <li>S3 - Required if you are including extra files using the external method</li> </ul> <p>You will need to keep these resources in mind when creating your IAM Policy.</p> <p>Note: When creating IAM Policies, you should keep the principle of least privilege in mind and only allow the minimum required access for the resources needed. Avoid using wildcard entries for <code>actions</code> and <code>resources</code> where possible.</p> <p>In this example, we will create a policy that only has access to the container repository secret that we create below</p> <p>Following the AWS CLI instructions from here we would do the following:</p> <ol> <li> <p>Create an IAM policy file</p> <p>Create IAM Policy file with the following content: <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": \"secretsmanager:GetSecretValue\",\n\"Resource\": \"arn:aws:secretsmanager:us-east-1:&lt;accountid&gt;:secret:demo/dockerpull-??????\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"kms:Encrypt\",\n\"kms:Decrypt\"\n],\n\"Resource\": \"arn:aws:kms:*:&lt;accountid&gt;:aws/secretsmanager\"\n}\n]\n}\n</code></pre></p> <p>Note: The <code>??????</code> is a wildcard that matches the random suffix added to an AWS Secrets Manager Secret. See here for more info.</p> </li> <li> <p>Create the IAM Policy <pre><code>aws iam create-policy --policy-name smilecdr-dockersecret-policy --policy-document file://my-policy.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#iam-role","title":"IAM Role","text":"<p>When creating the IAM role, it needs to be associated with the Kubernetes service account via a trust policy. To do this, we need a few details in advance:</p> <p>AWS Account number <pre><code>account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\n</code></pre></p> <p>EKS cluster's OIDC provider <pre><code>oidc_provider=$(aws eks describe-cluster --name mycluster --region us-east-1 --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\")\n</code></pre> Namespace and ServiceAccount Resource Names</p> <p>In the case of this example, we are using the <code>smilecdr</code> namespace, with the <code>my-smile</code> release name as mentioned above. This will result in a Service Account with the name <code>my-smile-smilecdr</code> <pre><code>export namespace=smilecdr\nexport service_account=my-smile-smilecdr\n</code></pre></p> <ol> <li> <p>Create trust policy file</p> <pre><code>cat &gt;trust-relationship.json &lt;&lt;EOF\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\"\n    },\n    \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n    \"Condition\": {\n        \"StringEquals\": {\n        \"$oidc_provider:aud\": \"sts.amazonaws.com\",\n        \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\"\n        }\n    }\n    }\n]\n}\nEOF\n</code></pre> </li> <li> <p>Create the IAM Role <pre><code>aws iam create-role --role-name smilecdr-role --assume-role-policy-document file://trust-relationship.json --description \"Smile CDR Application Role\"\n</code></pre></p> </li> <li> <p>Attach the IAM Policy to the role <pre><code>aws iam attach-role-policy --role-name smilecdr-role --policy-arn=arn:aws:iam::$account_id:policy/smilecdr-dockersecret-policy\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#using-iam-role-in-helm-values","title":"Using IAM Role in Helm Values","text":"<p>Configure your values file to use this role for the Service Account like so: <pre><code>serviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: arn:aws:iam::&lt;account id&gt;:role/smilecdr-role\n</code></pre></p> <p>Now, when you deploy Smile CDR, it will use the above IAM role whenever accessing AWS resources.</p>"},{"location":"examples/aws-dependencies/#creating-aws-secrets-manager-secrets","title":"Creating AWS Secrets Manager Secrets","text":"<p>Secrets can be a bit of a chicken-and-egg problem.</p> <p>If you want to avoid storing secrets in code, by using a secrets vault, how do you do this 'via code'?</p> <p>One mechanism is to get the vault software to generate a random secret, or rotate the secret after the initial creation. These are not always viable options, which is certainly the case for storing secrets to access external systems, such as a container registry.</p> <p>In this example, we will create the <code>docker pull</code> secret manually via the AWS CLI. You could just as easily deploy the secret using some other mechanism and then update it with the cli or with the AWS console.</p>"},{"location":"examples/aws-dependencies/#create-secret","title":"Create Secret","text":"<p>The value of Kubernetes <code>imagePullSecrets</code> needs to be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p> <p>This essentially means the secret value is a Json string representing the Docker <code>config.json</code> file. As an AWS Secrets Manager secret consists of a Json map of secrets and values, we end up with a nested Json data structure.</p> <p>The easiest way to pass this to the AWS CLI command is to temporarily store the Json in a file which can be passed in to the <code>create-secret</code> command as a parameter.</p> <ol> <li>Create the temporary password json file</li> </ol> <p>Update your user &amp; password before running the below.</p> <pre><code>cat &gt;dockerconf.json &lt;&lt;EOF\n{\n  \"auths\":{\n    \"docker.smilecdr.com\":{\n      \"auth\": \"$(echo -n \"user:password\" | base64)\"\n    }\n  }\n}\nEOF\ncat &gt;tempsecret.json &lt;&lt;EOF\n{\n  \"dockerconfigjson\": \"$(cat dockerconf.json)\"\n  }\n}\nEOF\n</code></pre> <ol> <li> <p>Create the Secrets Manager Secret <pre><code>aws secretsmanager create-secret \\\n--name \"demo/dockerpull\" \\\n--secret-string file://tempsecret.json\n</code></pre></p> <p>Note: The name <code>demo/dockerpull</code> is just an example. You may use any scheme that you like for secret names. If you have an existing standard, use that.</p> </li> <li> <p>Remove the temporary secret file <pre><code>rm tempsecret.json\n</code></pre></p> </li> </ol>"},{"location":"examples/aws-dependencies/#update-secret","title":"Update Secret","text":"<p>Use the following to update the secret with a new value if required.</p> <pre><code>aws secretsmanager update-secret \\\n--secret-id \"demo/dockerpull\" \\\n--secret-string file://tempsecret.json\n</code></pre> <p>Note: You need to update rather than delete and recreate, as AWS Secrets Manager implements a grace period on secrets deletion.</p>"},{"location":"examples/aws-dependencies/#use-the-secret","title":"Use The Secret","text":"<p>If the IAM Role, IAM Policies and Helm Values were all set up correctly as per the above steps, you will now be able to launch pods using images from the authenticated container repository.</p>"},{"location":"examples/external-db-multi/","title":"Multiple External DB Configuration","text":"<p>This example demonstrates using multiple external Postgres Databases.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Separate databases for Cluster Manager, Audit logs and default Persistence module</li> </ul>"},{"location":"examples/external-db-multi/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Multiple external Postgres databases provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published Json structure</li> </ul>"},{"location":"examples/external-db-multi/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\ndatabase:\nexternal:\nenabled: true\ncredentials:\ntype: sscsi\nprovider: aws\ndatabases:\n- secretName: clustermgrSecret\nsecretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\nmodule: clustermgr\n- secretName: auditSecret\nsecretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:auditSecret\"\nmodule: audit\n- secretName: persistenceSecret\nsecretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:persistenceSecret\"\nmodule: persistence\n</code></pre>"},{"location":"examples/external-db/","title":"External DB Configuration","text":"<p>This example demonstrates using an external Postgres Database.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>External database credentials and connection info passed in via Secret Store CSI Driver using AWS Secrets Manager</li> </ul>"},{"location":"examples/external-db/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>External Postgres database provisioned and accessible from the Kubernetes cluster</li> <li>Database credentials stored in AWS Secrets Manager using the published Json structure</li> </ul>"},{"location":"examples/external-db/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\ndatabase:\nexternal:\nenabled: true\ncredentials:\ntype: sscsi\nprovider: aws\ndatabases:\n- secretName: clustermgrSecret\nsecretArn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:clustermgrSecret\"\nmodule: clustermgr\n</code></pre>"},{"location":"examples/extra-files-external/","title":"Adding Files Configuration","text":"<p>This example demonstrates passing in extra files to the deployment from external sources.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR, using Amazon S3</li> <li>Elastic APM <code>.jar</code> file will be included in the <code>customerlib</code> directory in Smile CDR, using curl</li> </ul>"},{"location":"examples/extra-files-external/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>Amazon S3 bucket with a customized <code>logback.xml</code> file copied to a <code>classes</code> folder</li> <li>AWS IAM Role configured to access:<ul> <li>AWS Secrets Manager</li> <li>Amazon S3 bucket</li> </ul> </li> </ul>"},{"location":"examples/extra-files-external/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\ncopyFiles:\nclasses:\nsources:\n# Copies files recursively from S3 to the classes directory\n- type: s3\nbucket: s3-bucket-name\n# The below S3 bucket prefix must contain the custom logback.xml\n# file, as well as any other needed files.\npath: /path-to/classes\ncustomerlib:\nsources:\n# Downloads a single file using curl to the customerlib directory\n# (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n- type: curl\nfileName: elastic-apm/elastic-apm-agent-1.13.0.jar\nurl: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre>"},{"location":"examples/extra-files-helm/","title":"Adding Files Using Helm","text":"<p>This example demonstrates passing in extra files to the deployment via the Helm Chart.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> <li>Custom <code>logback.xml</code> file will be included in the <code>classes</code> directory in Smile CDR</li> </ul>"},{"location":"examples/extra-files-helm/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> <li>Customized <code>logback.xml</code> file available in your configuration repo/folder</li> </ul>"},{"location":"examples/extra-files-helm/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\nmappedFiles:\nlogback.xml:\ntype: configMap\npath: /home/smile/smilecdr/classes\n</code></pre>"},{"location":"examples/extra-files-helm/#extra-install-steps","title":"Extra Install Steps","text":"<p>To use this feature, you must update your <code>helm upgrade</code> command to include <code>--set-file mappedFiles.logback\\\\.xml.data=logback.xml</code></p>"},{"location":"examples/minimal/","title":"Minimal Configuration","text":"<p>This values file demonstrates the minimal required configurations to install Smile CDR in a secure manner (i.e. not passing repository secrets)</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/minimal/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/minimal/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\n</code></pre>"},{"location":"examples/modules-advanced/","title":"Advanced Modules Configuration","text":"<p>This example demonstrates defining all modules from scratch, not using any of the default modules.</p> <p>It is based on the minimal example.</p> <p>We will create a couple of examples.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Completely custom Smile CDR module configuraton<ul> <li>Disabled default module configuraton</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-advanced/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-advanced/#example-1-minimal-module-config","title":"Example 1 - Minimal module config","text":"<p>This example shows how you would configure Smile CDR to just use the following modules:</p> <ul> <li>Cluster Manager</li> <li>Persistence Module</li> <li>Local Security</li> <li>Admin Web</li> <li>Fhir Endpoint</li> </ul> <pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\nmodules:\nusedefaultmodules: false\nclustermgr:\nname: Cluster Manager Configuration\nenabled: true\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.schema_update_mode:  UPDATE\nstats.heartbeat_persist_frequency_ms: 15000\nstats.stats_persist_frequency_ms: 60000\nstats.stats_cleanup_frequency_ms: 300000\naudit_log.request_headers_to_store: Content-Type,Host\nseed_keystores.file: \"classpath:/config_seeding/keystores.json\"\npersistence:\nname: Database Configuration\nenabled: true\ntype: PERSISTENCE_R4\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.hibernate.showsql: false\ndb.hibernate_search.directory: ./database/lucene_fhir_persistence\ndb.schema_update_mode: UPDATE\ndao_config.expire_search_results_after_minutes: 60\ndao_config.allow_multiple_delete.enabled: false\ndao_config.allow_inline_match_url_references.enabled: false\ndao_config.allow_external_references.enabled: false\ndao_config.inline_resource_storage_below_size: 4000\nlocal_security:\nname: Local Storage Inbound Security\nenabled: true\ntype: SECURITY_IN_LOCAL\nconfig:\nseed.users.file: classpath:/config_seeding/users.json\n# This is required right now as the default is not being honored.\n# Can be removed if the default gets fixed. May be good to leave it explicit.\n# Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\npassword_encoding_type: BCRYPT_12_ROUND\nadmin_web:\nname: Web Admin\nenabled: true\ntype: ADMIN_WEB\nservice:\nenabled: true\nsvcName: admin-web\nhostName: default\nrequires:\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: \"\"\nport: 9100\ntls.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\nfhir_endpoint:\nname: FHIR Service\nenabled: true\ntype: ENDPOINT_FHIR_REST_R4\nservice:\nenabled: true\nsvcName: fhir\nhostName: default\nrequires:\nPERSISTENCE_R4: persistence\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: fhir_request\nport: 8000\nbase_url.fixed: default\nthreadpool.min: 2\nthreadpool.max: 10\nbrowser_highlight.enabled: true\ncors.enable: true\ndefault_encoding: JSON\ndefault_pretty_print: true\ntls.enabled: false\nanonymous.access.enabled: true\nsecurity.http.basic.enabled: true\nrequest_validating.enabled: false\nrequest_validating.fail_on_severity: ERROR\nrequest_validating.tags.enabled: false\nrequest_validating.response_headers.enabled: false\nrequest_validating.require_explicit_profile_definition.enabled:  false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\n</code></pre>"},{"location":"examples/modules-advanced/#example-2-minimal-module-config-with-separate-values-files","title":"Example 2 - Minimal module config with separate values files","text":"<p>As you can see from the above, the values file can start getting unwieldy.</p> <p>It is advised to split them up into manageable chunks like so.</p> <p>values-common.yaml <pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\nmodules:\nusedefaultmodules: false\n</code></pre></p> <p>values-clustermgr.yaml <pre><code>modules:\nclustermgr:\nname: Cluster Manager Configuration\nenabled: true\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.schema_update_mode:  UPDATE\nstats.heartbeat_persist_frequency_ms: 15000\nstats.stats_persist_frequency_ms: 60000\nstats.stats_cleanup_frequency_ms: 300000\naudit_log.request_headers_to_store: Content-Type,Host\nseed_keystores.file: \"classpath:/config_seeding/keystores.json\"\n</code></pre></p> <p>values-persistence-r4.yaml <pre><code>modules:\npersistence:\nname: Database Configuration\nenabled: true\ntype: PERSISTENCE_R4\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.hibernate.showsql: false\ndb.hibernate_search.directory: ./database/lucene_fhir_persistence\ndb.schema_update_mode: UPDATE\ndao_config.expire_search_results_after_minutes: 60\ndao_config.allow_multiple_delete.enabled: false\ndao_config.allow_inline_match_url_references.enabled: false\ndao_config.allow_external_references.enabled: false\ndao_config.inline_resource_storage_below_size: 4000\n</code></pre></p> <p>values-security.yaml <pre><code>modules:\nlocal_security:\nname: Local Storage Inbound Security\nenabled: true\ntype: SECURITY_IN_LOCAL\nconfig:\nseed.users.file: classpath:/config_seeding/users.json\n# This is required right now as the default is not being honored.\n# Can be removed if the default gets fixed. May be good to leave it explicit.\n# Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\npassword_encoding_type: BCRYPT_12_ROUND\n</code></pre></p> <p>values-admin-web.yaml <pre><code>modules:\nadmin_web:\nname: Web Admin\nenabled: true\ntype: ADMIN_WEB\nservice:\nenabled: true\nsvcName: admin-web\nhostName: default\nrequires:\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: \"\"\nport: 9100\ntls.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\n</code></pre></p> <p>values-fhir-endpoint.yaml <pre><code>modules:\nfhir_endpoint:\nname: FHIR Service\nenabled: true\ntype: ENDPOINT_FHIR_REST_R4\nservice:\nenabled: true\nsvcName: fhir\nhostName: default\nrequires:\nPERSISTENCE_R4: persistence\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: fhir_request\nport: 8000\nbase_url.fixed: default\nthreadpool.min: 2\nthreadpool.max: 10\nbrowser_highlight.enabled: true\ncors.enable: true\ndefault_encoding: JSON\ndefault_pretty_print: true\ntls.enabled: false\nanonymous.access.enabled: true\nsecurity.http.basic.enabled: true\nrequest_validating.enabled: false\nrequest_validating.fail_on_severity: ERROR\nrequest_validating.tags.enabled: false\nrequest_validating.response_headers.enabled: false\nrequest_validating.require_explicit_profile_definition.enabled:  false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\n</code></pre></p> <p>When installing the above, you would then pass in the multiple values file like so: <pre><code>helm upgrade -i my-smile-env -f values-clustermgr.yaml -f values-persistence-r4.yaml -f values-security.yaml -f values-admin-web.yaml -f values-fhir-endpoint.yaml smiledh/smilecdr\n</code></pre></p>"},{"location":"examples/modules-simple/","title":"Basic Modules Configuration","text":"<p>This example demonstrates simple reconfiguration of Smile CDR modules.</p> <p>It is based on the minimal example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Modified Smile CDR module configuraton</li> <li>We will only modify <code>dao_config.inline_resource_storage_below_size</code> for the persistence database</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/modules-simple/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/modules-simple/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\nmodules:\npersistence:\nconfig:\ndao_config.inline_resource_storage_below_size: 4000\n</code></pre>"},{"location":"examples/previousrootversion/","title":"Using Previous (root) Version","text":"<p>This example demonstrates running an older version of Smile CDR that requires the container to be running as root.</p> <p>This only applies if using versions <code>2022.11.R04</code> or earlier.</p> <p>It is based on the Advanced Modules Configuration example.</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Container explicitly set to run as root</li> <li>Completely custom Smile CDR module configuraton<ul> <li>Disabled default module configuraton, including audit and license modules</li> </ul> </li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via Secret Store CSI Driver using AWS Secrets Manager</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/previousrootversion/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Image repository credentials stored in AWS Secrets Manager</li> <li>AWS IAM Role configured to access AWS Secrets Manager</li> </ul>"},{"location":"examples/previousrootversion/#example-config","title":"Example Config","text":"Click to expand <pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ntag: \"2022.11.R04\"\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nsecurityContext:\nrunAsNonRoot: false\nrunAsUser: 0\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\nmodules:\n# Define modules to be used. Some of these will contain service definitions.\n# A service and an ingress rule will be created for modules that use services.\n# Canonical endpoint URLs will be generated by _smile-module-helpers.tpl and\n# populated in the smilecdr.services variable. These can be consumed by other\n# modules that reference them.\nusedefaultmodules: false\nclustermgr:\nname: Cluster Manager Configuration\nenabled: true\nconfig:\n# Valid options include H2_EMBEDDED, DERBY_EMBEDDED, MYSQL_5_7, MARIADB_10_1, POSTGRES_9_4, ORACLE_12C, MSSQL_2012\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.schema_update_mode:  UPDATE\nstats.heartbeat_persist_frequency_ms: 15000\nstats.stats_persist_frequency_ms: 60000\nstats.stats_cleanup_frequency_ms: 300000\naudit_log.request_headers_to_store: Content-Type,Host\nseed_keystores.file: \"classpath:/config_seeding/keystores.json\"\ntransactionlog.enabled: false\nretain_transaction_log_days: 7\npersistence:\nname: Database Configuration\nenabled: true\ntype: PERSISTENCE_R4\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\ndb.hibernate.showsql: false\ndb.hibernate_search.directory: ./database/lucene_fhir_persistence\ndb.schema_update_mode: UPDATE\ndao_config.expire_search_results_after_minutes: 60\ndao_config.allow_multiple_delete.enabled: false\ndao_config.allow_inline_match_url_references.enabled: true\ndao_config.allow_external_references.enabled: false\ndao_config.inline_resource_storage_below_size: 4000\nadmin_json:\nname: JSON Admin Services\nenabled: true\ntype: ADMIN_JSON\nservice:\nenabled: true\nsvcName: admin-json\nrequires:\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: json-admin\nport: 9000\ntls.enabled: false\nanonymous.access.enabled: true\nsecurity.http.basic.enabled: true\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\nlocal_security:\nname: Local Storage Inbound Security\nenabled: true\ntype: SECURITY_IN_LOCAL\nconfig:\nseed.users.file: classpath:/config_seeding/users.json\n# This is required right now as the default is not being honored.\n# Can be removed if the default gets fixed. May be good to leave it explicit.\n# Note: Smile CDR still chooses the wrong default as of `2022.11.R01`\npassword_encoding_type: BCRYPT_12_ROUND\nsubscription:\nname: Subscription\nenabled: true\ntype: SUBSCRIPTION_MATCHER\nrequires:\nPERSISTENCE_ALL: persistence\nadmin_web:\nname: Web Admin\nenabled: true\ntype: ADMIN_WEB\nservice:\nenabled: true\nsvcName: admin-web\nhostName: default\nrequires:\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: \"\"\nport: 9100\ntls.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\nfhirweb_endpoint:\nname: FHIRWeb Console\nenabled: true\ntype: ENDPOINT_FHIRWEB\nservice:\nenabled: true\nsvcName: fhirweb\nrequires:\nSECURITY_IN_UP: local_security\nENDPOINT_FHIR: fhir_endpoint\nconfig:\ncontext_path: fhirweb\nport: 8001\nthreadpool.min: 2\nthreadpool.max: 10\ntls.enabled: false\nanonymous.access.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\n# Fhir Endpoint\nfhir_endpoint:\nname: FHIR Service\nenabled: true\ntype: ENDPOINT_FHIR_REST_R4\nservice:\nenabled: true\nsvcName: fhir\nhostName: default\nrequires:\nPERSISTENCE_R4: persistence\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: fhir_request\nport: 8000\nbase_url.fixed: default\nthreadpool.min: 2\nthreadpool.max: 10\nbrowser_highlight.enabled: true\ncors.enable: true\ndefault_encoding: JSON\ndefault_pretty_print: true\ntls.enabled: false\nanonymous.access.enabled: true\nsecurity.http.basic.enabled: true\nrequest_validating.enabled: false\nrequest_validating.fail_on_severity: ERROR\nrequest_validating.tags.enabled: false\nrequest_validating.response_headers.enabled: false\nrequest_validating.require_explicit_profile_definition.enabled:  false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\nsmart_auth:\nname: SMART Security\nenabled: true\ntype: SECURITY_OUT_SMART\nservice:\nenabled: true\nsvcName: smart-auth\nrequires:\nCLUSTERMGR: clustermgr\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: smartauth\nport: 9200\nopenid.signing.keystore_id: default-keystore\nissuer.url: default\ntls.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\npackage_registry:\nname: Package Registry\nenabled: true\ntype: ENDPOINT_PACKAGE_REGISTRY\nservice:\nenabled: true\nsvcName: pkg-registry\nrequires:\nPACKAGE_CACHE: persistence\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: package_registry\nport: 8002\ntls.enabled: false\nanonymous.access.enabled: true\nsecurity.http.basic.enabled: true\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\n</code></pre>"},{"location":"examples/quickstart/","title":"Quickstart Configuration","text":"<p>This is the configuration used in the Quickstart</p> <p>This will configure Smile CDR as follows:</p> <ul> <li>Default Smile CDR module configuraton</li> <li>Ingress configured for <code>smilecdr.mycompany.com</code> using NginX Ingress</li> <li>Docker registry credentials passed in via values file (Don't do this!)</li> <li>Postgres DB automatically created</li> </ul>"},{"location":"examples/quickstart/#requirements","title":"Requirements","text":"<ul> <li>Nginx Ingress Controller must be installed, with TLS certificate</li> <li>DNS for <code>smilecdr.mycompany.com</code> needs to be exist and be pointing to the load balancer used by Nginx Ingress</li> <li>CrunchyData Operator must be installed</li> <li>Credentials to an image repository with the official Smile CDR images.</li> </ul>"},{"location":"examples/quickstart/#values-file","title":"Values File","text":"<pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: values\nvalues:\n- registry: docker.smilecdr.com\nusername: &lt;DOCKER_USERNAME&gt;\npassword: &lt;DOCKER_PASSWORD&gt;\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\n</code></pre>"},{"location":"guide/","title":"Chart Guides","text":"<p>The guides in this section cover all details of configuration using the Helm Charts</p>"},{"location":"guide/helm-repo/","title":"Helm Repository Setup","text":""},{"location":"guide/helm-repo/#configure-helm-repository","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>This will differ, depending on the method you will be using to deplpy the charts.</p>"},{"location":"guide/helm-repo/#native-helm","title":"Native Helm","text":"<p>The simplest way to get up and running is by using the native <code>helm</code> commands.</p> <p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"guide/helm-repo/#terraform","title":"Terraform","text":"<p>If installing the chart using Terraform, you may have a resource definition like so:</p> <pre><code>resource \"helm_release\" \"example\" {\n  name       = \"my-smilecdr-release\"\n  repository = \"https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\"\n  chart      = \"smilecdr\"\n  version    = \"~1.0.0\"\n\n  values = [\n    \"${file(\"my-values.yaml\")}\"\n  ]\n\n  set {\n    name  = \"values.override\"\n    value = \"value\"\n  }\n}\n</code></pre> <p>See the Terraform Helm Provider for more info on this.</p>"},{"location":"guide/helm-repo/#argocd","title":"ArgoCD","text":"<p>If installing in ArgoCD using an <code>Application</code> Custom Resource, you will need to create a custom 'Umbrella Chart' for your deployment so that you can pass in your values file (And any other files).</p> <p>To do this, you would create a configuration directory with your configuration files as well as a <code>Chart.yaml</code> file that may look like this:</p> <pre><code>apiVersion: v2\nname: umbrella-smilecdr\ndescription: An Umbrella Helm chart to deploy the Smile CDR Helm Chart\n# This Umbrella Helm Chart can be used to deploy Smile CDR in ArgoCD while\n# passing in your values files.\ntype: application\nversion: 1.0.0\n# Remember, when passing values files in to dependency charts, the entire yaml map needs to be\n# moved to a root key that matches the `name` of the dependency.\ndependencies:\n- name: smilecdr\nversion: \"~1.0.0-pre.10\"\nrepository: \"https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\"\n</code></pre>"},{"location":"guide/helm-repo/#provide-repo-credentials","title":"Provide Repo Credentials","text":"<p>As Smile CDR is not a publicly available product, it is not possible to use it from the container repository without providing credentials.</p> <p>The same may be the case if you have custom Docker images for Smile CDR that you have built yourself and published to your own private container registry.</p> <p>In either case, you will need to provide credentials that can be used by Kubernetes to pull the image.</p> <p>In Kubernetes, to pull from a private container registry, you need to provide the <code>imagePullSecrets</code> option in the <code>Pod</code> spec. To do this, you need a Kubernetes <code>Secret</code> object in the same namespace.</p> <p>Note For more information on using private container registries with Kubernetes, see the official documentation here</p> <p>As described in the Secrets Handling section of these docs, we support multiple methods to provide secrets.</p>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-secrets-store-csi-driver","title":"Configuring Repo Credentials using Secrets Store CSI Driver","text":"<p>Before using this configuration in your values file, ensure that you have followed the appropriate section in the Secrets Handling guide to set up Secrets Store CSI, the AWS Provider, your AWS Secret, your IAM Role and configured the <code>ServiceAccount</code>.</p> <p>Once you have done that, you can enable it like so:</p> <ul> <li>Specify the <code>image.credentials.type</code> as <code>sscsi</code></li> <li>Specify the <code>image.credentials.provider</code> as <code>aws</code></li> <li>Specify the AWS Secret ARN in <code>image.credentials.secretarn</code></li> </ul> <p>It would look like this: <pre><code>image:\ncredentials:\ntype: sscsi\nprovider: aws\nsecretarn: \"arn:aws:secretsmanager:us-east-1:1234567890:secret:secretname\"\nserviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/example-role-name\"\n</code></pre></p>"},{"location":"guide/helm-repo/#configuring-repo-credentials-using-kubernetes-secret","title":"Configuring Repo Credentials using Kubernetes Secret","text":"<p>Before using this configuration, you need to create a Kubernetes <code>secret</code> of type <code>kubernetes.io/dockerconfigjson</code>. For more info on this, refer to the Kubernetes section in Secrets Handling.</p> <p>Once your Kubernetes <code>Secret</code> object is created, you can use it like so:</p> <ul> <li>Specify the <code>image.credentials.type</code> as <code>externalsecret</code></li> <li>Reference the Secret name in <code>image.credentials.pullSecrets[0].name</code></li> </ul> <p>It would look like this in your custom values file: <pre><code>image:\ncredentials:\ntype: externalsecret\npullSecrets:\n- name: mysecretname\n</code></pre></p>"},{"location":"guide/secrets/","title":"Secrets Handling","text":"<p>Secrets management can be a hard subject to get right. Unfortunately, the easy way quite often lacks basic security considerations and can lead to unexpected data compromises.</p> <p>At Smile Digital Health, we take security very seriously, so we have designed these Helm Charts in a way that follows best practices, to reduce the likelihood of such compromises.</p>"},{"location":"guide/secrets/#secrets-best-practices","title":"Secrets Best Practices","text":"<p>There are multiple areas in these Helm Charts where secret values need to be used. These secret values may be required at the Kubernetes level, such as when pulling container images from private repositories, or at the application level, such as when connecting to databases or other external systems that require authentication.</p>"},{"location":"guide/secrets/#dont-store-secrets-in-your-configuration-code","title":"Don't Store Secrets in your Configuration Code","text":"<p>While it is easy to create secrets, such as passwords or API keys, in code to simplify provisioning and automation, it is generally considered bad practice to include them because it can compromise the security of your system.</p> <p>If the code containing the secrets is somehow made publicly available, anyone with access to the code can potentially gain access to the protected resources. Additionally, if the code is shared among multiple team members, it can be difficult to keep track of who has access to the secrets and when they were last rotated.</p>"},{"location":"guide/secrets/#use-secrets-management-tools","title":"Use Secrets Management Tools","text":"<p>It is generally more secure to use a secrets management tool to store and manage secrets separately from the code. This way, secrets can be more easily rotated and access can be tightly controlled.</p> <p>Various secrets management tools are available that allow you to store secrets in a secure, centralized location and control access to them through granular permissions. This helps ensure that only authorized personnel have access to sensitive information and helps prevent accidental disclosure of secrets.</p>"},{"location":"guide/secrets/#supported-secret-mechanisms","title":"Supported Secret Mechanisms","text":"<p>These Helm Charts support the following three methods to reference secrets.</p> Method Security Difficulty Notes Secrets Store CSI High Hardest Recommended method. You will need the SSCSI driver, an appropriate SSCSI provider, A secrets vault Secret and any IAM roles configured to access the secret Kubernetes Secret Medium Medium Need to manually set up Kubernetes Secret Values File Low Easiest K8s secret created by chart. Password is in your code (Bad)"},{"location":"guide/secrets/#secrets-store-csi-driver","title":"Secrets Store CSI Driver","text":"<p>Using the Secrets Store CSI Driver(SSCSI) is the preferred method to configure secrets in these Helm Charts.</p> <p>This mechanism is recommended by AWS, Azure and Google to retrieve secrets from their respective secret management services. It also has support for other Secret Vault providers such as HashiCorp Vault.</p> <p>Currently, the Smile CDR Helm Chart only supports the Secrets Store CSI Driver with the AWS Secrets Manager provider.</p> <p>Before you can use this method in your configuration, you will need to set up some pre-requisites.</p> <ul> <li>Create your secret in AWS Secrets Manager.<ul> <li>The secret data should be in a suitabe structured Json format, as described here</li> <li>Your secret should be encrypted using an AWS CMK (Customer Managed Key).</li> </ul> </li> <li>Create an IAM role and trust policy.</li> <li>Enable the <code>ServiceAccount</code> and reference the IAM role in the annotations.</li> <li>See the Service Account Configuration section for more details on setting this up.</li> </ul> <p>The way the secret is configured in your <code>values</code> file differs depending on the section of configuration. Please refer to the individual sections below for details:</p> <ul> <li>Image Repository Credentials</li> <li>Database Credentials</li> </ul>"},{"location":"guide/secrets/#kubernetes-secret","title":"Kubernetes Secret","text":"<p>Alternatively, you can create the Kubernetes <code>Secret</code> object through some other method. Although it avoids the secret data being included in your code, it does not provide a centralized location to store, manage and controll access to secrets.</p> <p>Be wary of including custom Kubernetes <code>Secret</code> manifests alongside your Helm values files. Although this is a convenient way to provision them, it just re-introduces the problem of secrets residing in your code, which is what we are trying to avoid.</p>"},{"location":"guide/secrets/#values-file","title":"Values File","text":"<p>Finally, we do support providing credentials in the values file itself. This is not a recommended solution and really only intended as a quickstart method to allow for quick experimentation with the charts.</p> <p>We may block this functionality in future versions of the charts as it can lead to insecure habits/practices forming.</p>"},{"location":"guide/serviceaccount/","title":"Service Accounts","text":"<p>Certain features of the application, when installed with these Helm Charts, require authorized access to external systems so that they can function correctly.</p> <p>Examples of this include:</p> <ul> <li>Retrieving credentials from secrets management systems (e.g. AWS Secrets Manager)</li> <li>Accessing AWS managed services such as:<ul> <li>RDS authentication using IAM roles</li> <li>AWS HealthLake</li> <li>Accessing Amazon MSK (Managed Kafka)</li> <li>Accessing S3 buckets</li> </ul> </li> <li>Waiting for Kubernetes jobs to complete (i.e. during product upgrades and migration tasks)</li> </ul> <p>As explained in the Secrets Handling section above, passing in secrets (such as AWS Access Keys &amp; Tokens etc) directly to your configuration is a dangerous practice. Instead we can use the mechanisms provided by various infrastructure providers to use secure methods to gain access to these external systems.</p>"},{"location":"guide/serviceaccount/#iam-roles-for-service-accounts-irsa","title":"IAM Roles For Service Accounts (IRSA)","text":"<p>To give the application access to AWS resources, we use IAM Roles For Service Accounts, also known as IRSA. This attaches AWS IAM roles to a Kubernetes <code>ServiceAccount</code> which then gets attached to the application Pods.</p> <p>As a result of this, the application can access AWS services without needing to directly pass in AWS IAM User credentials.</p> <p>Note Currently, the Smile CDR Helm Chart only supports this integration in AWS, but support for other cloud providers will be added.</p>"},{"location":"guide/serviceaccount/#service-account-configuration","title":"Service Account Configuration","text":"<p>To use this feature, you will need to enable the Service Account and reference the IAM role that it should be connected to. Note that the IAM role being used needs to have the appropriate Trust Policy set up so that it can be used by your Cluster. More info and instructions are available here</p> <ul> <li>Create an IAM role for your deployment.<ul> <li>This role will be used for any Role-based access that the application pods need,   so name it accordingly to avoid confusion, e.g. <code>smilecdr-role</code></li> <li>If being used for Secrets Store CSI, ensure that it has read access to the secrets it will need to provide, and any KMS key used to encrypt them.</li> </ul> </li> <li>Create a trust policy for the IAM role so that it can be used with IRSA. Instructions   here<ul> <li>These instructions use the <code>eksctl</code> command which abstracts away some details into CloudFormation templates. Using something like Terraform would require different steps.</li> </ul> </li> </ul> <p>Once the IAM role is set up correctly, enable the <code>ServiceAccount</code> and reference the IAM role in the annotations in your values file like so: <pre><code>serviceAccount:\ncreate: true\nannotations:\neks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/example-role-name\n</code></pre></p>"},{"location":"guide/serviceaccount/#examples","title":"Examples","text":"<p>Examples are available that show how you would correctly set up the IAM Policy, IAM Role, Trust Policy and Secret Manager dependencies. See here</p>"},{"location":"guide/values-files-management/","title":"Managing Values Files","text":"<p>As explained in the quick start, you should configure your instance of Smile CDR using Helm Values Files.</p>"},{"location":"guide/values-files-management/#create-new-values-files","title":"Create New Values Files","text":"<p>It is generally recommended to create a new, empty values file rather than copying the default values.yaml file from the Helm chart. The default values file can be lengthy and may contain values that are not relevant or suitable for your specific deployment.</p> <p>By starting with a fresh values file, you can customize your configuration to only include the values that you need to override. This can help to make your values file more concise and easier to manage. Additionally, starting from a blank file allows you to ensure that your deployment is not impacted by future updates to the default values file, which could potentially cause issues if you are using an older version.</p> <p>Creating your own values file from scratch gives you greater control and flexibility over your Helm chart deployment. It can help to ensure that your deployment is secure and stable, as you have the ability to carefully consider and set the values that are most relevant to your specific needs.</p>"},{"location":"guide/values-files-management/#organizing-values-files","title":"Organizing Values Files","text":"<p>It is a common practice to put all Helm Chart configurations into a single values file and provide that to the <code>helm upgrade</code> command.</p> <p>Using multiple values files can be a more efficient way to manage configurations, particularly in complex environments. This approach can help to avoid having a single, large values file that may be difficult to read and maintain. By dividing the configuration into smaller, more focused files, it can be easier to manage and update the settings as needed.</p> <p>To use multiple values files, you would simply provide multiple <code>-f valuesfle.yaml</code> options on the <code>helm upgrade</code> command.</p>"},{"location":"guide/values-files-management/#multiple-environments","title":"Multiple Environments","text":"<p>When deploying an application, it is often necessary to consider multiple environments, such as dev, uat, and prod. While it is possible to create a separate configuration for each environment, this approach can lead to repetition and duplication of settings.</p> <p>This can be problematic, as it can result in configuration drift between environments. If there are changes that need to be made to the configuration, it can be challenging to ensure that the updates are applied consistently across all environments.</p> <p>TODO Insert Diagram explaining Drift</p> <p>A more effective approach may be to use a base configuration, with per-environment overlays. This allows you to define a set of common configuration settings that apply to all environments, while also allowing you to specify any environment-specific settings as needed. This can help to minimize repetition and ensure that the configuration is consistent across all environments.</p> <p>This can be easily achieved using multiple directories for the different environments like so:</p> <p>TODO Insert Diagram showing multiple configurations in a per-directory model</p>"},{"location":"guide/values-files-management/#modular-configurations","title":"Modular Configurations","text":"<p>Using multiple values files can also be a useful way to create modular units of configuration that can be easily included or excluded in your environment. This can help to make your configuration more flexible and adaptable to changing needs.</p> <p>For example, you might create a set of values files that represent different configurations or modules that have been fully tested and approved for use in your organization. These might include a base configuration file that defines the minimum requirements for running Smile CDR, as well as additional files for specific features or components, such as an R4 persistence module, a MongoDB persistence module, or an AWS Healthlake module.</p> <p>TODO Insert Diagram showing module files</p> <p>This approach allows you to build your configuration in a modular way, which can be more manageable and easier to maintain. It also gives you the flexibility to selectively include or exclude certain modules as needed, depending on the specific requirements of your environment.</p>"},{"location":"guide/values-files-management/#flexible-solution","title":"Flexible Solution","text":"<p>When it comes to managing values files, there isn't a single \"right\" way to do it - the approach that works best will depend on specific needs and organizational standards.</p> <p>Although the above techniques can be helpful for keeping things organized and efficient, they may not be right for you or your organization. You should use a technique that works for your team and organization. If this means using a single large values file per environment or some other technique, then that is fine.</p> <p>The aim here is to find a solution that helps you maintain a stable, well-organized configuration.</p>"},{"location":"guide/smilecdr/","title":"Smile CDR Helm Chart User Guide","text":"<p>This guide covers configuration details for the Smile CDR Helm Chart</p>"},{"location":"guide/smilecdr/#smile-cdr-configuration-options","title":"Smile CDR Configuration Options","text":"<p>Please note that for details on the Smile CDR configuration options, you should consult the official product documentation on the Smile CDR website</p>"},{"location":"guide/smilecdr/cdrversions/","title":"Supported Smile CDR Versions","text":"<p>By default, this Helm Chart supports the latest published version of the Smile CDR docker image.</p> <p>WARNING - Pre-release versions of this Helm Chart may default to pre-release versions of Smile CDR</p>"},{"location":"guide/smilecdr/cdrversions/#current-version","title":"Current Version","text":"<p>Versions <code>v1.0.0-pre.43</code> and newer of the chart support <code>2023.02.PRE-43</code> and newer.(There is no correlation between <code>pre.43</code> and <code>PRE-43</code>, this is purely co-incidence!)</p> <p>This version included some major changes from previous versions that cause some incompatibility.</p> <ul> <li>Pod is now configured with an enhanced security posture<ul> <li>Containers now run as non-root user</li> <li>Root filesystem is mounted read-only</li> <li>Extra ephemeral volumes are used for certain directories that need write access   e.g. logs, tmp</li> </ul> </li> <li>New Audit logging mechanism introduced that optionally utilizes separate database</li> <li>Licencing module introduced</li> </ul>"},{"location":"guide/smilecdr/cdrversions/#previous-versions","title":"Previous Versions","text":"<p>In order to run Smile CDR versions <code>2022.11</code> and earlier with this Helm Chart, you will need to make some extra configurations in your Helm values file.</p> <ul> <li>Disable running as non-root user</li> <li>Disable Audit and License modules</li> <li>Set the image tag</li> </ul> <p>As with any version of Smile CDR that you use with this Helm Chart, part of the deployment process is developing a set of Helm Values that works for your particular use case. This does not change that, but these settings may need to be added/changed if you already have a versions file that you have developed.</p>"},{"location":"guide/smilecdr/cdrversions/#disable-running-as-non-root-user","title":"Disable Running as non-root User","text":"<p>In order to do this, we must override the <code>podSecurityContext</code> that is defined in the default <code>values.yaml</code> file in the Helm Chart.</p>"},{"location":"guide/smilecdr/cdrversions/#default-valuesyaml","title":"Default <code>values.yaml</code>","text":"<pre><code>securityContext:\ncapabilities:\ndrop:\n- ALL\nreadOnlyRootFilesystem: true\nrunAsNonRoot: true\nrunAsUser: 1000\nprivileged: false\nallowPrivilegeEscalation: false\n</code></pre>"},{"location":"guide/smilecdr/cdrversions/#required-additions-to-my-valuesyaml-to-re-enable-running-as-root","title":"Required additions to <code>my-values.yaml</code> to re-enable running as root","text":"<pre><code>securityContext:\nrunAsNonRoot: false\nrunAsUser: 0\n</code></pre> <p>Note - You do not need to disable the capabilities dropping or the read only root file-system as previous versions of Smile CDR still function with these security enhancements in place.</p>"},{"location":"guide/smilecdr/cdrversions/#disable-audit-and-license-modules","title":"Disable Audit and License modules","text":"<p>In order to disable default modules, you need to set <code>useDefaultModules</code> to false. See here for more info.</p> <p>Then you need to explicityly define ALL modules that you need to configure.</p>"},{"location":"guide/smilecdr/cdrversions/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>modules:\nuseDefaultModules: false\nclustermgr:\n...\n</code></pre> <p>Note See examples section for a complete configuration showing this.</p>"},{"location":"guide/smilecdr/cdrversions/#set-image-tag","title":"Set Image Tag","text":"<p>Set <code>image.tag</code> to your required version</p>"},{"location":"guide/smilecdr/cdrversions/#my-valuesyaml_1","title":"<code>my-values.yaml</code>","text":"<pre><code>image:\ntag: \"2022.11.R04\"\n</code></pre> <p>Warning Do not forget this step when using this Helm Chart to update an existing installation that is running a previous version of Smile CDR.Doing so may automatically upgrade your database to the latest version. If you then revert back to the previous version, it may not function correctly with the updated DB schema.</p>"},{"location":"guide/smilecdr/database/","title":"Database Configuration","text":"<p>To use this chart, you must configure a database. There are two ways to do this:</p> <ul> <li>Use or provision an external database (or databases) using existing techniques/processes in your   organisation. Any external database can be referenced in this chart and Smile CDR will be configured   to use it.</li> <li>As a quick-start convenience, support has been included to provision a PostgreSQL cluster locally in   the Kubernetes cluster using the CrunchyData PostreSQL Operator. When enabling this option, the   database(s) will be automatically created and Smile CDR will be configured to connect to it.</li> </ul> <p>If you do not specify one or the other, the chart will fail to render any output and will return a descriptive error instead</p> <p>WARNING - Do not use built-in H2 database: Due to the ephemeral and stateless nature of Kubernetes Pods, there is no use case where it makes sense to provision Smile CDR using the internal H2 database. You are free to configure your persistence module to do so, but every time the Pod restarts, it will start with an empty database and will perform a fresh install of Smile CDR. In addition to this, if you were to configure multiple replicas, each Pod would appear as its own distinct Smile CDR install. As such, you should not configure Smile CDR in this fashion and you must instead provision some external database.</p>"},{"location":"guide/smilecdr/database/#referencing-externally-provisioned-databases","title":"Referencing Externally Provisioned Databases","text":"<p>To reference a database that is external to the cluster, you will need:</p> <ul> <li>Network connectivity from the K8s cluster to your database.</li> <li>A secret containing the connection credentials in a structured Json format.</li> <li>It is common practice to include all connection credentials in DB secrets, this way it becomes simple   to manage the database without having to reconfigure Smile CDR. e.g. when 'restoring' an RDS instance, the   DB cluster name will typically change. By keeping these details inside the secret then any such change will be automatically applied without reconfiguring. See   here   for info on the schema used by AWS for this purpose. Note that an app restart will be required to pick up the new secret value.</li> <li>The secret can be a plain Kubernetes secret that you provision externally, or it can be a secret in a   secure secrets vault. The latter is the preferred option for increased security and the ability to easily   rotate credentials. At this time, only AWS Secrets Manager is supported via the Secrets Store CSI Driver.   See the Secrets Handling section for more info on this.</li> </ul> <p>If using AWS Secrets Manager, set the <code>credentials.type</code> to <code>sscsi</code> and <code>credentials.provider</code> to <code>aws</code>. If you have created a <code>Secret</code> object in Kubernetes, set it to <code>externalsecret</code>.</p>"},{"location":"guide/smilecdr/database/#example-secret-configurations","title":"Example Secret Configurations","text":""},{"location":"guide/smilecdr/database/#using-aws-secret-json-structure","title":"Using AWS Secret Json structure","text":"<p>If you are using the above mentioned Json structure (i.e. <code>engine</code>, <code>host</code>, <code>username</code>, <code>password</code>, <code>dbname</code> and <code>port</code>) in your secret, then you should simply configure your secret as per the following yaml fragment. Those default keys will be used to extract the credentials.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\nexternal:\nenabled: true\ncredentials:\ntype: sscsi\nprovider: aws\ndatabases:\n- secretName: clustermgrSecret\nsecretARN: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\nmodule: clustermgr\n</code></pre> <p>Note: <code>clustermgrSecret</code> can be any friendly name, it's not important. The Kubernetes <code>Secret</code> resource will be named using this value.</p>"},{"location":"guide/smilecdr/database/#using-custom-secret-json-structure","title":"Using Custom Secret Json structure","text":"<p>If the Json keys in your secret are different than above, they can be overridden by specifying them with the <code>*Key</code> attributes to override the defaults.</p> <p>The below are just examples, to show how the Json keys can be overridden. You need to ensure that this matches the configuration of your secret and the keys it contains.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_1","title":"<code>my-values.yaml</code>","text":"<pre><code>database:\nexternal:\nenabled: true\ncredentials:\ntype: sscsi\nprovider: aws\ndatabases:\n- secretName: clustermgrSecret\nsecretARN: arn:aws:secretsmanager:us-east-1:012345678901:secret:clustermgrSecret\nmodule: clustermgr\nurlKey: url-key-name\nportKey: port-key-name\ndbnameKey: dbname-key-name\nuserKey: user-key-name\npassKey: password-key-name\n</code></pre> <p>If a required field is not included in the secret, you can specify it in your values file like so.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_2","title":"<code>my-values.yaml</code>","text":"<pre><code>- secretName: smilecdr\nmodule: clustermgr\nurl: db-url # this is the actual url/hostname\nport: 5432\ndbname: dbname\nuser: username\npassKey: password\n</code></pre> <p>NOTE: You cannot override the passKey value. The password will always come from the referenced secret.</p>"},{"location":"guide/smilecdr/database/#using-crunchydata-pgo-databases","title":"Using CrunchyData PGO Databases","text":"<p>In order to use this feature, you will need to ensure that your K8s cluster has the CrunchyData PGO already installed (Instructions here). Simply enable this feature using the following yaml fragment for your database configuration:</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml_3","title":"<code>my-values.yaml</code>","text":"<p><pre><code>database:\ncrunchypgo:\nenabled: true\ninternal: true\n</code></pre> This will create a 2 instance HA PostgreSQL cluster, each with 1cpu, 2GiB memory and 10GiB storage. These defaults can be configured using <code>database.crunchypgo.config</code> keys. Backups are enabled by default as it's a feature of the Operator.</p>"},{"location":"guide/smilecdr/database/#configuring-multiple-databases","title":"Configuring Multiple Databases","text":"<p>This chart has support to use multiple databases. It is recommended to configure Smile CDR this way, with a separate DB for the Cluster Manager, Audit logs and for any Persistence Modules.</p> <p>The <code>module</code> key is important here as it tells the Helm Chart which module uses this database. If there is only one database configured then it will be used for all modules.</p> <p>If you provide multiple databases, the <code>module</code> key specified in each one is used to determine which Smile CDR module it is used by.</p> <p>With multiple databases, the above examples may look like this:</p> <p>Note The CrunchyData PGO is a little different in that it uses the concept of 'users' in the configuration to configure multiple databases. That is why we are specifying multiple users below in the CrunchyData PGO example.</p>"},{"location":"guide/smilecdr/database/#my-valuesyaml-external-database","title":"<code>my-values.yaml</code> (External Database)","text":"<pre><code>database:\nexternal:\nenabled: true\ncredentials:\ntype: sscsi\nprovider: aws\ndatabases:\n- secretName: smilecdr\nmodule: clustermgr\n- secretName: smilecdr-audit\nmodule: audit\n- secretName: smilecdr-pers\nmodule: persistence\n</code></pre>"},{"location":"guide/smilecdr/database/#my-valuesyaml-crunchydata-pgo","title":"<code>my-values.yaml</code> (CrunchyData PGO)","text":"<p><pre><code>database:\ncrunchypgo:\nenabled: true\ninternal: true\nusers:\n- name: smilecdr\nmodule: clustermgr\n- name: smilecdr-audit\nmodule: audit\n- name: persistence\nmodule: persistence\n</code></pre> In both of the above examples, the <code>clustermgr</code>, <code>audit</code> and <code>persistence</code> modules will automatically have their own set of environment variables for DB connections as follows: <code>CLUSTERMGR_DB_*</code>, <code>AUDIT_DB_*</code> and <code>PERSISTENCE_DB_*</code></p> <p>NOTE: You do NOT need to update these environment variable references in your module configurations. When the <code>clustermgr</code> module definition references <code>DB_URL</code>, this will be automatically mutated to <code>CLUSTERMGR_DB_URL</code>. This will happen automatically for any module that references <code>DB_*</code> environment variables.</p>"},{"location":"guide/smilecdr/files/","title":"Including Extra Files","text":"<p>It is often required to include extra files into your Smile CDR instance. This could be to provide updated configuration changes (e.g. a modified <code>logback.xml</code>), provide <code>.js</code> scripts, <code>.jar</code> files and other libraries to extend the functionality of Smile CDR.</p> <p>Rather than having to build a custom Smile CDR container image to include these files, it is possible to include them using this Helm Chart.</p>"},{"location":"guide/smilecdr/files/#available-methods","title":"Available Methods","text":"<p>There are two mechanisms available to load files.</p> <ul> <li>Including files in the Helm deployment</li> <li>Pulling files from an external location</li> </ul> <p>Each of these mechanisms has its own advantages.</p>"},{"location":"guide/smilecdr/files/#choosing-which-method-to-use","title":"Choosing Which Method To Use","text":""},{"location":"guide/smilecdr/files/#helm-chart-method","title":"Helm Chart Method","text":"<p>Using the Helm Chart method is ideal when:</p> <ul> <li>The files are text based and under 1MiB in size<ul> <li>Config files and small scripts are good examples</li> <li>Not ideal for binary files, even if small</li> </ul> </li> <li>You do not have many files to add<ul> <li>Although there is no limit, your configuration will get very hard to manage if you use too many</li> <li>Between 5 &amp; 10 would be a good limit, but this is just a suggestion</li> </ul> </li> <li>You don't have a mechanism in place to stage the files somewhere (i.e. Amazon S3)<ul> <li>This method provides a simple deployment solution as it has no external dependencies</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/files/#external-pull-method","title":"External Pull Method","text":"<p>Using the External Pull method is ideal when:</p> <ul> <li>You have binary files or large files<ul> <li>Any file over 1MiB requires you use this method</li> </ul> </li> <li>You have many files<ul> <li>This mechanism will copy files recursively without clogging up your configuration</li> </ul> </li> <li>You are able to stage your files and file updates on Amazon S3<ul> <li>Currently only S3 is supported, but other external file sources will be added as required</li> </ul> </li> <li>You wish to pull files that are publically hosted (e.g. public <code>.jar</code> files)</li> </ul>"},{"location":"guide/smilecdr/files/#using-both-methods","title":"Using Both Methods:","text":"<p>Using both methods is possible too:</p> <ul> <li>If you had a set of <code>.jar</code> files and scripts being staged on S3, you could still add files using the Helm chart method if it makes for a simpler workflow</li> <li>Be wary that having it split up like this could make your configuration more confusing (i.e. \"Where was that file copied from again?\")</li> <li>Files copied using the Helm Chart method will take precedence over any files copied from an external source.</li> </ul>"},{"location":"guide/smilecdr/files/#using-the-helm-chart-method","title":"Using the Helm Chart Method","text":"<p>To pass in files using the Helm Chart, there are two things you need to do: 1. Use a Helm commandline option to load the file into the deployment 2. Reference and configure the file in your values file.</p>"},{"location":"guide/smilecdr/files/#include-file-in-helm-deployment","title":"Include File in Helm Deployment","text":"<p>To include a file in the deployment, use the following commandline option: <pre><code>helm upgrade -i my-smile-env --devel -f my-values.yaml --set-file mappedFiles.logback\\\\.xml.data=logback.xml smiledh/smilecdr\n</code></pre></p> <p>WARNING: Pay special attention to the escaping required to include the period in the filename. You need to use <code>\\\\.</code> when running this from a shell. This is just the way this works.</p> <p>This will encode the file and load it into the provided values under the <code>mappedFiles.logback.xml.data</code> key.</p>"},{"location":"guide/smilecdr/files/#include-file-in-values-file","title":"Include File in Values File","text":"<p>The included file also needs to be referenced from your values file so that the chart knows where to mount the file in the application's Pod: <pre><code>mappedFiles:\nlogback.xml:\npath: /home/smile/smilecdr/classes\n</code></pre> As the result of the above, a <code>ConfigMap</code> will be created and mapped into the pod at <code>/home/smile/smilecdr/classes/logback.xml</code> using <code>Volume</code> and <code>VolumeMount</code> resources. If the content of the file is changed, then it will be automatically picked up on the next deployment. (See Automatic Deployment of Config Changes for more info on this)</p>"},{"location":"guide/smilecdr/files/#using-the-external-pull-method","title":"Using the External Pull Method","text":"<p>The external pull method can be used to pull files from Amazon S3 or from public websites that publish resources (e.g. Maven).</p>"},{"location":"guide/smilecdr/files/#how-it-works","title":"How It Works","text":""},{"location":"guide/smilecdr/files/#shared-volumes","title":"Shared Volumes","text":"<p>Pod-local shared volumes are used for the <code>classes</code> and <code>customerlib</code> directories so that the files can be copied there before the main Smile CDR container starts up.</p> <p>These volumes are only accessible to containers running inside the pods and are deleted when the pod is terminated so they are not accessible outside the pod's lifecycle. If the underlying Kubernetes node volume uses encrypted storage, then these volumes will also be encrypted.</p>"},{"location":"guide/smilecdr/files/#init-containers","title":"Init Containers","text":"<p>Kubernetes init containers are then used to pull files from S3, or some other location.</p> <p>It uses multiple Kubernetes init containers to synchronize and pull files to these shared volumes during pod startup.</p> <p>This feature has been implemented to support Amazon S3 and curl. Other mechanisms may be introduced in a future version of this chart.</p> <p>The init containers are auto-configured based on the provided <code>copyFiles</code> settings. They function as follows:</p> <p><code>init-sync-classes</code></p> <ul> <li>This container copies the default files from the classes directory from the Smile CDR base image to a <code>classes</code> shared volume that is local to the pod</li> <li>The <code>init-pull-classes</code> container will overwrite any of these files with the same names</li> <li>This is a required step if you wish to retain the default files. As such, it's enabled by default</li> <li>If you require a 'clean' <code>classes</code> directory, this step can be disabled using <code>copyFiles.classes.disableSyncDefaults: true</code>.<ul> <li>If disabled, you will need to provide all <code>classes</code> files that are required for Smile CDR to start up (With the exception of the config properties file which is generated by this Helm Chart).</li> </ul> </li> </ul> <p>Note: There is no <code>init-sync</code> container for the <code>customerlib</code> as there are no pre-existing files in that directory in the standard Smile CDR image. See the <code>init-pull-customerlib</code> section below for more info on this.</p> <p><code>init-pull-classes-*</code></p> <ul> <li>These containers copy files from the specified location to the classes shared volume</li> <li>Currently they support pulling files from Amazon S3 and downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> </ul> <p><code>init-pull-customerlib-*</code></p> <ul> <li>This container copies files from the specified location to the customerlib shared volume</li> <li>Currently they support pulling files from Amazon S3 and downloading files from public websites using <code>curl</code>.</li> <li>Any files copied will be available to Smile CDR when it starts up</li> <li>If you are using a custom Smile CDR image with files already copied into the <code>customerlib</code> directory, they will not be available to the Smile CDR if you have enabled this container<ul> <li>You should define your <code>customerlib</code> files in one place</li> <li>It makes the most sense to include them all via this mechanism so you can avoid the extra complexity of creating and hosting custom images</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/files/#s3-prerequisites","title":"S3 Prerequisites","text":"<p>To pass in files from an Amazon S3 bucket, you need the following prerequisites in place:</p> <ul> <li>An S3 bucket with:<ul> <li>A folder containing your <code>classes</code> files</li> <li>A folder containing your <code>customerlib</code> files</li> <li>Ideally, these should be in a higher level folder to control versioning<ul> <li>e.g. <code>v1</code>, <code>v2</code> or a <code>UID</code></li> </ul> </li> <li>Bucket should not be publically accessible<ul> <li>It will work with public buckets too, but this is a bad security practice</li> </ul> </li> <li>Bucket should use encryption<ul> <li>Again, it will work without, but it's good security practice to encrypt everything by default</li> </ul> </li> <li>The mechanism to copy the files into this bucket is out of the scope of this Helm Chart</li> </ul> </li> <li>Service Account must be enabled and configured to use IRSA. See here for more info on this</li> <li>The IAM Role used for the Service Account must have read access to the S3 bucket</li> </ul>"},{"location":"guide/smilecdr/files/#a-note-on-file-versioning","title":"A Note On File Versioning","text":"<p>Though not required, it is reccommended to include some versioning structure in your S3 bucket.</p> <p>While already running pods cannot be affected by this (As they have already copied their files), any new pods that start up (e.g in scaling or reconciliation events) may be adversely affected if files have been unexpectedly changed or deleted.</p> <p>By including a new version whenever a given set of files is updated, previous deployments of the application will remain unaffected. This is also beneficial during rollbacks as the previous set of files will remain.</p> <p>This does introduce challenges of file duplication and managing multiple old versions. As the number of files included is typically low, this should not be of huge concern.</p>"},{"location":"guide/smilecdr/files/#configure-helm-values-file","title":"Configure Helm Values File","text":"<p>To enable this feature, add the following snipped to your values file. Replace the bucket name and path to match your environment.</p> <pre><code>copyFiles:\nclasses:\nsources:\n# Copies files recursively from S3 to the classes directory\n- type: s3\n# disableSyncDefaults: true &lt;- Optional. Use with caution! (See above)\nbucket: s3-bucket-name\npath: /path-to/classes\n# Example versioned locations.\n# path: /v1/classes\n# path: /v1.1/classes\n# path: /v2/classes\n# path: /&lt;sha256-of-file-content&gt;/classes &lt;- You could generate a sha256 hash of the entire file contents.\n# path: /&lt;UID&gt;/classes &lt;- You could generate a unique UID for each new version\ncustomerlib:\nsources:\n# Copies files recursively from S3 to the customerlib directory\n- type: s3\nbucket: s3-bucket-name\npath: /path-to/customerlib-src\n# Downloads a single file using curl to the customerlib directory (In this case, customerlib/elastic-apm/elastic-apm-agent-1.13.0.jar)\n- type: curl\nfileName: elastic-apm/elastic-apm-agent-1.13.0.jar\nurl: https://repo.maven.apache.org/maven2/co/elastic/apm/elastic-apm-agent/1.13.0/elastic-apm-agent-1.13.0.jar\n</code></pre> <p>Note: The Service Account configurations have been left out for clarity. Please refer to the Service Account guide for instructions on enabling IRSA and IAM roles.</p>"},{"location":"guide/smilecdr/ingress/","title":"Configuring Ingress","text":"<p>This chart supports multiple Ingress options, currently including Nginx Ingress, AWS Load Balancer Controller and Azure Application Gateway Controller.</p>"},{"location":"guide/smilecdr/ingress/#ingress-type","title":"Ingress Type","text":"<p>Select the ingress type by setting <code>ingress.type</code> to the appropriate value. Doing this will automatically configure the <code>Service</code> and <code>Ingress</code> resources so that the chosen controller can configure infrastructure resources appropriately.</p> <p>There are three ingress types currently supported:</p> <ul> <li>Nginx Ingress Controller (Default)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Controller</li> </ul>"},{"location":"guide/smilecdr/ingress/#nginx-ingress","title":"Nginx Ingress","text":"<p>By default, this chart is configured to use the Nginx Ingress Controller. <code>ingress.type</code> is already set to <code>nginx-ingress</code> so you do not need to do anything to use this method.</p> <p>When used in conjunction with the AWS Load Balancer Controller, the Nginx Ingress will be backed by an AWS NLB (Network Load Balancer).</p> <p>The <code>Service</code> objects will be set as <code>ClusterIP</code> rather than <code>NodePort</code>. This increases the security stance of the deployment as Kubernetes does not expose these services externally to the cluster. All traffic comes from the Nginx Ingress pods directly to the application pods.</p>"},{"location":"guide/smilecdr/ingress/#dedicated-nginx-ingress","title":"Dedicated Nginx Ingress","text":"<p>By default, this option uses the <code>nginx</code> ingress class. Any ingresses using this class will share the same underlying NLB.</p> <p>If you need to use a dedicated NLB for this deployment you can do so by first creating a separate Nginx Ingress Controller with a different ingress class name. You can then specify this ingress class with <code>ingress.ingressClassNameOverride</code>.</p>"},{"location":"guide/smilecdr/ingress/#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>To directly use the AWS Load Balancer Controller set <code>ingress.type</code> to <code>aws-lbc-alb</code>. By default, this option uses the <code>alb</code> ingress class.</p> <p>This automatically adds appropriate default <code>Ingress</code> annotations for the AWS Load Balancer Controller. The controller will then create an AWS ALB (Application Load Balancer).</p> <p>You will still need to add some extra annotations, such as <code>alb.ingress.kubernetes.io/certificate-arn</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#known-problems","title":"Known Problems","text":"<p>There is currently a problem with the AWS Load Balancer Controller configuration where the health checks do not function correctly. This is somewhat mitigated by the fact that the <code>Service</code> objects are using <code>NodePort</code>. This will be addressed in a future release of this chart.</p>"},{"location":"guide/smilecdr/ingress/#azure-application-gateway-controller","title":"Azure Application Gateway Controller","text":"<p>If you wish to use the Azure Application Gateway Controller, set <code>ingress.type</code> to <code>azure-appgw</code>. By default, this option uses the <code>azure/application-gateway</code> ingress class.</p> <p>When using this method, the chart will automatically add <code>Ingress</code> annotations for the Azure Application Gateway Controller. The controller will then create an Azure Application Gateway to be used as ingress.</p> <p>You will still need to add some extra annotations, such as <code>appgw.ingress.kubernetes.io/appgw-ssl-certificate</code>. See the Extra Annotations section below for more info.</p> <p>Warning: Be aware that the <code>Service</code> objects will be set as <code>NodePort</code> rather than  <code>ClusterIP</code>. This means that the application services will be made available externally to the cluster which may have security implications you need to be aware of.</p>"},{"location":"guide/smilecdr/ingress/#extra-annotations","title":"Extra Annotations","text":"<p>Depending on the ingress type you select, the chart will automatically add a set of default annotations that are appropriate for the ingress type being used.</p> <p>However, it is not possible for the chart to automatically include all annotations as some need to be specified in your configuration.</p> <p>To add any extra annotations, or override existing ones, include them in your values file like so:</p> <pre><code>ingress:\nannotations:\nalb.ingress.kubernetes.io/certificate-arn: arn:aws:acm::01234567890:certificate/abcdef\nalb.ingress.kubernetes.io/inbound-cidrs: 0.0.0.0\n</code></pre> <p>or <pre><code>ingress:\nannotations:\nappgw.ingress.kubernetes.io/appgw-ssl-certificate: mysslcert\n</code></pre></p>"},{"location":"guide/smilecdr/ingress/#ingress-class-name","title":"Ingress Class Name","text":"<p>This chart assumes the following class names for your ingress controllers</p> Selected <code>ingress.type</code> Default <code>ingress.class</code> <code>nginx-ingress</code> <code>nginx</code> <code>aws-lbc-alb</code> <code>alb</code> <code>azure-appgw</code> <code>azure/application-gateway</code> <p>If you have configured your ingress with a different <code>IngressClass</code> name, you can override it using <code>ingress.ingressClassNameOverride</code>.</p> <p>For example, if you had a dedicated Nginx Ingress Controller with the <code>IngressClass</code> of <code>nginx-dedicated</code>, you would include it in your values file like so:</p> <pre><code>ingress:\ningressClassNameOverride: nginx-dedicated\n</code></pre>"},{"location":"guide/smilecdr/ingress/#service-type","title":"Service Type","text":"<p>The appropriate type for the <code>Service</code> resources depend on which Ingress type is being used. The default <code>Service</code> created by this chart is <code>ClusterIP</code>. This is the preferred option as it does not expose the Services externally to the cluster.</p> <p>When using the AWS Load Balancer Controller, or Azure Application Gateway Controller, the service objects are instead set to <code>NodePort</code>.</p> <p>This can be overriden using <code>service.type</code> in your values file, but it is not recommended and may cause unpredictable behaviour.</p>"},{"location":"guide/smilecdr/install/","title":"Installing Smile CDR","text":""},{"location":"guide/smilecdr/install/#prepare-values-file","title":"Prepare Values File","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p> <p>Refer to the section on Values Files Management for more info on how to organise your values files. You can start out with one of the values files in the Examples section, or create your own from scratch using techniques from the configuration section.</p> <p>For the remainder of this section, we will assume the same values file that was used in the QuickStart guide.</p>"},{"location":"guide/smilecdr/install/#install-the-helm-chart","title":"Install the Helm Chart","text":"<p>With your custom values file(s) you install as follows: <pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre></p> <p>Smile, we're up and running! :)</p> <p>If your cluster has spare capacity available, all pods should be in the <code>Running</code> state after about 2-3 minutes. If your cluster needs to auto-scale to provision enough resources, it may take longer while the K8s worker nodes get created. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL.</p> <p>You can now continue to reconfigure it using this guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"guide/smilecdr/messagebroker/","title":"Message Broker Configuration","text":"<p>Much like with the database configuration, you can use an externally provisioned message broker, or you can have this chart provision Kafka for you if you have the Strimzi Operator installed in your cluster.</p>"},{"location":"guide/smilecdr/messagebroker/#configuring-external-message-broker","title":"Configuring external message broker.","text":"<p>You can use the <code>messageBroker.external</code> section to configure an external message broker like so: <pre><code>messageBroker:\nexternal:\nenabled: true\ntype: kafka\nbootstrapAddress: kafka-example.local\ntls: true\n</code></pre> You can also do some of the message broker tuning in the <code>clustermgr</code> module. The configurations provided in the <code>messageBroker</code> section above will override any in the module definition.</p>"},{"location":"guide/smilecdr/messagebroker/#provisioning-kafka-with-strimzi","title":"Provisioning Kafka with Strimzi","text":"<p>If you have the Strimzi Operator installed in your cluster, you can use the following values file section to automate provisioning of a Kafka cluster. Your Smile CDR instance will then be automatically configured to use this HA Kafka cluster. <pre><code>messageBroker:\nstrimzi:\nenabled: true\n</code></pre></p>"},{"location":"guide/smilecdr/messagebroker/#configuring-kafka-via-strimzi","title":"Configuring Kafka via Strimzi","text":"<p>With the configuration provided above, you will have a production-like Kafka cluster with the followinf configuration:</p> <ul> <li>3 ZooKeeper nodes with the following specifications<ul> <li>0.5cpu</li> <li>512MiB memory</li> <li>10GiB storage</li> </ul> </li> <li>3 Kafka Broker nodes with the following specifications<ul> <li>0.5cpu</li> <li>1GiB memory</li> <li>10GiB storage</li> </ul> </li> </ul> <p>All of the Kafka configurations can be configured using <code>messageBroker.strimzi.config</code> like so:</p> <pre><code>messageBroker:\nstrimzi:\nenabled: true\nconfig:\nversion: \"3.3.1\"\nprotocolVersion: \"3.3\"\ntls: true\nkafka:\nreplicas: 4\nvolumeSize: 20Gi\nresources:\nrequests:\ncpu: 0.5\nmemory: 4Gi\nlimits:\nmemory: 4Gi\nzookeeper:\nreplicas: 2\nvolumeSize: 10Gi\nresources:\nrequests:\ncpu: 0.5\nmemory: 512Mi\nlimits:\nmemory: 512Mi\n</code></pre> <p>For more details on how to configure Kafka using Strimzi, please consult the Strimzi Operator documentation here</p>"},{"location":"guide/smilecdr/modules/","title":"Module Configuration","text":"<p>Configuring modules is fairly straight forward, but somewhat different than the existing methods using the <code>cdr-config-Master.properties</code> file. This file is still used behind the scenes, but it is generated by the Helm Chart and included in the application deployment.</p> <p>NOTE: When using Helm Charts, they become the 'single source of truth' for your configuration. This means that repeatable, consistent deployments become a breeze. It also means you should not edit your configuration options in the Smile CDR web admin console.</p> <p>You can define your modules in your main values file, or you can define them in separate files and include them using the <code>-f</code> command. This is possible because Helm accepts multiple values files</p> <p>We recommend defining them in one or more separate files, as this allows you to manage common settings as well as per-environment overlays. We will discuss this further down in the Advanced Configuration section below.</p> <p>Mapping existing configurations to values files is relatively straight forwards:</p>"},{"location":"guide/smilecdr/modules/#identify-the-module-configuration-parameter","title":"Identify the module configuration parameter.","text":"<p>e.g. Concurrent Bundle Validation Config.properties format: <code>module.persistence.config.dao_config.concurrent_bundle_validation = false</code></p>"},{"location":"guide/smilecdr/modules/#specify-them-in-th-values-yaml-file-format","title":"Specify them in th values yaml file format:","text":"<p><pre><code>modules:\npersistence:\nconfig:\ndao_config.concurrent_bundle_validation: \"false\"\n</code></pre> The same effective mapping can be used for any module configurations supported by Smile CDR.</p>"},{"location":"guide/smilecdr/modules/#module-definition-considerations","title":"Module definition considerations","text":"<p>Here are some additional fields/considerations that need to be included in your module definitions files:</p> <ul> <li>Though not strictly required by the <code>yaml</code> spec, all values should be quoted.   You may run into trouble with some values if you do not quote them.   Specifically, values starting with <code>*</code> or <code>#</code> will fail if not quoted.</li> <li>The <code>module id</code> is taken from the yaml key name.</li> <li>Modules can be defined, but disabled. They need to be enabled with the <code>enabled: true</code> entry.</li> <li>Modules other than the cluster manager need to define <code>type</code>. A list of module types is available here</li> <li>Modules which expose an endpoint need to de defined with a <code>service</code> entry, which includes <code>enabled</code> and <code>svcName</code> entries.</li> <li>DB credentials/details can be referenced from your module configurations via <code>DB_XXX</code> environment variables.</li> </ul> <p>Any configurations you specify will merge with the defaults, priority going to the values file.</p>"},{"location":"guide/smilecdr/modules/#disabling-included-default-module-definitios","title":"Disabling included default module definitios","text":"<p>If you wish to disable any of the default modules, we recommend you disable all default modules and define your own from scratch. This way it will be easier to determine the exact modules you have defined just by looking at your values files. You can disable all default modules using: <pre><code>modules:\nuseDefaultModules: false\n</code></pre> You use the <code>default-modules.yaml</code> file as a reference by untarring the Helm Chart.</p> <p>Here is an example of what your module definition may look like when configuring Smile CDR with the <code>clustermgr</code>, <code>persistence</code>, <code>local_security</code>, <code>fhir_endpoint</code> and <code>admin_web</code> modules.</p>"},{"location":"guide/smilecdr/modules/#my-module-valuesyaml","title":"<code>my-module-values.yaml</code>","text":"Click to expand <pre><code>modules:\nuseDefaultModules: false\nclustermgr:\nname: Cluster Manager Configuration\nenabled: true\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\npersistence:\nname: Database Configuration\nenabled: true\ntype: PERSISTENCE_R4\nconfig:\ndb.driver: POSTGRES_9_4\ndb.url: jdbc:postgresql://#{env['DB_URL']}:#{env['DB_PORT']}/#{env['DB_DATABASE']}?sslmode=require\ndb.password: \"#{env['DB_PASS']}\"\ndb.username: \"#{env['DB_USER']}\"\nlocal_security:\nname: Local Storage Inbound Security\nenabled: true\ntype: SECURITY_IN_LOCAL\nconfig:\nseed.users.file: classpath:/config_seeding/users.json\npassword_encoding_type: BCRYPT_12_ROUND\nadmin_web:\nname: Web Admin\nenabled: true\ntype: ADMIN_WEB\nservice:\nenabled: true\nsvcName: admin-web\nhostName: default\nrequires:\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: \"\"\nport: 9100\ntls.enabled: false\nhttps_forwarding_assumed: true\nrespect_forward_headers: true\nfhir_endpoint:\nname: FHIR Service\nenabled: true\ntype: ENDPOINT_FHIR_REST_R4\nservice:\nenabled: true\nsvcName: fhir\nhostName: default\nrequires:\nPERSISTENCE_R4: persistence\nSECURITY_IN_UP: local_security\nconfig:\ncontext_path: fhir_request\nport: 8000\nbase_url.fixed: default\n</code></pre>"},{"location":"guide/smilecdr/modules/#install-smile-cdr-with-extra-modules-definitions","title":"Install Smile CDR with extra modules definitions","text":"<pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml -f my-module-values.yaml smiledh/smilecdr\n</code></pre>"},{"location":"guide/smilecdr/requirements/","title":"Requirements and Dependencies","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Helm Chart. Due to the complicated nature of configuring the product, and enforcing strong security practices, there is no quickstart option without ensuring some, or all, of these pre-requisites have been met.</p>"},{"location":"guide/smilecdr/requirements/#minimum-requirements","title":"Minimum Requirements","text":"<p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes add-ons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>One of the following supported database options:<ul> <li>Externally provisioned database in the official Smile CDR supported databases list here</li> <li>CrunchyData Postgres Operator installed in cluster. See Extra Requirements below if you follow this option.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#recommended-requirements","title":"Recommended Requirements","text":"<p>These dependencies are recommended in order to follow security best practices. These are in addition to those listed above.</p> <ul> <li>Kubernetes/EKS/AKS cluster should be configured with best practices in mind.<ul> <li>Kubernetes best practices</li> <li>Amazon EKS best practices</li> <li>Azure AKS best practices</li> </ul> </li> <li>Kubernetes cluster should, at the very least, have the following configurations<ul> <li>Secret Encryption (EKS Secret Encryption)</li> <li>Storage Class with encryption enabled if using persistent storage (PostgreSQL or Kafka)</li> <li>Enforce all pods should set resource requests</li> </ul> </li> <li>AWS IAM Role for the Smile CDR application.<ul> <li>Should follow the principle of least privilege and only have access to required AWS services</li> </ul> </li> <li>Secrets Store CSI Driver + Provider<ul> <li>Only the AWS Provider is supported at this time</li> <li>AWS IAM Role needs access to read &amp; decrypt the secrets in AWS Secrets Manager</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/requirements/#extra-requirements","title":"Extra Requirements","text":"<ul> <li>Strimzi Kafka Operator<ul> <li>Allows you to install a production ready Kafka cluster as a part of the Smile CDR deployment.</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment.</li> </ul> </li> </ul>"},{"location":"guide/smilecdr/resources/","title":"Configuring Compute Resources","text":""},{"location":"guide/smilecdr/resources/#kubernetes-memory-requests-vs-limits","title":"Kubernetes Memory Requests vs Limits","text":"<p><code>requests.memory</code> will be set to the same value as <code>limits.memory</code> unless you override it.</p> <p>The values you should use for CPU resources will depend on the number of cores you are licenced for with Smile CDR. Your total cores can be calculated by <code>replicas * requests.limits.cpu</code>, or <code>autoscaling.maxReplicas * requests.limits.cpu</code> if you are using Horizontal Pod Autoscaling.</p>"},{"location":"guide/smilecdr/resources/#jvm-sizing","title":"JVM Sizing","text":"<p>As Smile CDR is a high performance Java based application, special consideration needs to be given to the resource settings and JVM tuning parameters.</p> <p>Typical cloud best practices suggest starting small and increasing resources as workload increases. We have tested Smile CDR in its default module configuration and determined that the max heap size should be no smaller than 2GB. When smaller than this, there are excessive GC events which is not ideal.</p> <p>NOTE: If you reconfigure Smile CDR to have more modules, it may require more memory/cpu. If you split up the cluster into multiple nodes, then each node may be able to run with less memory/cpu, though total cluster may end up higher depending on your architecture.</p>"},{"location":"guide/smilecdr/resources/#jvm-heap-auto-sizing","title":"JVM Heap Auto Sizing","text":"<p>When running Java applications in Kubernetes, the <code>requests.memory</code> should be set much higher than the max heap size. Typically the Java heap size should be set to 50-75% of the total available memory.</p> <p>This Helm Chart will take the specified <code>limits.memory</code> and use <code>jvm.memoryFactor</code> to calculate the value for the Java heap size. By default, this value is <code>0.5</code>. With the default <code>limits.memory</code> of 4Gib, the chart sets Java <code>-Xmx</code> to <code>2048m</code>.</p> <p>Setting this number higher will make more efficient use of memory resources in your K8s cluster, but may increase the likelihood of <code>OOM Killed</code> errors.</p>"},{"location":"guide/smilecdr/resources/#setting-heap-minimum-size","title":"Setting Heap Minimum Size","text":"<p>If you were to set <code>jvm.memoryFactor</code> to <code>1</code> your pod is almost guaranteed to be killed with such an error, but it will happen at an unpredictable time, once the currently allocated heap grows to a certain point. This can increase difficulty of troubleshooting due to the unpredictable timing. It may fail in a few minutes, or a few hours/days/weeks/never depending on the workload characteristics.</p> <p>To reduce the likelihood of such unpredictable <code>OOM Killed</code> errors, we recommend setting <code>-Xms</code> to be the same as <code>-Xmx</code>. This can be done by setting <code>jvm.xms</code> to <code>true</code></p> <p>NOTE: You can pass in extra JVM commandline options by adding them to the list <code>jvm.args</code></p>"},{"location":"guide/smilecdr/updating/","title":"Deploying changes","text":"<p>When you make changes to the Helm Chart configuration, you need to apply them using the <code>helm upgrade</code> command. This chart has been designed in such a way that there should not be any outages during updates.</p> <p>There are multiple components that a configuration can affect. Broadly, it can affect the configuration of the Smile CDR app itself, or it can affect the surrounding infrastructure.</p> <p>In the event that a configuration change affects the Smile CDR application, then this chart will update any configuration files and create new application pods with zero-outage.</p>"},{"location":"guide/smilecdr/updating/#rolling-deployments","title":"Rolling Deployments","text":"<p>We achieve zero-outage by making use of Rolling Deployments in Kubernetes. The rolling deployment has been configured to create one new Pod with the new configuration at a time. Once each new Pod has successfully started up and is able to accept traffic, Kubernetes will start routing requests to it and then terminate one of the pods with the older configuration.</p> <p>The result of this is that the changes will be rolled out over the entire cluster in a controlled fashion over a few minutes, without any downtime or outage.</p> <p>This is a conservative rolling deployment model, but it means that if pods with the new configuration fail to come up without error, then the existing deployment will remain unaffected.</p>"},{"location":"guide/smilecdr/updating/#making-a-config-change-with-rolling-deployments","title":"Making a config change with Rolling Deployments","text":"<p>There is nothing you need to do to make use of this rolling deployment mechanism. If your chart configuration changes include something that will update the Smile CDR configuration, and if you have a sufficient number of replicas, then this will happen automatically.</p> <p>All changes other than those listed here will cause a rolling deployment of the application</p> <ul> <li><code>replicaCount</code> or <code>autoScaling</code> changes</li> <li><code>ingress</code> configuration - i.e. switching to a different ingress provider.</li> <li>CrunchyPGO database infrastructure configuration<ul> <li>Updating <code>users</code> config WILL cause a rolling deployment</li> </ul> </li> <li>Strimzi Kafka infrastructure resource configuration<ul> <li>Updating protocol/connection config will cause a rolling deployment</li> </ul> </li> </ul> <p>The method used to apply your updates will depend on how you have deployed the Helm Chart. If you have used a code reconciliation system or some other automation, you should not need to do anything.</p> <p>If using native Helm commands, you would use the same command you used to install the chart, like so:</p> <p><code>helm upgrade -i my-smile-env -f my-values.yaml smiledh/smilecdr</code></p>"},{"location":"guide/smilecdr/updating/#automatic-deployment-of-config-changes","title":"Automatic Deployment of Config Changes","text":"<p>Normally, changes that do not directly affect the Pod definition of a Deployment in Kubernetes will not trigger a deployment. Typically, this means that manual recycling of Pods may be required to force updates.</p> <p>To ensure that all changes are automatically deployed, the Smile CDR Helm Chart uses a unique <code>sha256</code> hash to identify any <code>ConfigMap</code> objects. This means that any configuration changes will be detected and automatically deployed without interruption using the Rolling Deployment strategy.</p> <p>NOTE: An extra benefit of this technique is that if a new configuration has an error and the pods fail to come up, then the existing Pods will still use their original configuration, even if they need to be restarted.</p> <p>This feature can be disabled if required by setting <code>autoDeploy</code> to <code>false</code></p>"},{"location":"guide/smilecdr/updating/#argocd-considerations","title":"ArgoCD Considerations","text":"<p>If you ArgoCD to deploy your charts, then this mechanism would cause previous versions of the <code>ConfigMap</code> to be deleted after you perform configuration changes. This interferes with the ability for the existing <code>ReplicaSet</code> to scale or self-heal. To avoid this issue, you should set <code>argocd.enabled</code> to true to prevent this issue. By doing this, it will add annotations to any <code>ConfigMap</code> resources that are identified by their hash, so that ArgoCD does not prune the resources.</p>"},{"location":"guide/smilecdr/updating/#long-running-processes","title":"Long Running Processes","text":"<p>Although these techniques will avoid any disruption to the application availability, any long running processes may be interrupted. Remember to design any workflows to be able to handle unexpected disruption, using retry mechanisms for any tasks that do not complete correctly due to transient infrastructure interruption.</p>"},{"location":"quickstart/","title":"Deployment Quickstart","text":"<p>This section of the documentation will get you up and running quickly to show how the chart works. For any real deployments, please look through the advanced deplopyments in the User Guide and Examples sections to design a solution that works for your environment.</p>"},{"location":"quickstart/#preparation","title":"Preparation","text":"<p>To deploy Smile CDR using these Helm Charts, you will need to do the following:</p> <ul> <li>Ensure all requirements and dependencies are in place</li> <li>Prepare a set of configurations to suit your planned installation architecture</li> <li>Perform the deployment</li> </ul> <p>The following pages will guide you through the above steps to so that you can gain familiarity with how these Helm Charts function.</p>"},{"location":"quickstart/#advanced-deployment","title":"Advanced Deployment","text":"<p>The Quickstart shows you a basic install that does not follow security best practices. To install with best practices in mind, refer to the advanced configurations in the User Guide section which goes into detail on all available configuration options.</p>"},{"location":"quickstart/helm-repo/","title":"Configure Helm Repository:","text":"<p>Before you can use the Smile Digital Health Helm Charts, you need to configure your deployment tool to point to the repository where the charts are hosted.</p> <p>In this Quickstart, we will use the native <code>helm</code> command, but you may wish to deploy using alternative tooling in your environment. Please check the User Guide for more info on this.</p>"},{"location":"quickstart/helm-repo/#add-repository","title":"Add repository","text":"<p>Add the repository like so.</p> <pre><code>$ helm repo add smiledh https://gitlab.com/api/v4/projects/40759898/packages/helm/devel\n$ helm repo update\n</code></pre> <p>Note It is also possible to run the <code>helm install</code> command by pointing directly to the repository. In this case, there is no need to run the <code>helm repo</code> commands above.</p>"},{"location":"quickstart/install-smilecdr/","title":"Install Smile CDR","text":""},{"location":"quickstart/install-smilecdr/#install-the-helm-chart","title":"Install the Helm Chart","text":"<pre><code>$ helm upgrade -i my-smile-env --devel -f my-values.yaml smiledh/smilecdr\n</code></pre> <p>Smile, we're up and running! :)</p> <p>After about 2-3 minutes, all pods should be in the <code>Running</code> state with <code>1/1</code> containers in the <code>Ready</code> state. <pre><code>$ kubectl get pods\nNAME                                 READY   STATUS      RESTARTS        AGE\nmy-smile-env-pg-backup-xsc6-trp8d    0/1     Completed   0               2m29s\nmy-smile-env-pg-instance1-84cn-0     0/3     Pending     0               2m59s\nmy-smile-env-pg-instance1-9tkd-0     3/3     Running     0               2m59s\nmy-smile-env-pg-repo-host-0          1/1     Running     0               2m59s\nmy-smile-env-scdr-5b449f8749-6ksnc   1/1     Running     2 (2m28s ago)   2m59s\n</code></pre></p> <p>NOTE: Don't be alarmed about the restarts. This was because the database was not ready yet. This demonstrates how the pod self-healed by restarting until the DB became available.</p> <p>At this point, your Smile CDR instance is up and can be accessed at the configured URL. You can try re-configuring it using the instructions in the User Guide, or you can delete it like so: <pre><code>$ helm delete my-smile-env\n</code></pre></p> <p>WARNING: If you delete the helm release, the underlying <code>PersistentVolume</code> will also be deleted and you will lose your database and backups. You can prevent this by using a custom <code>StorageClass</code> that sets the <code>ReclaimPolicy</code> to <code>Retain</code>.</p>"},{"location":"quickstart/requirements/","title":"Quickstart Requirements","text":"<p>There are a number of prerequisites that must be in place before deploying Smile CDR using this Quickstart guide.</p> <p>These dependencies are sufficient to get you started with deploying an instance for testing purposes.</p> <ul> <li>Access to a container repository with the required Smile CDR Docker images<ul> <li>e.g. <code>docker.smilecdr.com</code> or your own registry with a custom Docker image for Smile CDR</li> </ul> </li> <li>Kubernetes Cluster that you have suitable administrative permissions on.<ul> <li>You will need permissions to create namespaces and maybe install Kubernetes add-ons</li> </ul> </li> <li>Sufficient spare compute resources on the Kubernetes cluster.<ul> <li>Minimum spare of 1 vCPU and 4GB memory for a 1 pod install of just Smile CDR</li> </ul> </li> <li>One of the following supported Ingress controllers:<ul> <li>Nginx Ingress Controller (Preferred)</li> <li>AWS Load Balancer Controller</li> <li>Azure Application Gateway Ingress Controller</li> </ul> </li> <li>TLS certificate that can be provisioned on the load balancer used by the Ingress objects<ul> <li>e.g. AWS Certificate Manager.</li> </ul> </li> <li>DNS entries pointing to load balancer.<ul> <li>e.g. Amazon Route 53</li> </ul> </li> <li>CrunchyData Postgres Operator<ul> <li>Allows you to install a PostgreSQL cluster as a part of the Smile CDR deployment</li> <li>This is used for the Quickstart as it is the easiest way to get up and running without   having to provision an external database and configure credentials and connectivity</li> </ul> </li> <li>Persistent Volume provider that can be used to create <code>PersistentVolume</code> resources for the database</li> </ul>"},{"location":"quickstart/values-file/","title":"Create a Helm values file for your environment","text":"<p>To use the Smile CDR helm Chart, you will need to create a values file with some mandatory fields provided.</p>"},{"location":"quickstart/values-file/#a-note-on-creating-values-files","title":"A note on creating values files","text":"<p>Do not copy the default <code>values.yaml</code> file from the Helm Chart, start from a fresh empty file instead.</p> <p>See the section on Values Files Management for more info on this.</p>"},{"location":"quickstart/values-file/#example-values-file","title":"Example Values File","text":"<p>The following example will work in any Kubernetes environment that has the following components installed.</p> <ul> <li>Nginx Ingress</li> <li>CrunchyData PGO</li> <li>A suitable Persistent Volume storage provider (For the database).</li> </ul> <p>You will need to update the values specific to your environment and include credentials for a container repository that contains the Smile CDR Docker images.</p> <p>WARNING: The following method of providing Docker credentials in the values file is insecure and only shown in this quick-start demonstration to show the chart in action. You should instead use an alternative such as an external secret vault. At the very least, provision the Kubernetes Secret object in a separate process that does not store the secret anywhere in code.</p> <p><code>my-values.yaml</code> file <pre><code>specs:\nhostname: smilecdr.mycompany.com\nimage:\nrepository: docker.smilecdr.com/smilecdr\ncredentials:\ntype: values\nvalues:\n- registry: docker.smilecdr.com\nusername: &lt;DOCKER_USERNAME&gt;\npassword: &lt;DOCKER_PASSWORD&gt;\ndatabase:\ncrunchypgo:\nenabled: true\ninternal: true\n</code></pre></p>"}]}